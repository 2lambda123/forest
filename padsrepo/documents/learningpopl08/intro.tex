
An {\em ad hoc data source} is any semistructured data source
for which useful data analysis and transformation tools
are not readily available. XML, HTML and CSV are {\em not} 
ad hoc data sources as there are numerous programming libraries,
query languages, manuals and other resources dedicated to
helping analysts manipulate data in these formats.
However, despite the prevalence of standard formats,
massive quantities of legacy ad hoc data persist in fields ranging from
computational biology to finance to physics to networking to health care and
systems administration.  Moreover, engineers and scientists are continuously
producing new ad hoc formats ---despite the presence of existing 
standards--- because it is often expedient to do so.  Over time, these
expedient formats become difficult to work with because of missing
documentation, a lack of tools, and corruption caused by repeated
redesign, reuse and extension.

The goal of the \pads{}
project~\cite{fisher+:pads,fisher+:popl06,mandelbaum+:pads-ml,padsweb}
is to improve the productivity of data analysts who need to cope with
new and evolving ad hoc data sources on a daily basis.  Our central
technology is a domain-specific language in which programmers can
specify the structure and expected properties of ad hoc data sources,
whether they be ASCII, binary, Cobol or a mixture of formats.  These
specifications, which resemble extended type declarations from
conventional programming languages, are compiled into a suite of
programming libraries, such as parsers and printers, which are then
linked to generic 
data processing tools including an XML-translator, a query
engine~\cite{fernandez+:padx}, a 
simple statistical analysis tool, and others.  Hence, an important
benefit of using \pads{} is that  a single declarative description
may be used to generate many useful end-to-end data processing tools completely
automatically.

On the other hand, a significant impediment to using \pads{}
is the time and expertise needed
to write a \pads{} description for a new ad hoc data source.
For data experts possessing clear, unambiguous documentation about a
simple data source, writing a \pads{} description may take 
anywhere from a few minutes to a few hours.  However,
it is relatively common to encounter ad hoc data sources that 
contain valuable information, yet have little or no documentation.
Understanding the structure of the data and creating descriptions for such 
sources can take days or weeks depending upon the complexity
and volume of the data in question.  In one specific example, Fisher
spent approximately three weeks (off and on) attempting to understand
and describe an important data source used at AT\&T.  One
of the difficulties was that the data source suddenly 
switched formats after approximately 1.5 million entries.  Of course,
if dealing with the vagaries of ad hoc data is
time-consuming and error-prone for experts, it is even worse for
novice users.

To improve the productivity of experts and to 
make the \pads{} toolkit accessible to new users with 
little time to learn the specification language,
we have developed an automatic format inference engine.
This format inference engine reads arbitrary ASCII data sources
and produces an accurate, human-readable description of the source.
These machine-produced descriptions give experts a running start
in any data analysis task as the libraries generated from these
descriptions may be incorporated directly into an ordinary C program.
The inference engine is also directly connected to the rest of the
\pads{} infrastructure, making it possible for first-time users,
with no knowledge of the \pads{} domain-specific language, 
to translate data into a form suitable for loading into a relational database, 
to load it into an Excel spreadsheet,
to convert the data into \xml{},
to query it in XQuery,
to detect errors in additional data from the same source,
and to draw graphs of various data components, all
with just a ``push of a button.''

In designing a format inference engine for \pads{}, we are in
territory explored in the past by the machine learning community.  For
example, there have been many attempts to devise algorithms that learn
regular expressions, context free grammars and more exotic language
classes.  These algorithms have been used to perform tasks ranging
from natural language understanding to type inference for \xml{}
documents to information extraction from web pages.  One key
difference in our work is that we target an
understudied domain (ad hoc systems data) that allows new techniques for
effective inference.  A second key difference is that we solve a
new problem by showing how to generate an entire suite of end-to-end
data processing tools with no human intervention.  
Section~\ref{sec:related} contains a more in-depth analysis of related
work.  To summarize, this paper makes three main contributions:
\begin{itemize}
\item We have developed a multi-phase algorithm 
that infers the format of complex, ad hoc data sources,
producing compact and accurate \pads{} descriptions.  
%This algorithm
%adapts techniques previously used to infer the structure of web pages
%and \xml{} documents to our new problem domain.

\item We have incorporated the inference algorithm into 
a modular software system that uses sample data to
generate a toolkit of useful data processing tools,
without requiring any human intervention.
 
\item We have evaluated the correctness and performance of
our system on a range of ASCII data sources.  For many data
sources, training on 5\% or less of the data results in
accuracy rates greater than 95\% (often perfect).  
%In {\em all cases}, additional
%training data elevates accuracy rates above 95\% and
%in {\em no cases} need a user ever be unsure about the
%accuracy rate of generated descriptions -- the automatically
%generated accumulator tool measures both overall accuracy
%and field-by-field accuracy of the description on any new
%data source.
In all our benchmarks, the 
inference algorithm scales linearly with the quantity of
data.
\end{itemize}
\noindent
For readers interested in seeing our system operate live, 
there is an online demo illustrating its many features
(\url{http://www.padsproj.org}).  The remainder of this paper
describes the subset of the \pads{} description language we
attempt to infer (Section~\ref{sec:review}), the inference
algorithm itself and generated tools (Section~\ref{sec:inference}), 
the performance (Section~\ref{sec:exp}), strengths and 
weaknesses of our approach (Section~\ref{sec:discussion}),
related work (Section~\ref{sec:related}) and 
conclusions (Section~\ref{sec:conclusion}).
%% In the next section of this paper, we describe the internal representation
%% used during the course of the inference algorithm.  For those
%% readers familiar with the \pads{} description language, this is largely a 
%% review.  Section~\ref{sec:inference} describes our format inference algorithm
%% in depth and illustrates its action on two sample data sources.  
%% Section~\ref{sec:exp} evaluates the performance and correctness of our
%% system on 15 different ad hoc data sources, drawn mostly from systems
%% and networking domains.  Section~\ref{sec:discussion} discusses
%% how users can deal with errors
%% in generated descriptions and points out weaknesses 
%% we plan to address in future work.
%% Sections~\ref{sec:related} and \ref{sec:conclusion} present related work
%% and conclude respectively.  
This paper is an extended version of a 2-page
summary presented at the CAGI 2007 workshop on grammar 
induction~\cite{burke+:cagi07}. 


%% Given the difficulties 

%% Our end
%% goal is to provide users with an end system that allows them to
%% automatically generate sufficiently accurate \pads{} descriptions 
%% that these descriptions may be fed into our compiler to generate
%% useful programming libraries and data 
%% processing and visualization tools. 

%% The goal of this project is to provide a generic framework that includes
%% languages and tools to seamlessly automate data stream analysis. 
%% Given some samples of the data stream, our prototype system produces 
%% an intermediate
%% representation of the structure of the data through structure discovery
%% and refinement, and translates that representation into a
%% declarative data-description language, \padsc{}. \padsc{} is 
%% expressive enough to describe a variety of data feeds 
%% including ASCII, binary, EBCDIC, Cobol, and mixed data formats.  
%% From \padsc{}, a suite of tools can generated with functions for 
%% parsing, manipulating, and summarizing the data. All these can be 
%% done with a ``push of a button.''   


% Transactional data streams, such as sequences of stock-market buy/sell orders,
% credit-card purchase records, web server entries, and electronic fund
% transfer orders, can be mined very profitably.  As an example,
% researchers at AT\&T have built customer profiles from streams of
% call-detail records to significant financial effect~\cite{kdd99}.   
% Often such streams are high-volume: AT\&T's call-detail stream contains
% roughly 300~million calls per day requiring approximately 7GBs of
% storage space.  Typically, such stream data arrives ``as is'' in
% \textit{ad hoc} formats with poor documentation.  In addition, the
% data frequently contains errors.  The appropriate response to such
% errors is application-specific. 
% %Some applications can simply discard
% %unexpected or erroneous values and continue processing.  For other
% %applications, however, errors in the data can be the most interesting
% %part of the data.  

% Understanding a new data stream and producing a suitable parser are
% crucial first steps in any use of stream data.  Unfortunately, writing
% parsers for such data is a difficult task, both tedious and
% error-prone. It is complicated by lack of documentation, convoluted
% encodings designed to save space, the need to handle errors
% robustly, and the need to produce efficient code to cope with the
% scale of the stream.  Often, the hard-won understanding of the data
% ends up embedded in parsing code, making long-term maintenance
% difficult for the original writer and sharing the knowledge with
% others nearly impossible.

%\paragraph*{notes.}
%this is the introduction.  * explain system goals from a user
%perspective * explain what kinds of questions users can ask and get
%answered concerning data that they have. explain who, exactly, would
%want to use our system and what tasks exactly can our system help a
%target user achieve. explain goals not mechanisms * list contributions


%% \begin{figure}
%% \begin{center}
%% \begin{tabular}{|l|l|}
%% \hline
%% Name \& Use   &  Representation               \\ \hline\hline
%% Web server logs (CLF):  &  Fixed-column ASCII records \\ 
%% Measure web workloads &                             \\ \hline
%% AT\&T provisioning data: & Variable-width ASCII records  \\ 
%% Monitor service activation &                              \\ \hline
%% Call detail: Fraud detection  &  Fixed-width binary records \\  \hline 
%% AT\&T billing data: & Various Cobol data formats  \\ 
%% Monitor billing process   &                             \\ \hline
%% %IP backbone data:  & ASCII   \\
%% %Monitor network performance  &        \\ \hline
%% Netflow:                        & Data-dependent number of     \\ 
%% Monitor network performance  & fixed-width binary records  \\ \hline
%% Newick:   Immune                 & Fixed-width ASCII records \\ 
%% system response simulation & in tree-shaped hierarchy\\ \hline                                
%% Gene Ontology:             & Variable-width ASCII records \\
%% Gene-gene correlations     & in DAG-shaped hierarchy \\ \hline
%% %HL7:             & Variable-width ASCII records \\
%% %Medical lab results     &  \\ \hline
%% CPT codes: Medical diagnoses & Floating point numbers \\ \hline
%% SnowMed: Medical clinic notes & keyword tags  \\ \hline

%% \end{tabular}


%% \caption{Selected ad hoc data sources.}
%% \label{figure:data-sources}
%% \end{center}
%% \end{figure}
 
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
