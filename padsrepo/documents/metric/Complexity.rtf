{\rtf1\mac\ansicpg10000\cocoartf824\cocoasubrtf410
{\fonttbl\f0\fswiss\fcharset77 Helvetica;\f1\fswiss\fcharset77 Helvetica-Oblique;}
{\colortbl;\red255\green255\blue255;\red255\green0\blue255;\red153\green102\blue51;\red255\green0\blue0;
}
\margl1440\margr1440\vieww14820\viewh18480\viewkind0
\deftab160
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab160\ql\qnatural\pardirnatural

\f0\fs24 \cf0 \
We had a whole bunch of interesting discussions last week on various different aspects of the complexity/scoring functions.  I decided to write up a little note to try to remember/systematize/formalize the various decisions that we made or were considering or in some cases, slight generalizations of the decisions/ideas.  Hopefully, the note can serve as the basis for future discussions.  Since I wrote this all up in one pass, it is very likely that I made plenty of mistakes -- please let me know when and where you see problems. \
Also, my definitions will surely deviate slightly from what has been implemented -- I generated the definitions from our discussions and then tried to make them as systematic as possible.  In places where there are deviations, should what I've written be changed to reflect practical realities?  Should the implementation be changed to coincide with the definitions?  Ideally, we should try to strive for the following things at least: \
\
1. semantic consistency between definitions and our implementation\
2. clarity of definitions\
3. simplicity of definitions where practical details are uninteresting\
4. reproducibility (given the definitions, can some smart person come close to reimplementing our metric in practice)\
5. uniformity of definitions\
6. adherence to information-theoretic principles\
7. tractability (must be efficiently implementable)\
\
One place I imagine there are inconsistencies is in the encoding of arrays/strings/regexps of arbitrary length.  there are two encodings that we discussed:\
\

\f1\i \cf2 -- This is actually a discussion of \ul type\ulnone  complexity for different encodings, not a discussion of the encodings themselves.
\f0\i0 \cf0 \
Encoding 1 (the "C string encoding") for string s:\
\
s.length * (log (#ofValidCharacters + 1))\
\
\
Encoding 2 (the "length-field encoding") for string s:\
\
LengthBits + s.length * (log (#ofValidCharacters))\
\
\
Encoding 2 has the disadvantage that it implicitly assigns a maximum length of 2^LengthBits for all arrays and forces us to choose a constant for LengthBits.  I'm not sure of the optimum value for that (I suggest using 32 bits to start with and see if that has any negative practical consequences).\
\
I chose Encoding 2 because (at the time) it was a simpler for me to formulate the data complexity for arrays with arbitrary element type (ie: I did not have to figure out how many things can inhabit the arbitrary element type and then add 1).  However, this was a bit of an arbitrary choice -- I am sure we could use encoding 1 if we wanted to.  If we want to switch to Encoding 1, then I think we should do it universally for all different sorts of variable-length structures.\
\
FYI, the document also contains the syntax of a little formal calculus we can use as the basis for explaining our inference algorithm in a future paper.  I should define a useful semantics for it at some point ...\
\
I will put the document on the wiki so we can modify it as we find ways to improve it.\
\
\
Dave\
Inductive definitions for the complexity measure and alternatives.\
\
My goals here, in order of relative importance, are to:\
\
A. provide us with a relatively complete document for discussion of the\
     nuances of the various complexity functions.  I'm just trying to\
     make sure we are all on the same page and that I actually understand\
     what is going on....\
\
B. to be completely uniform in approach (trying to avoid making one choice\
     in one place and a different choice in another -- this is facilitated\
     by incorporating all the different bits and pieces we've been discussing\
     over the last few weeks).\
\
C. to adhere completely strictly to the information-theoretic idea of \
     generating an encoding for the data, though for simplicity, my\
     encodings are clearly sub-optimal in places (ie: I'm sure they\
     could be compressed).  If we avoid deviating from the principles\
     in any way then my POPL-oriented brain appreciates it.  If practical\
     considerations mandate deviation in the future, that is fine --\
     we will have a strong reason we can document.\
\
D. provide a series of definitions, which if we agree on them, could\
     form the basis for part of an academic paper on the inference\
     system. \
\
Note -- I did this all in one pass so there are probably mistakes.  I'll\
come back and revise later.\
\
-------------------------\
Terminology/Abbreviations\
-------------------------\
\
SC(s)    = Complexity of a string constant s\
\
TC(T)    = type complexity of T in bits (ie: an encoding of the syntax of T)\
\
ADC(T,d) = "atomic" data complexity of the \cf3 single word\cf0  d relative to T in bits\
             (atomic refers to the fact that we are only considering a \
             single word d, not an entire data set)\

\f1\i \cf2 -- Does "single word" mean a "record" in PADS? Does it mean any string of characters long enough to be parsed by the type T?\

\f0\i0 \cf0 \
ADC(r,d) = "atomic" data complexity of the single word d relative to\
             regular expression r in bits\

\f1\i \cf2 -- **** I am not sure that it is useful to look at a single word of data. The current complexity metric often considers average length of tokens seen in a data run, and for switches considers the weighted average of the branches of a switch, where the weights used are frequencies. The purpose of the inferred type is to have predictive power for data as yet unseen. If we apply the type to a single data item, are we making the sort of "type error" that comes from applying statistical arguments to an individual?\

\f0\i0 \cf0 \
DC(T,ds) = data complexity of a set of ds relative to T in bits\
\
C(T,ds)  = total complexity metric for an entire data set of ds relative to T\
             in bits\
\
Card(base) = total number of base types (size of base type "code book")\
\
Card(type) = total number of type constructors including base types\
\
--------------------\
Description Language\
--------------------\
\
--------------------------------------\
Constants & Parsed Data Representations\
--------------------------------------\
\
The following pseudo-BNF defines the language under consideration.\
If the following language could be used as the centerpiece for\
the purposes of writing a paper that would be cool.\
\
characters a                 (a ranges over ASCII characters)\
integers i                   (i is any, say, 32-bit integer)\
strings s                    (s is a sequence of ASCII characters)\
constants c ::= a | i | s \
\
representations of parsed data\
d ::= c        (constant)\
    | (d1,d2)  (pair of data items)\
    | in_i(d)  (injection into the ith alternative of a union)\
    | d1...dk  (sequence of data items)\
\
\
--------------------------------------\
Base Types\
--------------------------------------\
\
variables x                  (identifiers used to tag base types for\
                              representation of dependencies)\
base types\
b ::= Pint | Pint[i_min,i_max] | PintConst[i]\
    | Pstring | PstringFW[i] | PstringFWX[x] | PstringConst[s]\
    | ComplexB\
\
\
Note: ComplexB is supposed to represent some complex base type such as\
a time or date or path.  It is any PADS base type that can be defined by \
a regular expression r as expressed below.\
\
regular expressions\
r ::= epsilon   (empty string)\
    | a         (single character)\
    | r1.r2     (concatenation)\
    | r1|...|rk (multiple union)\
    | r*        (Kleene star)\
\

\f1\i \cf2 -- The PADS manual allows "regular expressions" that are not "regular", per the Perl syntax.\

\f0\i0 \cf0 Practical regular expression languages have many extensions to this\
basic syntax.  However, any such extension that is truly "regular"\
can be expressed using some combination of the regular expressions above.\
Note, I specifically chose the multiple-element union r1|...|rk as opposed to\
the binary union r1|r2 to improve the encoding below.  Using a binary\
union is possible but the simplest, natural, inductive encoding uses\
far more bits than the multiple union.\
\
I chose the set of base types above not because it's complete but\
because it seems to cover all the issues that we have been discussing\
that I can think of.  If there is some interesting base type that\
is missing and brings up issues not covered, we should certainly add it.\

\f1\i \cf2 How can we handle the "everything but" syntax (e.g. [^ab] is everything that is not a or b)? In particular, what is the complete character set?\

\f0\i0 \cf0 \
Card(base) = 8   (by complete chance, a nice round number)\
\
-----\
Types\
-----\
\

\f1\i \cf2 -- We probably need the usual restrictions on scope of variables here. What does x:arrayFWX[x] mean?\
-- I don't think I completely grok the variables yet.....\

\f0\i0 \cf0 T ::= b                      (base type where nothing depends on it)\
    | x:b                    (x used when something depends on base type)  \
    | struct T1 ... Tk\
    | union T1 ... Tk\
    | enumStrings s1 ... sk\
    | enumInts i1 ... ik\
    | switchStrings x of (s1 => T1 ... sk => Tk)\
    | switchInts x of (i1 => T1 ... ik => Tk)\
    | array T                (unbounded length)\
    | arrayFW T[i]           (constant fixed length i)\cf2  
\f1\i -- According to Kenny we can do this with RArray?
\f0\i0 \
\cf0     | arrayFWX T[x]          (dependent fixed length x)\
\
Invariant: given any type T, all x attached to base types are unique\
Convention: when the x in (x:base) is unimportant (ie: unused elsewhere in\
  the description), we drop the x and merely write (base).\
\
Card(type) = 11 + Card(base) (not such a round number, too bad :-)\

\f1\i \cf2 -- I called this "C" in my slides. \

\f0\i0 \cf0 \
--------------------\
Complexity Functions\
--------------------\
\
------------------------------------\
Type Complexity of Base Types (TC(b))\
-------------------------------------\
Recall definition: The number of bits to transmit the syntax of a base type.\
\

\f1\i \cf2 -- In the current definition, a "LargeInt" is used, which has no limit on the size.\
-- Also, for IntBits, don't forget to add 1 for the sign (+ or -)\

\f0\i0 \cf0 IntBits = the number of bits needed to transmit an integer parameter\
             (We can adjust this constant to suit our purposes --\
              it doesn't have to be the "real" transmission cost of a\
              integer PADS parameter -- but for now, let us assume 32)\
\
CharBits = the number of bits needed to transmit an ASCII character\
\
LengthBits = the number of bits needed to transmit a length for a\
             potentially unbounded string or array.  (This actually\
             will have the effect of assuming that all strings and\
             arrays must be less than some max size.  It seems\
             unlikely that this will be problematic.)  Length bits\
             could potentially be the same as IntBits.  Again,\
             we can futz with this number as we see fit.\
\
VarBits = the number of bits needed to encode a parameter.\
\
Complexity of a string s:\
SC(s) = LengthBits + CharBits^(s.length)\
\
TC(Pint)              = log(Card(type)) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
TC(Pint[i_min,i_max]) = log(Card(type)) + IntBits + IntBits 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
TC(PintConst[i])      = log(Card(type)) + IntBits 
\f1\i \cf2 -- Check, but I am adding 1 here for the sign
\f0\i0 \cf0 \
TC(Pstring)           = log(Card(type)) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
TC(PstringFW[i])      = log(Card(type)) + IntBits 
\f1\i \cf2 -- Check, but this base type is not implemented yet\
-- For the next base type (PstringFWX[x]), the variable already appears elsewhere in the overall type structure, and its type complexity has already been counted. Are we counting it twice here?
\f0\i0 \cf0 \
TC(PstringFWX[x])     = log(Card(type)) + VarBits\
TC(PstringConst[s])   = log(Card(type)) + SC(s) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
TC(ComplexB)          = log(Card(type)) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
-----------\
Note on PstringFW[i] vs PstringFWX[x]: Since any complete description\
will have relatively few variables in it (fewer for instance in the\
total number of possible 32-bit integers), the encoding metaphor\
suggests \
\
VarBits < IntBits \
\
and therefore that \
\
TC(PstringFWX[x]) < TC(PstringFW[i]).\
\
Now, consider the following 2 description\cf4 s\cf0 :\
\
Description 1:       struct (x:PintConst[i]) (PstringFWX[x])\
Description 2:       struct (PintConst[i]) (PstringFW[i])\
\cf2 -- Do the discriminators (either x:PintConst[i] or PintConst[i]) ever occur after the disciminants in the data stream? Does such forward reference present any problems?\
\cf0 \
\cf2 -- Does "equally accurate" mean "semantically equivalent", i.e. describing the same set of data items? Is it a theorem that this implies equal data complexity? This sounds plausible, but I can't envision a proof at the moment.\
\cf3 These two descriptions are equally accurate yet syntactically\
different.  Since they are equally accurate, their data complexities\
should be identical. \cf2  \cf0 The only way to discriminate between the two is\
based on their type complexity.  If VarBits < .5*IntBits then\
information theory chooses the first as superior (see below for\
encoding of (x:b) vs. just b -- the former requires encoding the\
dependent variable x but the latter doesn't).  Is that what we want to\
do?  It seems we have a choice based on the relative weights of\
VarBits vs IntBits.\
-----------\
\
-----------------------------------\
Type Complexity of Any Type (TC(T))\
-----------------------------------\
\
TC(b)                     = TC(b)     (TC(b) includes log(Card(type)) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
TC(x:b)                   = TC(b) + VarBits 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
TC(struct T1 ... Tk)      = log(Card(type)) \
                          + LengthBits   (to encode number of sub-elements)\
                          + Sum(i=1..k) TC(Ti)
\f1\i \cf2 -- Check, but I forgot the LengthBits (now fixed)
\f0\i0 \cf0 \
\
TC(union T1 ... Tk)       = log(Card(type)) \
                          + LengthBits   (to encode number of sub-elements)\
                          + Sum(i=1..k) TC(Ti) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
TC(enumStrings s1 ... sk) = log(Card(type)) \
                          + LengthBits   (to encode number of strings)\
                          + Sum(i=1..k) SC(si) 
\f1\i \cf2 -- Check, I was using maximum complexity, but I changed it to the sum.
\f0\i0 \cf0 \
\
TC(enumInts i1 ... ik)    = log(Card(type)) \
                          + LengthBits   (to encode number of integers)\
                          + Sum(i=1..k) IntBits 
\f1\i \cf2 -- Check, same comment as previous.
\f0\i0 \cf0 \
\
TC(switchStrings x of (s1 => T1 ... sk => Tk)) \
                          = log(Card(type)) \
                          + VarBits      (to encode variable x)  
\f1\i \cf2 -- Is this part already accounted for elsewhere in the type structure?
\f0\i0 \cf0 \
                          + LengthBits   (to encode number of branches)\
                          + Sum(i=1..k) (SC(si) + TC(Ti)) 
\f1\i \cf2 -- Otherwise, check
\f0\i0 \cf0 \
\
TC(switchInts x of (i1 => T1 ... ik => Tk))\
                          = log(Card(type))\
                          + VarBits      (to encode variable x) 
\f1\i \cf2 -- Is this part already accounted for elsewhere in the type structure?
\f0\i0 \cf0 \
                          + LengthBits   (to encode number of branches)\
                          + Sum(i=1..k) (IntBits + TC(Ti)) 
\f1\i \cf2 -- Otherwise, check
\f0\i0 \cf0 \
\
TC(array T)               = log(Card(type)) + TC(T) 
\f1\i \cf2 -- Current implementation is very different, use separator and terminator.
\f0\i0 \cf0 \
\
TC(arrayFW T[i])          = log(Card(type)) + TC(T) + IntBits 
\f1\i \cf2 -- Don't have this yet.
\f0\i0 \cf0 \
\
TC(arrayFWX T[x])         = log(Card(type)) + TC(T) + VarBits 
\f1\i \cf2 -- Don't have this yet.
\f0\i0 \cf0 \
\
----------------------------------------------------------------------------\
Atomic Data Complexity of Data d Relative to Regular Expression r (ADC(r,d))\
----------------------------------------------------------------------------\
\
ADC(r,d) = some number of bits.  d must be a parsed representation\
           of data that matches r.  \
\
ADC(epsilon, "")        = 0\
ADC(a, a)               = 0 \
ADC(r1.r2, (d1,d2)      = ADC(r1,d1) + ADC(r2,d3)\
ADC(r1|...|rk, in_i(d)) = log(k) + ADC(ri,d) 
\f1\i \cf2 -- I think "lrk" should be "rlk". I still don't grok this one.
\f0\i0 \cf0 \
ADC(r*, d1...dk)        = LengthBits + Sum(i=1..k) ADC(r,di)\
\
\
Notes: \
\
1. complexity for alternatives r1 | r2 may be wrong when r1 and r2 are\
equivalent regular expressions or have subparts that are equivalent?\
Let's just avoid that problem practice, if it is a problem -- I'm not\
sure what the ramifications are.\

\f1\i \cf2 -- I think "overlapping" regular expressions that are adjacent in the data could be a real problem. For example, if a date or time is followed by an integer. This is more of a problem for type inference than for complexity, but it might be nice if the complexity measure could inform the type inference here?\

\f0\i0 \cf0 \
2. the regular expression r* is really bounded to have no more than\
2^LengthBits items, like other arrays.\
\
3. note that we only actually need the data d to find the complexity\
of data relative to a Kleene star expressions.  Hence, in practice, our\
parser only need to extract the data that matches Kleene-star items\
and measure the number of repetitions.  This will be simpler to implement \
on a case-by-case basis than completely implementing LexGen's regexp \
engine to extract the data bits that we need for measurement of the \
complexity function.\
\
----------------------------------------------------------------------------\
Atomic Data Complexity of Base Types\
----------------------------------------------------------------------------\
\
ADC(b,d) = some number of bits\
\

\f1\i \cf2 -- In many of these I am using average token length, see previous comment labeled ****\

\f0\i0 \cf0 ADC(Pint, i)              = IntBits\
ADC(Pint[i_min,i_max], i) = log(i_min - i_max)\
ADC(PintConst[i], i)      = 0\
ADC(Pstring, s)           = SC(s)\
ADC(PstringFW[i], s)      = s.length * CharBits\
ADC(PstringFWX[x],s)      = s.length * CharBits\
ADC(PstringConst[s], s)   = 0\

\f1\i \cf2 -- Here there be dragons: I think the question of overlapping regular expressions would come up here.\

\f0\i0 \cf0 ADC(ComplexB,d)           = ADC(r,d) where r is the regexp defining ComplexB\
\
Notes:\
\

\f1\i \cf2 -- I think the Pint[min,max] type is important in some applications. For example, when a data item represents a code for a data type, say for example "Pint" is 1, "Pstring" is 2, and so on. On the other hand, maybe an Enum covers this case well enough?\

\f0\i0 \cf0 1. Compare total complexities of  Pint[i_min,i_max] to Pint -- \
Pint[i_min,i_max] is likely to overfit.  Must avoid that. I am\
not sure if Pint[i_min,i_max] is of type I should be including\
in this calculus.  If I do perhaps I should be doing so with\
an eye towards translating it into a fixed selection of other\
simpler base types such as:  Pint64, Pint32, Pint16, Pint8, Pint4.\
If this is the case, then Pint[i_min,i_max] is really an "abbreviation"\
for Pintk when all integers in the range i_min,i_max are representable in\
k bits.  \
\

\f1\i \cf2 -- Reference again comment ****\

\f0\i0 \cf0 ----------------------------------------------------------------------------\
Atomic Data Complexity of Types\
----------------------------------------------------------------------------\
\
ADC(T,d) = some number of bits\
\
ADC(b, c)                          = ADC(b,c) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
ADC(x:b, c)                        = ADC(b,c) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
ADC(struct T1 ... Tk, (d1,...,dk)) = Sum(i=1..k) ADC(Ti,di) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
ADC(union T1 ... Tk, (in_i d))     = log(k) + ADC(Ti, d)) 
\f1\i \cf2 -- Should use a weighted sum? Mine is also incorrect here?
\f0\i0 \cf0 \
\
ADC(enumStrings s1 ... sk, s)      = log(k) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
ADC(enumInts i1 ... ik, i)         = log(k) 
\f1\i \cf2 -- Check
\f0\i0 \cf0 \
\
ADC(switchStrings x of (s1 => T1 ... sk => Tk), in_i d) = ADC(Ti,d)
\f1\i \cf2  -- I am using weighted sum here, and I have a term for log k
\f0\i0 \cf0 \
\
ADC(switchInts x of (i1 => T1 ... ik => Tk), in_i d)    = ADC(Ti,d) 
\f1\i \cf2 -- I am using weighted sum here, and I have a term for log k
\f0\i0 \cf0 \
\
ADC(array T, d1...dk)              = LengthBits + Sum(i=1..k) ADC(T,di)
\f1\i \cf2  -- I think mine is messed up here
\f0\i0 \cf0 \
\
ADC(arrayFW T[i], d1...dk)         = Sum(i=1..k) ADC(T,di)
\f1\i \cf2  -- Not implemented yet
\f0\i0 \cf0 \
\
ADC(arrayFWX T[x], d1...dk)        = Sum(i=1..k) ADC(T,di)
\f1\i \cf2  -- Not implemented yet
\f0\i0 \cf0 \
\
\
----------------------------------------------------------------------------\
Full Data Complexity of Types\
----------------------------------------------------------------------------\
\
DC(T, d1...dk) = Sum(i=1..k) ADC(T,di)\
\
Note: this term grows linearly with the number of data items.\
When the amount of data is large, it dominates the type complexity.\
\
----------------------------------------------------------------------------\
Complexity of Types\
----------------------------------------------------------------------------\
\
Given a collection of data records d1...dk, the end information-theoretic\
goal is to find the type T such that \cf4 D\cf0 C(T, d1...dk) is minimal.\
\
C(T, d1...dk) = h*TC(T) + DC(T, d1...dk)\
\
   where h is a hyper-parameter, which we will initially set to 1.\
\
----------------------------------------------------------------------------\
Relative Complexity of Types\
----------------------------------------------------------------------------\
\
The complexity "relative" (RC) to the number of bits in the data may be useful:\
\
RC(T,d1...dk) = C(T, d1...dk) / log(sizeof(d1...dk))\
\
(you can guess how sizeof is defined)\
\
We can also look for bad type complexity relative to the average size of\
the objects:\
\
RTC(T,d1...dk) = TC(T) / log(averagesizeof(d1...dk))\
\
If that number is very high that we have a hugely complex type that\
describes small data items.\
\
I have now run out of steam....}