
\subsection{A Data-Centric Monitor Generation System}

\paragraph*{Basic Architecture}
Figure~\ref{fig:arch} presents the architecture of our proposed
system.  At the top of the picture is the declarative description of all
data that will be used by the monitoring system.  It is here that
programmers encode all their knowledge about their data sources,
including the physical layout of data, its semantic properties
(including constraints on data fields and expected relations
with other fields) so deviations can be flagged as errors, 
its location, access protocol, when the data
will be ready for fetching and how often, etc.

In exchange for the programmer's work in describing their
data sources, the compiler system will generate
a robust and efficient library of {\em format-specific routines} (the center
square in the picture).
The core format-specific 
library includes parser, printer, error detection and data traversal
routines.  The core libraries will also be responsible for controlling
access to and aggregation of any data distributed across wide area
networks and archiving local data along with its description.  

Since these core libraries are compiler-generated from a
high-level data specification, they have many advantages over
hand-written code.  First, the generated code checks
all possible error cases: system errors related to the input file,
buffer, socket, or remote data provider; 
syntax errors related to deviations in the physical
format; and semantic errors in which the data violates user
constraints.  Because these checks appear only in generated code, they
do not clutter the high-level declarative description of the data
source.  Moreover, since tools are generated
automatically by a compiler rather than written by hand, 
they are far more likely to be robust
and far less likely to have dangerous vulnerabilities such as
buffer overflows.  Moreover, as formats change and are updated,
one can make a small change in the high-level description
and the compiler takes care of the rest.  It is extremely unlikely
new vulnerabilities or buffer overflows will be added to the code
base over time as formats change.  Finally, all routines will
be highly optimized for processing the massive
data sets one sees in practice.

In addition to the compiler, which generates the core, format-specific
libraries, the system includes a number of {\em format-independent stubs}
that make use of the generated libraries and implement higher-level
functionality.  Each of these stubs are programmed once, independent
of any format.  In order to implement format-specific behavior, they
are linked with the core libraries, which will be carefully designed to
satisfy a generic, format-independent interface.  Examples of
format-independent tools include a generic query engine that
allows users to extract information from the data, 
a visualization tool that produces
web page summaries of data statistics or a reformatter to convert data
to a new format required by an off-the-shelf tool.  In addition,
a programmer may simply use the generated format-specific libraries directly 
within their own application program, custom built for some unique task. 
Of course whenever a programmer builds a new tool for their application, 
they may do so using the format-independent interface to the generated
libraries.  If they do so, their new tool may be reused by others
who have different data, but require a similar functionality.

At this point, the astute reader may ask why not simply generate
exactly one tool -- a translator that maps the ad hoc data into
XML, a standard format with hundreds of available tools and
rich programming support in all modern, widely used languages.
The problem with such an architecture is that ad hoc formats
are usually quite compact and exploding them into XML representations
can easily result in a space blowup of 8-10 times and an increase
in processing overhead.  Consequently, tiny
sensors in a sensor network cannot afford the expense (processor,
network bandwidth, battery life) of managing
XML data.  More generally, when the data sets get large, and we 
have seen they do in monitoring systems, 
ranging from 100s of MB/week to 100GB/day,
the extra overhead of a 10 times blowup is simply unaffordable.
While compression can reduce this impact, the decompression overheads
of most modern compressors can overwhelm the data processing overhead
of the underlying data.
In an environment where the data is being used and updated
by more than just the monitoring systems, maintaining a parallel
representation in XML is even more painful and impractical.