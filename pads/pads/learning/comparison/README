This directory stores the results from golden and silver IRs for a number of
selected examples. Currently the examples being tested are:

1967Transactions.short
ai.3000
boot.txt
crashreporter.log
dibbler.1000
ls-l.txt
quarterlypersonalincome.txt
railroad.txt
rpmpkgs.txt
yum.txt

The results from the golden IRs are saved in files with an extension ".golden";
the results from the silver IRs are saved in files with an extension ".solver".

This file will be a placeholder for future comments about the comparisons.

=======================
April 25/07
=======================
thoughts from discussions with Kenny, Kathleen, Dave:

1.  only create enums/constants from simple types (int/string/xml tag), not
complex types.  Pwhite has no enums, but does have constants.  (already implemented
by Kenny).

2.  tokenization of complex items (eg: url, path, filename especially) remains
difficult since lexemes are ambiguous/overlap.  Some lexemes are also somewhat
context sensitive and require at least single-character look-ahead (hence
regular expressions do not work well).
	-- short-term solution: multiple lex files (Kenny/Dave will explore)
	-- long-term solution: (1) learn probabilistic model & incorporate with other
             statistical infrastructure, and/or (2) implement simple tokenization
             but sophisticated rewriting -- take advantage of the fact that
             while it may be ambiguous what class a single string of characters
             falls into, we may be able to accumulate high probability evidence
             if if we work not with one string/record but with a large collection of
             strings/records during the rewriting phase.  (ie: note that tokenization
             can only work one string at a time).

3.  In crashreporter.log one sees several enums and several switches.  
The "silver" structure chooses the incorrect thing to switch on.  The
correct enum to switch on is the first enum in the sequence for which
a dependency arises.  Confusing things perhaps is the fact that
inside that enum there is a union.  Anyway, when introducing switches,
we might conjecture that the left-most thing in a file is the best thing
to switch on to avoid this problem.  However, it is not at all clear whether
this is a general solution (or to what extent it will fix the crash reporter).

To be specific, here is the following golden structure:

--------------------
[Enum] {[StringConst] "crashdump", [StringConst] "crashreporterd", }
...
Switch(sw)
case [StringConst] "crashdump":
  Punion
    [StringConst] "crashdump started";
    Pstruct [StringConst] "Started writing crash report to: " [Path] End Pstruct;
    ...
  End Punion;
case [StringConst] "crashreporterd":
    ...
End Switch;
--------------------
However, what ends up being inferred is:
--------------------
[Enum] {[StringConst] "crashdump", [StringConst] "crashreporterd", }
...
[Enum] {[StringConst] "Started", [StringConst] "crashdump", ... } (Id = BTy_17)

Switch(BTy_17):
	case [Enum] {[StringConst] "Started", [StringConst] "crashdump"...):
--------------------
Notice that the switch is conditioned on the second enum.  The second enum however,
is really the first word in an error message.  Separating this first word in the
error message from the other words results in a suboptimal structure.

4.  A heuristic that may help crash reporter is looking at all of the records
and finding a common prefix of tokens.  The common initial prefix may not
be obvious from the histograms if that common prefix contains tokens that
appear at varying rates in later portions of the records.

5.  We found a concrete example of a case where rewriting hits a local maximum
score in 1 rewriting step but if 2 rewriting steps were to be combined in sequence,
one would hit a better global maximum (the first step decreases the score,
but the second step more than makes up for the decrease).  The example involves
Pwhite and string constants.  Currently Pwhite in a certain case does not get
reduced to a string constant because of a score increase.  However, performing
a reduction to a string constant followed by merging with adjacent string constants
is a good rule.  I can't remember the details here,but it seems to me as though
we should be fiddling with the scoring rule to ensure that Pwhite is always
reduced to a constant string (eg: "  ") where possible.  Perhaps I am
misremembering the exact context here.


==============================================================================
dibbler.1000 thoughts
==============================================================================
(not all these thoughts pertain directly to dibbler.1000)

1. there is a rough structure processing step which should trigger if
it sees a histogram with a variable number of occurrences of a
particular token, but that variable number of occurrences is always
greater than some fixed number, say N.  What it should do is chop the
the input into two halves -- the first half with the fixed initial N
tokens and the second half with a variable number of tokens.  This
does not seem to be happening in the dibbler.1000 example.  Perhaps
because of the single header record that screws it up? Perhaps the
decision needs to be done on an approximate basis -- do it if there
are only one or two records that deviate from the pattern; throw
the deviant records into another case in a sum?

2. I know Kenny has worked hard at Pempty elimination, but I think the
following transformation, which uses a Pempty (print as PADS/C Pvoid)
is a good one:

Poption [ Union [ field1; field2; ...; fieldk ] ]

==>

Union [ field1; field2; ...; fieldk; Pempty fieldname]

It is like flattening nested unions into a single union.

I think that this should have the tiniest decrease in complexity
because in the first description, a Poption will have a (log 2) factor
+ a (log k) factor (the latter from the number of branches in the
union; the former from the fact that Poption is a union with two
branches).  Whereas in the second description, there will only be a
(log (k+1)) factor which is a tiny bit smaller than (log 2) + (log k).

3. it would be nice to have a compiler flag that did not print out the
complexity information.

4. my perception of how well the inference algorithm does may be
influenced by formatting.  a more-compact-looking description gives me
more satisfaction than a less-compact-looking description, even when the
descriptions are semantically the same.  in the future, it would be nice to
be able to print out descriptions in PADS/ML syntax.  In particular,
the presence of anonymous tuples would decrease the "apparent" complexity
of types (though making no impact on the "real" complexity) -- since label
names are meaningless anyway (except where there are dependencies), eliminating
simplifies the presentation.

Note the beauty of:

-----
pdatatype elem =
  Field0 of (Pstring * White) option * Pint
| Field1 of  Pstring * (White * Pstring) option

ptype record = elem option Plist('\n','|')
-----

vs this (which isn't PADS/C syntax, but is closer to the PADS/C level of
 verboseness):

-----
RArray
	Separator: [StringConst] "|"
	Poption
		Punion
			Pstruct
				Poption
					Pstruct
						[String] 
						[White] 
					End Pstruct;
				End Poption;
				[Pint] 
			End Pstruct;
			Pstruct
				[String] 
				Poption
					Pstruct
						[White]
						[String]
					End Pstruct;
				End Poption;
			End Pstruct;
		End Punion;
	End Poption;
End RArray
-----

==========================================================================
ls -l thoughts
=============================================================================

1. briefly: 
I started looking at the ls-l data.  it seems that there is not enough data.  It looks over-fit to me.  I wonder if we can define a metric that avoids overfitting by not adding dependencies/enums/switches when there is not a "statistically significant" enum/dependency hypothesis.

