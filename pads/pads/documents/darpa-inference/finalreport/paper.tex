%acmconf commands
\documentclass{article}
\setlength{\voffset}{-1in}
\setlength{\hoffset}{-1in}
\setlength{\textwidth}{7in}
\setlength{\textheight}{8.65in}
\setlength{\topmargin}{1in}
\setlength{\topskip}{9pt}
\setlength{\oddsidemargin}{0.75in}
\setlength{\evensidemargin}{0.75in}
\setlength{\footskip}{.35in}
%\setlength{\columnsep}{.83cm}
%\setlength{\columnseprule}{0pt}
\setlength{\headheight}{9pt}
\setlength{\headsep}{20pt}
\setlength{\marginparwidth}{0in}
\setlength{\marginparsep}{0in}

\usepackage{latin-abbrevs,math-cmds,math-envs,code,latexsym,amssymb}
\usepackage[dvips]{epsfig}


\title{Real-time Network Forensic Analysis Seedingl Proposal Final Report//
DARPA Contract No. FA8750-07-C-0014}

\author{
David Burke \qquad
Kathleen Fisher \qquad
Andy Adams-Moran \qquad
David Walker
}

\begin{document}
\maketitle{}
%\tableofcontents
%\vspace{-1cm}

\section{Introduction}
\label{sec:intro}

An empty citation to prevent bibtex from complaining.~\cite{fisher+:pads}

\section{Expert Interviews}
\label{sec:interviews}

\paragraph*{Chris Miller}

\begin{itemize}
\item Position: Princeton Computer Science Department Systems Administrator

\item What tasks might you use of such a workbench for?  
What kind of problems are you trying to solve?  
(Prevention?  Detection?  Investigation?  Other?)
Normally, the problems Miller attempts to solve are ``investigation'' problems.
Either a user alerts him to a problem or he notices that some machine
has gone down or some software is unresponsive or exhibiting unusual
behavior.  The goal is usually to track down the unusual behavior and
isolate the machine or the user.  In his experience attempting
``prevention'' using automatic methods (anomaly detection) 
does not work very well --
there are too many false positives.

\item What is your workflow?  Miller's workflow may be summarized as
doing 50-60 grep commands in succession over various different kinds of
ASCII log files and email to find out what is different
about the questionable account or machine in the last month.
The process is slow and it would be delightful if it could be speeded up,
but it seems to work.  Normally, the problem is obvious to a human
when it is found ({\em e.g.,} suddenly a surge in logins to a machine
from Brazil).

\item Who are your customers/clients?  Miller works for the computer science 
department, making sure the departmental computing resources are up and 
functioning.

\item What kinds of tools do you use now?  
What would you like improved?
The main tool is grep, 
over and over again over ASCII log files.
This works but it is slow.  If there was a tool that could
summarize all of the log files pertinent to a particular machine or
user over a particular time period ({\em e.g.} the last month)
that would be extremely helpful.  For instance, he might type in
a user name, machine name or an ip address and expect the system to bring up
all ``anomalous'' data relative to that name.  Anything that
speeded up the process of collecting and analyzing log files 
would be helpful.

\item Have you ever had a ``Bad Day'' that you attribute to a lack of 
NFWB.  Is 1-8 hours diagnosing a security problem a ``Bad Day''?  Nothing
more specific mentioned.

\item How much time do you spend on a particular incident?
It varies widely by event, but for just the triage phase 
(the part where Miller figures out *what* happened; 
excluding the part where it must be fixed), it takes anywhere 
from 1 to 8 hours. Miller does not remember ever been in the triage phase 
longer than a full work day. On average, he tends to be able to say 
pretty definitively what happened within 2 hours or so. The higher end 
is for the really serious, widespread incidents. 

\item If you were using the workbench, what would need to be in it to make it useful to you?  ``Good search.''  A facility to help detect ``anomalies''
would be helpful if it worked well, but not absolutely essential.
A facility from graphing certain properties ({\em e.g.,} SPAM vs. non-SPAM
or number of logins/day) might be a nice ``bell'' but not essential.

\item Can you characterize the kinds of data you encounter?  Almost
exclusively ASCII log files and email.  Logs of user logins to 
all our authenticated services, as well as packet filters, 
DHCP logs, and any logged software failures. 

\item What kinds of performance requirements do you have?
\begin{itemize}
\item Size?  In our primary collection, Miller stores 
approximately 150-300MB of gzip-compressed logs per month. 
For a representative month this year (August, 2006), 
that comes out to about 16mil lines of log file per month.

\item How much variation within single file?
In *format*, the individual files do not have much variation; 
probably describable in a dozen-or-less definitions per file. 
Obviously, though, the data itself varies with the size of the 
internet attacking us!

\item How much data do you need to process?
 For a given incident, Miller typically needs to look at 3 months of data. 
Sometimes more, but rarely less. 

\item How frequently do you process data?  
We haven't seen a major incident recently, but since Miller has worked
in the Computer science Department, they have averaged 
a major incident every 2 years or so.

In terms of processing logs, the machines are logged continuously and
the logs are rotated nightly.

\item What bit rate?

Hard to answer, but the CS department 
syslog server (where these logs are collected) 
averages 100kbps of traffic sent to it. 

\item How many different kinds of data sources do you need to process?
Here is a wishlist:
\begin{itemize}
\item syslog files
\item web server log files
\item cisco sflow   (it may
                     not be appropriate, but would be neat.) 
\end{itemize}

\item How frequently do you get a new data source to process?
 A new *type* of source is rare, but new sources are pretty common 
(in the shape of new servers that send additional syslog data). 

\item Are correlations between data sources important?
That would be very important for selling the tool to Miller, 
as correlation between these sources is the biggest time-sink in 
diagnosing an incident.

\end{itemize}  

\item What functions over the data do you need?  Good search/querying 
support. 

\item What would be a show-stopper?  No real show-stoppers.  It has to
be more time efficient than repeated grep.

\item Can we get example data?  No.  The data cannot be released for 
privacy reasons (it could be if we had an appropriate anonymization tool.)

\end{itemize}

\paragraph*{Lefty Elfeterious, AT\&T Labs Research}
\begin{itemize}
\item What tasks might you use such a workbench for?  What kinds of problems are you trying to solve? (prevention? detection? investigation? other?)

Lefty has built a system that monitors AT\&T's hosting service. It
detects and correlates relevant alarms.  It finds places where a
system is down, a disk is full, a cpu is overloaded, \textit{etc}.
The tool helps operations people understand how the monitored system
is behaving, not just whether it is under attack.  It is used for
planning purposes and to administer triage when the system is misbehaving.

\item What is your workflow?

Logs and active monitoring information such as the results of pings
are sent to collection point every 5 minutes, processed into internal
format, normalized, sorted, and made ready for visualization.  A web
browswer then gives the user a variety of options for visualizing the
data, navigated using pull down menus.  Graphs show current data
against a "signature" of the device/resource previously. They have a
topology of system presented as graph, which can be used to navigate
the data.

\item Who are your customers/clients?  
Operations people and network managers.


\item What kinds of tools do you use now?  What do you like about
  them?  What would you like improved? 
The system is hand-built by Lefty, using locally developed tools and
libraries such as pzip, graphviz.  It has a web based interface with
pre-packaged queries.  It is fast. It uses rules to trigger alarms. 
They measure direction and speed of change of the data and are able to
shift alarm definitions as circumstances warrant. They want to be able
to do more statistical analysis. 

\item If you were using the workbench, what would need to be in it to
 make it useful to you?  See what the tool does.

\item Can you characterize the kinds of data you encounter?
The raw data is typically ASCII, some in XML.  Lefty's tool converts
it into his DDS format, which is a binary, fixed-width format.  They
process something like 2.4 million records a day.  There are always
new formats, from different operating systems, different machines,
different versions, different applications.

\item What kinds of performance requirements do you have?  
They process something like 3GBits/day, in roughly five minute
increments.  They have ``lots and lots'' of different data sources to
process; the data sources to process are evolving, and correlations
between data sources are important.

\item  What would be show-stoppers?  Not discussed.

\item Have you ever had a "bad day" that you attribute to the lack of
  a network forensic analysis workbench?
The people I spoke with were responsible for building the workbench,
so the question isn't really relevant.

\item How much time does it take to track down your "average" security problem?
These people don't do the tracking.

\item What functions over the data do you need?
They provide canned time plots of historical values versus current
values, color-coded alarm values, a topology view, the ability to see
all current values associated with a given machine.

\item Can we get example data?
Yes, Lefty has sent pointer to where some of the data lives on an
AT\&T machine.
\end{itemize}

\paragraph*{Dan Sheleheda}
\begin{itemize}

\item  What tasks might you use such a workbench for?  What kinds of
  problems are you trying to solve? (prevention? detection?
  investigation? other?) 

He would use a workbench to help ingest new data formats into a system
for managing alerts for AT\&T's internal network.  They are mostly doing
triage on virus outbreaks with occasional intrusion events.  

\item What is your workflow?

Raw data sources send files to collection machines, where data is
vetted, normalized, and aggregated, pushed into daytona database and
into commercial rule-based system, which raises alerts based on
hard-coded patterns in the data.  Alerts and relevant data are passed
to web portal (aurora). Case workers manage alerts and can view
relevant data; can't query back into main system for information
privacy considerations.  Both legacy S and T have such systems.  Use
different commercial rule engines (intellitactics, accusite) Ingest
logs from firewall, vpn, microsoft domains, proxies, and intrusion
detection systems.


\item Who are your customers/clients?  
Operations agents who solve alerts and network managers.

\item What kinds of tools do you use now?   (see above)


\item If you were using the workbench, what would need to be in it to make it useful to you?
He would love to have registry of all files managed by system,
something that managed the feeds knowing the frequency of data, pull
vs push, and timeliness requirements. Historical profiles of the data
and topology views would be very helpful.  Integrating a new data
source is really time consuming.  Assistance in auto-understanding
data would be marvelous.  Commercial tools from accusite provides
parsers for a up-front fee, seat license, and per use charges.  It is
very expensive.


\item Can you characterize the kinds of data you encounter?
They generally process ASCII, avoiding binary.  They deal with enough
data that they have to aggregate it before storing it in databases. It
is common for files to have a fair bit of variation and for
``records'' to span more than a single line.


\item What kinds of performance requirements do you have?  
They process roughly 200 million raw events per day.  They process
data in five minute batches.  They have many different kinds of data
sources they need to process.  They have a huge backlog of new data
sources that they need to process.  Correlations are really important,
but very difficult.  What do you join on?

\item What would be show-stoppers?  
Linux is main platform.

\item Have you ever had a "bad day" that you attribute to the lack of a network forensic analysis workbench?
Not answered.

\item How much time does it take to track down your "average" security
  problem?
Not known.

\item What functions over the data do you need?
They provide a data dump to web front end.

\item Can we get example data?
He will work on sending.

\end{itemize}

\paragraph*{Chris Volinsky}
\begin{itemize}

\item What tasks might you use such a workbench for?  What kinds of
  problems are you trying to solve? (prevention? detection?
  investigation? other?) 
He would use it to vet and transform data before loading into R.

\item What is your workflow?
Chris gets a new data source and batch of questions every month or so.
He uses hand-written perl code to understand and normalize the data,
tabularize it, remove error values, and convert all representations of
missing values to a standard missing value representation.  

\item Who are your customers/clients?  
Managers trying to understand marketing applications, system performance. 

\item What kinds of tools do you use now?  
Chris uses  S and  perl. 
Input to S must be clean, rectangular data.  Perl good at some stuff, but clunky at others.  

\item If you were using the workbench, what would need to be in it to make it useful to you?
Support for transforming data into R

\item Can you characterize the kinds of data you encounter?
Chris processes all kinds of data, typically containing transactions.
The files range in size, up to Gigabytes.  He gets them in batches,
sometimes daily, sometimes monthly.  They often have a lot of
variation within a single file.

\item What kinds of performance requirements do you have?  
He gets a new data source about once a month, and correlations between
data sources are important.

\item What would be show-stoppers?  
Not addressed.

\item Have you ever had a "bad day" that you attribute to the lack of
  a network forensic analysis workbench?
Not addressed

\item How much time does it take to track down your "average" security problem?
Not focused on security problems per se

\item What functions over the data do you need?
Histograms, outliers, skewed distribution (3rd moments) would all be
useful, as would general measures of whether two variables the same or
correlated? Given a particular column, figure out how other columsn
relate to it via regressions, fuzzy functional dependencies. 
Box plots |-------[   |   ]--------| (min, 25\%, median, 75\%, max)
sometimes outliers represented as separate dots are also very useful..
Time progressions, grouped by day, week, month, and also by monday,
tuesday, etc, and hour of the day would be very useful.

\item Can we get example data?
He will send some.
\end{itemize}

\bibliographystyle{abbrv}
{
\bibliography{pads}
}

\appendix

\section{Example Data Formats}
\label{sec:example formats}

\paragraph{UNIX {\tt last}.} Chris Miller often starts out by
invoking this command when he needs to investigate a security breach.

\begin{verbatim}
dpw      pts/1        nj-71-0-108-124. Sat Dec  9 17:34 - 17:50  (00:15)    
dpw      pts/1        nj-71-0-108-124. Sat Dec  9 16:26 - 17:31  (01:04)    
dpw      pts/26       nj-71-0-108-124. Tue Dec  5 18:35 - 19:32  (00:57)    
dpw      pts/22       nj-71-0-108-124. Tue Dec  5 18:29 - 19:32  (01:03)    

wtmp begins Fri Dec  1 04:26:51 2006
\end{verbatim}   

\end{document}










































