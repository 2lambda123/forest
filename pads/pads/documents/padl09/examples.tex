\section{The Token Ambiguity Problem} \label{sec:examples}
\cut{
\begin{verbatim}
- example text
- describe where the examples come from, what the data is 
  (could be) used for
- show description written by hand (ideal case)
- show description from old learning system, which is verbose 
  and bad
  - explain why it's verbose and bad
\end{verbatim}
 }
% \begin{figure}[t]
% \begin{center}
% {\small
% \begin{verbatim}
% May 02 06:19:57 Updated: openssl.i686 0.9.7a-43.8
% Jul 16 12:37:13 Erased: dhcp-devel
% Jul 16 12:37:13 Erased: dhcpv6_client
% Dec 10 04:07:51 Updated: openldap.x86_64 2.2.13-4
% Dec 10 04:07:52 Updated: util-linux.x86_64 2.12a-16.EL4.12
% Dec 10 04:07:58 Updated: nss_ldap.x86_64 226-10
% Dec 10 04:07:58 Updated: shadow-utils.x86_64 2:4.0.3-58.RHEL4
% Dec 10 04:11:17 Installed: postgresql-libs.x86_64 7.4.8-1.RHEL4.1
% Dec 10 04:11:27 Installed: fonts-xorg-75dpi.noarch 6.8.1.1-1.EL.1
% \end{verbatim}
% }
% \end{center}
% \caption{Fragment of {\tt yum.txt} log file} \label{fig:yum}
% \end{figure}

\begin{figure}[t]
\begin{minipage}[t]{0.5\columnwidth}
\begin{code}
Penum action \{
  install Pfrom("Installed");
  update Pfrom("Updated");
  erase Pfrom("Erased");
\};
Pstruct version_hdr \{
  \kw{Pint} major; ':';
\}
Pstruct sp_version \{
  ' ';  
  Popt version_hdr h_opt;
  \kw{Pid} version;
\}
\end{code}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\columnwidth}
\begin{code}
Precord Pstruct entry_t \{
        \kw{Pdate} date;	
  ' ';  \kw{Ptime} time;	
  ' ';  action m; 	
  ": "; \kw{Pid} package;	
        Popt sp_version sv;
\};
Psource Parray yum \{
        entry_t[];
\};
\end{code}
\end{minipage}
\vskip -2ex
\caption{Ideal \pads{} description of {\tt yum.txt} format.}\label{fig:yum-gold}
\end{figure}


% In this section, we describe a real-world ad hoc data source to
% develop intuition for the token ambiguity problem, and we motivate
% using a probabilistic approach to solve it. Although we
% focus on a single format, many other ad hoc data sources share the
% token ambiguity problem, \eg{}, many of the examples listed on the
% \pads{} web site~\cite{padsweb}.

Consider
the log files generated by {\tt yum}, a common software package manager.
%, a tool for installing,
%updating and removing packages and dependencies in RPM-based systems.   
These log files consist of a series of lines,
each of which is broken into several distinct fields: 
date, time, action taken, package name and version. 
Single spaces separate the fields.  For instance:
{\small\begin{verbatim}
   May 02 06:19:57 Updated: openssl.i686 0.9.7a-43.8
   Jul 16 12:37:13 Erased: dhcp-devel
   Dec 10 04:07:51 Updated: openldap.x86_64 2.2.13-4
   ...
\end{verbatim}}
\normalsize
%
%look generates a log file describing its
%actions, a fragment of which appears in \figref{fig:yum}. 
%
%\cut{  I don't think this material belongs here. We would need to
%  explain why chunks are important, which is really an artifact of the
%  algorithm, whereas here we are talking about the data.
%The lines in \texttt{yum.txt} constitute a {\em unit of
%repetition} in the data source, which we call a {\em chunk}.
%% We should only use one term, and record is ambiguous.
%% or a {\em record}
%Depending upon the data source, the largest unit of repetition could
%be an input line, as in \texttt{yum.txt}, or a collection of lines if
%the data is grouped into blocks, or even full files if the data is
%spread across multiple files.
%}
%
%Each line in {\tt yum.txt} consists of several distinct fields: 
%date, time, action taken, package name and version. 
%Single spaces separate the fields.
%This file is used by system administrators to keep track of the software updates on the system. 

\figref{fig:yum-gold} shows an {\em ideal} \pads{} description of
{\tt yum.txt} written by a human expert.  The description is
structured as a series of C-like type declarations.  There
are {\em base types} like \cd{Pdate} (a date),
\cd{Ptime} (a time) and \cd{Pint} (an integer).
There are also {\em structured types} such as
\cd{Penum} (one of several strings), \cd{Pstruct}
(a sequence of items with different types, 
separated by punctuation symbols), \cd{Popt} 
(an optional type) and \cd{Parray} (a sequence of
items with the same type).  \pads{} descriptions
are often easiest read from bottom to top,
so the best place to start examining the figure
is the last declaration in the right-hand column.
There, the declaration says that the entire
source file (as indicated by the \cd{Psource} 
annotation) is an array type called \cd{yum}.
The elements of the array are items with
type \cd{entry\_t}.  Next, we can examine
the type \cd{entry\_t} and observe that it
is a new-line terminated 
record (as indicated by the \cd{Precord}
annotation) and it contains a series of fields
including a date, followed by a space, followed
by a time, followed by an action (which is another
user-defined type), followed by a colon and a
space, \etc{} We leave the reader to peruse the
rest of the figure.
%% The \cd{entry\_t} type describes each line in the data source.
%% The keyword \kw{Precord} denotes that a newline follows each \cd{entry\_t}.
%% \cd{Pdate} describes dates, \cd{Ptime} describes times, 
%% \cd{action} enumerates three different actions,
%% and \cd{Pid} matches a system identifier such as a file
%% name, a MAC address, \etc{} We call types such as \cd{Pdate} and \cd{Pid}
%% call {\em base types} because they represent a single token 
%% in \pads{}; types such as {\tt Pstruct}, {\tt Penum} and {\tt Popt} are 
%% compound types that represent
%% structures. 
%% %We highlight the base types in the \pads{} description. 

\cut{
\begin{figure}[t]
{\tiny
\begin{minipage}[t]{0.33\columnwidth}
\begin{verbatim}
#include "vanilla.p"
Penum Enum_10 {
	Installed10 Pfrom("Installed"),
	Updated10 Pfrom("Updated"),
	Erased10 Pfrom("Erased")
};
Popt Pstring_ME(:"/\\+\\+/":) Opt_22;
Penum Enum_31 {
	x86_6431 Pfrom("x86_64"),
	noarch31 Pfrom("noarch"),
	i68631 Pfrom("i686"),
	i38631 Pfrom("i386")
};
Pstruct Struct_53 {
	PPstring  v_string_49;
	'.';
};
Popt Struct_53 Opt_55;
Pstruct Struct_56 {
	PPip  v_ip_42;
	'.';
	Opt_55  v_opt_55;
};
Popt Struct_56 Opt_40;
Pstruct Struct_59 {
	Opt_40  v_opt_40;
	Pint64  v_int_57;
};
Popt Struct_59 Opt_60;
Punion Union_66 {
	v_stringconst_63 Pfrom(".");
	v_stringconst_68 Pfrom(":");
	v_stringconst_73 Pfrom("_");
};
Pstruct Struct_78 {
	Union_66  v_union_66;
	Pint64  v_int_76;
};
Parray Array_35 {
	Struct_78[] : Plongest;
};
Popt PPip Opt_83;
Penum Enum_106 {
	centos4106 Pfrom("centos4"),
	rhel4106 Pfrom("rhel4"),
	RHEL4106 Pfrom("RHEL4"),
	pre5106 Pfrom("pre5"),
	rc1106 Pfrom("rc1"),
	p23106 Pfrom("p23"),
	el4106 Pfrom("el4"),
	EL4106 Pfrom("EL4"),
	c4106 Pfrom("c4"),
	EL106 Pfrom("EL")
};
Pstruct Struct_110 {
	Enum_106  v_enum_106;
	'.';
};
Popt Struct_110 Opt_112;
Pstruct Struct_100 {
	'.';
	Opt_112  v_opt_112;
};
\end{verbatim}
\end{minipage}
\begin{minipage}[t]{0.33\columnwidth}
\begin{verbatim}
Punion Union_113 {
	v_stringconst_95 Pfrom("E.");
	Struct_100  v_struct_100;
};
Pstruct Struct_116 {
	Union_113  v_union_113;
	Pint64  v_int_114;
};
Parray Array_89 {
	Struct_116[] : Plongest;
};
Penum Enum_133 {
	centos4133 Pfrom("centos4"),
	rhel4133 Pfrom("rhel4"),
	RHEL4133 Pfrom("RHEL4"),
	ent133 Pfrom("ent"),
	el4133 Pfrom("el4"),
	EL4133 Pfrom("EL4"),
	c4133 Pfrom("c4"),
	EL133 Pfrom("EL")
};
Penum Enum_144 {
	centos4144 Pfrom("centos4"),
	kb144 Pfrom("kb")
};
Pstruct Struct_146 {
	'.';
	Enum_144  v_enum_144;
};
Popt Struct_146 Opt_140;
Pstruct Struct_126 {
	'.';
	Enum_133  v_enum_133;
	Opt_140  v_opt_140;
};
Penum Enum_150 {
	nonptl150 Pfrom("nonptl"),
	EL4150 Pfrom("EL4")
};
Pstruct Struct_152 {
	'_';
	Enum_150  v_enum_150;
};
Punion Union_121 {
	Struct_126  v_struct_126;
	Struct_152  v_struct_152;
	PPstring  v_string_123;
	Pempty;
};
Pstruct Struct_154 {
	Opt_60  v_opt_60;
	Array_35  v_array_35;
	Opt_83  v_opt_83;
	'-';
	Pint64  v_int_91;
	Array_89  v_array_89;
	Union_121  v_union_121;
};
Popt PPstring Opt_163;
\end{verbatim}
\end{minipage}
\begin{minipage}[t]{0.33\columnwidth}
\begin{verbatim}
Penum Enum_174 {
	string174_0 Pfrom("CENTOS-0"),
	string174_1 Pfrom("p1-8"),
	string174_2 Pfrom("a-43"),
	string174_3 Pfrom("a-33"),
	string174_4 Pfrom("a-16"),
	string174_5 Pfrom("a-3")
};
Popt Enum_174 Opt_178;
Pstruct Struct_179 {
	Pint64  v_int_172;
	Opt_178  v_opt_178;
};
Penum Enum_182 {
	centos4182 Pfrom("centos4"),
	RHEL4182 Pfrom("RHEL4"),
	string182_2 Pfrom("EL-1"),
	EL4182 Pfrom("EL4")
};
Punion Union_180 {
	Struct_179  v_struct_179;
	Enum_182  v_enum_182;
};
Parray Array_156 {
	Union_180[] : Psep('.') && Plongest;
};
Pstruct Struct_223 {
	Pint64  v_int_158;
	Opt_163  v_opt_163;
	'.';
	Array_156  v_array_156;
};
Punion Union_155 {
	Struct_154  v_struct_154;
	Struct_223  v_struct_223;
};
Pstruct Struct_205 {
	Opt_22  v_opt_22;
	'.';
	Enum_31  v_enum_31;
	' ';
	Union_155  v_union_155;
};
Popt Struct_205 Opt_206;
Precord Pstruct Struct_219 {
	PPdate  v_date_1;
	' ';
	PPtime  v_time_6;
	' ';
	Enum_10  v_enum_10;
	": ";
	PPstring  v_string_17;
	Opt_206  v_opt_206;
};
Psource Parray entries_t {
	Struct_219[];
};
\end{verbatim}
\end{minipage}
}
\cut{ %%%%%%%%%%%%%%%%%%%%%%% 
\begin{centercode}
Penum Enum_10 \{
  Installed10 Pfrom("Installed"),
  Updated10 Pfrom("Updated"),
  Erased10 Pfrom("Erased")
\};
Popt \kw{Pstring_ME}(:"/\\+\\+/":) Opt_22;
Penum Enum_31 \{
  x86_6431 Pfrom("x86_64"),
  noarch31 Pfrom("noarch"),
  i68631 Pfrom("i686"),
  i38631 Pfrom("i386")
\};

... /* Complex definition of types 
       Struct_154 and Struct_233 
       (\kw{141 lines of code}) ommitted */

Punion Union_155 \{
  Struct_154  v_struct_154;
  Struct_223  v_struct_223;
\};
Pstruct Struct_205 \{
       Opt_22  v_opt_22;
  '.'; Enum_31  v_enum_31;
  ' '; Union_155  v_union_155;
\};
Popt Struct_205 Opt_206;
Precord Pstruct entry_t \{
        \kw{Pdate}  v_date_1;
  ' ';  \kw{Ptime}  v_time_6;
  ' ';  Enum_10  v_enum_10;
  ": "; \kw{Pword} v_string_17;
        Opt_206  v_opt_206;
\};
Psource Parray entries_t \{
        entry_t[];
\};

\end{centercode}
}%%%%%%%%%%%%%%%%%%%%%%%
\caption{\pads{} description of {\tt yum.txt} inferred by the original \learnpads}\label{fig:yum-bad}
\end{figure}
}

%In contrast to the compact -defined description, 
%\figref{fig:yum-bad} shows the description of {\tt
%  yum.txt} inferred by the original \learnpads{} system
Unfortunately, when we ran our original format
inference algorithm~\cite{fisher+:dirttoshovels}
on this data source, rather than inferring a compact 23-line
description, our algorithm returned a verbose 179-line description
that was difficult to understand and even harder to work with.
%.  This description,
%though it correctly parses the \texttt{yum} data, is incredibly
%verbose, taking 179~lines compared to the 23~lines required by the 
%human expert.  
After investigation, we discovered the problem.  The data can be
tokenized in many ways, and the inference system was using a set of
regular expressions to do the tokenization that was a poor match for
this data set. 
 More concretely, consider 
the string ``{\tt 2.2.13-4}.'' This string may be parsed by 
any of the following token sequences: {\small
\begin{verbatim}
Option 1: [int] [.] [int] [.] [int] [-] [int]
Option 2: [float] [.] [int] [-] [int]
Option 3: [int] [.] [float] [-] [int]
Option 4: [id]
\end{verbatim}
}\normalsize
The best choice for this format is Option 4, \cd{id},
because \cd{id} can be used to parse the data found at this point in
all lines of the {\tt yum} format.  Unfortunately, the simplistic 
disambiguation rules for the original system chose Option 2.  Moreover,
other lines are tokenized in different ways.
%{\em different choices are made over and over again.
%bad local choices like this have a snowball effect on description 
%complexity because they cannot be merged with other descriptions
%of ``similar'' parts of the data.  
For instance, {\tt dhcp-devel}, which also could have been an {\tt id}
is tokenized as {\tt [word]} and 
{\tt 0.9.7a-43.8} is tokenized as
{\tt [float] [.] [int] [char] [-] [float]}.  As each distinct
tokenization of similar data regions is introduced, the inference
engine attempts to find common patterns and unify them.  However,
in this case, unification was unsuccessful and the
result was an overly complex format.
  
% this
% it made
% with the automated inference engine ({\em i.e.,} tokens)
% to use as building blocks in the description.  

% For example, rather than
% using the \cd{id} token, which describes any
% sequence of letters, numbers, and punctuation starting with a letter
% and ending with a space, it built a very complex structure 
% involving nested unions, structures and arrays of  
% used simpler tokens such as \cd{word}, \cd{int}, and individual
% punctuation symbols.
% %to represent the package names and version numbers. 
% %This choice reflected the complex structure within the package names 
% %and versions into the description.  
% The structure it inferred
% is semantically equivalent but far, far more verbose.

% Why did the old inference algorithm avoid the \cd{id} token? 
% The definition of a complex token like \cd{id} includes that
% of other simple tokens in the system such as \cd{word} and
% \cd{int}. This situation leads to the {\em token ambiguity problem}: 
% given an input string, there are multiple ways to map it into tokens.
% such ambiguiuty occurs in numerous places in {\tt yum.txt}.  For instance,
% the string ``{\tt 2.2.13-4}'' can correspond to any of the following
% token sequences: {\small
% \begin{verbatim}
% Option 1: [int] [.] [int] [.] [int] [-] [int]
% Option 2: [float] [.] [int] [-] [int]
% Option 3: [int] [.] [float] [-] [int]
% Option 4: [id]
% \end{verbatim}
% }

% % Similarly, the string ``{\tt Dec 10 04:07:51}'' can be interpreted 
% % as any of these token sequences, among others:

% % {\small
% % \begin{verbatim}
% % Option 1: [word] [' '] [int] [' '] [int] [:] [int] [:] [int]
% % Option 2: [month] [' '] [int] [' '] [time]
% % Option 3: [date] [' '] [time]
% % \end{verbatim}
% % }

The original inference algorithm disambiguates between overlapping
tokens by using the same strategy as common lexer-generators:  It
tries each token in a predefined order 
and picks the first, longest token that matches.
% leads to a successful
% tokenization of the full input.
While effective for some data sources, this simple policy 
%is not ideal because it makes greedy decisions that may be sub-optimal,
%it 
makes fixed tokenization decisions up front,
does not take contextual information into account, and 
restricts the use of complex tokens like {\tt id}, {\tt url} and 
{\tt message} that {\em shadow} simpler ones.

% In the next section, we propose a revised inference algorithm
% that addresses these limitations by
% using statistical models to assign probabilities to possible token sequences,
% defering tokenization decisions, making them
% incrementally as the structure is constructed, and
% taking the context into account when deciding which tokens to use.
