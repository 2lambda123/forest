\section{Introduction}
\label{sec:intro}

%% WHAT IS AD HOC DATA?

An {\em ad hoc data format} is any non-standard data format for which
parsing, querying, analysis, or transformation tools are not readily
available.  Despite the increasing use of standard data formats such
as \xml{}, ad hoc data sources continue to arise in numerous
industries such as finance, health care, transportation, and
telecommunications as well as in scientific domains, such as
computational biology and chemistry.  The absence of tools for
processing ad hoc data formats complicates the daily data-management
tasks of IT professionals, who may have to cope with numerous ad
hoc formats even within a single application.  

Common characteristics of ad hoc data complicate the building of tools
to perform even basic data-processing tasks.  Documentation of ad hoc
formats, for example, is often incomplete or inaccurate, making it
difficult to define a database schema for the data or to build a
reliable data parser.  The data itself often contains numerous kinds
of errors, which can thwart standard database loaders.  Surprisingly,
few meta-language tools, such as data-description languages or parser
generators, exist to assist in management of ad hoc data.  And
although ad hoc data sources are among the richest for database and
data mining researchers, they often ignore such sources as the work
necessary to clean and vet the data is prohibitively expensive.

\cut{Before the data can be processed
reliably by an application, errors must be identified, filtered out,
or corrected, and the cleansed data must be transformed into a
standard loading format.
such as loading ad hoc data into a database system. 

Once in a standardized format, the data can be directly loaded into a
database or manipulated by standard, widely available tools.}

%% EXAMPLE SOURCES AND CHARACTERISTICS

\cut{\xml{}, for
example, is not ad hoc, because numerous, standard tools for parsing,
querying, and transforming exist for \xml{}.
There are vast amounts of useful data stored in traditional databases
and \xml{} formats, but there is just as much in ad hoc formats.}

The variety of application domains, format characteristics, sources of
errors, and volume of data makes processing ad hoc data both
interesting and challenging.  \figref{figure:data-sources} summarizes
several ad hoc sources from the networking and telecommunication
domains at AT\&T and from computational biology applications at
Princeton.  Formats include ASCII, binary, and Cobol, with both fixed
and variable-width records arranged in linear sequences and in
tree-shaped or DAG-shaped hierarchies.  Even within one format, there
can be a great deal of syntactic variability.  For example,
\figref{fig:darkstar-records1} contains two records from the
network-monitoring application.  Note that each record has different
number of fields (delimited by '$|$') and that individual fields contain
structured values (e.g., attribute-value pairs separated by '=' and
delimited by ';').

%% WHY IS PROCESSING IT HARD?
%% No control of source or target format
Unfortunately, data analysts have little control of the format of
ad hoc data at its source nor at its final destination, for
example, in a database.  The data arrives ``as is'', and the analyst
who receives it can only thank the supplier, not request a more
convenient format.  Once an analyst has the data, a common task is
converting the source format to a standard database loading format.
This transformation proceeds in three stages.  First, the analyst
writes a parser for the ad hoc format, using whatever (in)accurate
documentation may be available.  Second, he writes a program that 
detects and handles erroneous data records, selects records
of interest, and possibly normalizes records into a standard format,
for example, by reordering, removing, or transforming fields.
Unfortunately, the parsing, error handling, and transformation code is
often tightly interleaved.  This interleaving hides the knowledge of
the ad hoc format obtained by an analyst and severely limits the
parser's reuse in other applications.

\begin{figure*}
\begin{center}
\scriptsize
\begin{tabular}{@{}|l|l|l|}
\hline
\textbf{Name:} Use & Record Format (Size) 
%& Size
           & Common Errors \\ \hline\hline
\textbf{Web server logs (CLF):}           & Fixed-column ASCII & Race conditions on log entry\\ 
Measuring Web workloads  & ($\leq$12GB/week)  & Unexpected values\\ \hline
\textbf{AT\&T provisioning data (\dibbler{}):} & Variable-width ASCII & Unexpected values \\ 
Monitoring service activation  & (2.2GB/week) & Corrupted data feeds \\ \hline
\textbf{Call detail:}                   & Fixed-width binary &  Undocumented data\\
Fraud detection                         &   (\appr{}7GB/day) & \\ \hline 
\textbf{AT\&T billing data (\ningaui{}):}      & Cobol      & Unexpected values\\ 
Monitoring billing process  &  ($>$250GB/day) & Corrupted data feeds \\ \hline
\textbf{IP backbone data (\darkstar{}):}  & ASCII & Multiple representations \\
{Network Monitoring}       &  ($\ge$ 15 sources,\appr{}15 GB/day)  & of missing values \\
          & & Undocumented data \\ \hline
\textbf{Netflow:}               & Data-dependent number of & Missed packets\\ 
{Network Monitoring}  & fixed-width binary records & \\ 
                      & ($\ge$1Gigabit/second) & \\ \hline
\textbf{Gene Ontology data:}    & Variable-width ASCII & \\
Gene-gene correlations in Magic & in DAG-shaped hiearchy & \\\hline
\textbf{Newick data}              & Fixed-width ASCII & Manual entry errors \\
Immune system response simulation & in tree-shaped hierarchy 
& \\
\hline
\end{tabular}
\normalsize
\caption{Selected ad hoc data sources.}
\label{figure:data-sources}
\end{center}
\end{figure*}

\begin{figure}
  \centering
  \small
\begin{verbatim}
 2:3004092508||5001|dns1=abc.com;dns2=xyz.com|c=slow link;w=lost packets
 |INTERNATIONAL
 3:|3004097201|5074|dns1=bob.com;dns2=alice.com|src_addr=192.168.0.10;
 dst_addr=192.168.23.10;start_time=1234567890;end_time=1234568000;
 cycle_time=17412|SPECIAL
\end{verbatim}  
  \caption{Simplified network-monitoring data. Newlines 
inserted for legibility.}
  \label{fig:darkstar-records1}
\end{figure}

\cut{In addition to poor documentation and error-prone data sources, other
common characteristics of ad hoc data make basic processing tasks
challenging.}

\cut{A common phenomenon is for a
field in a data source to fall into disuse.  After a while, a new
piece of information becomes interesting, but compatibility issues
prevent data suppliers from modifying the shape of their data, so
instead they hijack the unused field, often failing to update the
documentation in the process.}

%% Sources, meaning, and handling of errors
Another challenge is handling the variety of errors and the
variety of application-dependent strategies for handling errors in ad
hoc data.  Some common errors, listed in ~\figref{figure:data-sources},
include undocumented data, corrupted data, missing data, and multiple
representations for missing values.  Some sources of errors
that we have encountered in ad hoc sources include malfunctioning
equipment, race conditions on log entry~\cite{wpp}, presence of
non-standard values to indicate ``no data available,'' human error
when entering data, and unexpected data values.  A wide range of
responses are possible when errors are detected, and they are highly
application dependent.  Possible responses range from haulting processing
and alerting a human operator, to partitioning erroneous from error-free
records for examination off-line, to simply discarding erroneous or
unexpected values.  One of the most challenging aspects of processing
ad hoc data is that erroneous data is often more important than
error-free data, because it may indicate, for example, that two
systems are failing to communicate.  Writing code that is reliably
\emph{error aware}, however, is difficult and tedious.

\cut{Another notable characteristic of
these data sources is their large size: Web server logs, for example,
can reach 12GB per week and net-flow applications over one Gigabit
\emph{per second}.}

%% High-volume
The high volume of ad hoc data sources is another challenge.
~\figref{figure:data-sources} gives the volume of several sources.
AT\&T's call-detail stream, for example, contains roughly 300~million
calls per day requiring approximately 7GBs of storage space.  Although
this data is eventually archived in a database, data analysts mine it
profitably before such archiving~\cite{kdd98,kdd99}.  More
challenging, the \ningaui{} project at AT\&T accumulates billing data
at a rate of 250-300GB/day, with occasional spurts of 750GBs/day, and
netflow data arrives from Cisco routers at rates over a Gigabit per
second~\cite{gigascope}!  Such volumes require that the data be
processed without loading it into memory all at once.  Not
surprisingly, flexible error-response strategies are especially
critical with high-volume sources, so that error detection does not
hault or delay normal processing.

%% EXISTING SOLUTIONS

% Contivo, Pervasive, SAS ... We need to say something for real in the
% final paper. 
Commercial data-management products for ad hoc data address 
some of these problems, but to our knowledge, none can handle all the variability
in formats that we have encountered, nor do they support error-aware
processing of high volume sources.  Without tools adequate to the
task, analysts often write custom programs in \C{} or \perl{}.
Unfortunately, writing parsers, transformers, and printers by hand is
tedious and error-prone.  These tasks are complicated by lack of
documentation, convoluted encodings designed to save space, the need
to produce efficient code, and the need to handle errors robustly to
avoid corrupting down-stream data.  Moreover, the parser writers'
hard-won understanding of the data ends up embedded in parsing code,
making long-term maintenance difficult for the original writers and
sharing the knowledge with others nearly impossible.

%% OUR SOLUTION

Our answer to processing ad hoc data is \datatype{}, a functional
programming language with extensive support for describing and
transforming ad hoc data.  \datatype{} is a synthesis, not merely a
combination, of language constructs for data description and
transformation.  The iterative process of writing a \datatype{} data
description, parsing ad hoc data, and refining the description results
in a ``living documentation'' of the data source that can be used in
other \datatype{} programs.  More significantly, the meta-data
acquired during parsing of data is not lost once parsing is finished,
but instead plays a prominent role in the programmatic elements of the
language.  As we are particularly concerned with error-related
meta-data, we call this synthesis \emph{error-aware computing}.

%% CONTRIBUTIONS

At its core, \datatype{} is a functional language with standard
features such as pattern matching and higher-order functions, which we
find are critical to supporting data-driven transforms. In addition,
\datatype{} allows programmers to enforce semantic constraints on
data by using \datatype{} descriptions as a special form of runtime
contracts. 

The basic values of the language will be pairs of data
items and their corresponding meta-data. For each data item, the meta
data will include, among other things, descriptions of the error
content of the data item. Through the type system, the language will
ensure that data and associated meta data are kept in sync. This
property (which we call ``error-aware computing'') enables three
critical language features.
\begin{enumerate}
\item Safe, error-transparent transformations
\item Error querying - analyst can extract detailed picture of data
  error profile.
\item Flexible, programmatic repair of faulty records - analyst can
  choose when in the processing stream to address errors, rather than
  being forced to drop all faulty records at the beginning of the process.
\end{enumerate}

~\cite{fisher+:pads}
  
Before we talked about two contributions:-- a universal data
description language based on polymorphic, recursive anddependent data
types-- the transformation languageThere might be a third:--
collection and analysis a variety of different examples of ad hoc
datafrom widely differing domains (networking to genomics).  The
analysis of thedifferent sorts of ad hoc data is a substantial
contribution.  It helps usunderstand what is required and what is not.

\subsection{\datatype{} Architecture}

\begin{figure}[tp]
  \includegraphics[height=3in,width=5in]{architecture}
%  \includegraphics{architecture.eps}
\label{fig:pads-arch}
\caption{The \pads{} Architecture}
\end{figure}

In the next section, we will describe, in detail, the \pads{} approach
to data and meta data (inherited by \datatype{}), the \datatype{}
syntax for data description, and illustrative examples. Next, in
section~\ref{sec:data-transformation} we will elaborate on
\datatype{}'s support for data transformation, including design,
syntax, brief overview of the semantics and some examples.
Section~\ref{sec:implementation-techniques} will discuss our proposed
implementation techiniques, and section~\ref{sec:related-work} the
related work. A discussion of conclusions and future work is included
in section~\ref{sec:conclusion}.

% The goal of this paper is to describe a new language for efficient and
% reliable computing with ad hoc data.  More specifically, we describe a
% high-level programming language, \datatype{}, that comes with
% intrinsic support for processing ad hoc data.  Our programming
% language will use the rich data descriptions both as directives for
% parsing ad hoc data sources and as types for describing
% representations of ad hoc data within the programming environment.  In
% addition, a critical facet of our language will be its support for
% {\em error-aware computing}.  Our error-aware infrastructure will
% allow programmers to conveniently verify correctness of data relative
% to a description or alternatively detect data errors and handle them
% in domain-specific ways.  Finally, we will be sure our language design
% is founded on strong programming language principles by studying its
% type system and metatheory extensively.

Solution: Data description Language + transform language
Combine data description language with a transform language.

\pads{} is bound to C. However, functional programming languages are
better suited to the task of data tranformation. We therefore propose
a new language, \datatype{}, that combines a data description language
based on the style of ML types (and datatypes) with a functional data
transformation language. 

Our contributions are twofold. First, we propose an ML-style syntax
for the \pads{} data description language, based on type definitions
and {\em(polymorphic coming soon?)} parameterized recursive datatypes
(support for recursive datatypes is new to the \pads{} language, based
on recent results in our other work). This new syntax has a number of
advantages. It is more concise than the C-style encoding and more
naturally suited to describing recursive datatypes. Most importantly,
the new syntax is much more appropriate for transformation language
proposed below, which is based heavily on pattern-matching and data
constructors.

Our second contribution is the data transformation language itself.
{\em \pads{} is not a programming language.}  It merely generates
libraries that can be used by \C{} programmers.  \C{} is a very
low-level language that makes transforming ad hoc data awkward,
cumbersome and potentially error-prone.  More importantly, it provides
no intrinsic support for dealing with the errors that appear in ad hoc
data.  In addition, \C's type system and operational model provide no
support for checking the rich invariants found in ad hoc data either
at run time or at compile time.  In contrast, \datatype{} is a
high-level language with an elegant and convenient syntax for
data-driven programming, intrinsic support for handling errors and
intrinsic mechanisms for checking data invariants at run time {\em
  coming soon: and a sophisticated type system for enforcing data
  invariants at compile time}. 
