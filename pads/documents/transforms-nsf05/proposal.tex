\documentclass[11pt]{article}
\usepackage{times,url}
\usepackage{code} 
%\usepackage{proof}
%\usepackage{newcode}
\usepackage{epsfig}

\input{defs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\voffset             0in    %  top vertical offset
\hoffset             0in    %  left horizontal offset
\oddsidemargin       0pt    %  Left margin on odd-numbered pages.
\evensidemargin      0pt    %  Left margin on even-numbered pages.
\topmargin           0pt    %  Nominal distance from top of page to top of
\headheight          0pt    %  Height of box containing running head.
\headsep             0pt    %  Space between running head and text.
\textwidth         6.5in    %  Width of text on page
\textheight          9in    %  Height of text on page
\setlength{\parskip}{.05in}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\textfraction}{0.1}
\renewcommand{\baselinestretch}{1.01}
%\input{macro}  %%%  other macro definitions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\setcounter{page}{1}
\pagenumbering{roman}
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{NSF 05-576 \datatype{}: Language Support for Processing Ad Hoc Data}
%%%%%%%
\input{summary}
\newpage
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Table of Contents}
% \tableofcontents\
% \vspace*{.1in} 
% \begin{center}
% {\Large\bf\em{}This is a place-holder. Fastlane generates this page automatically.}
% \end{center} 
% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{arabic}
\setcounter{page}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Description}

\subsection{Introduction}
\label{ssec:intro}

An {\em ad hoc data format} is any nonstandard data format for which
parsing, querying, analysis or transformation tools are not readily
available.  \xml{} is not an ad hoc data format --- there are hundreds
of tools for reading, querying, and transforming \xml{}.  However,
computational biologists, chemists and physicists, computer programmers,
Wall Street traders, healthcare and airline information systems managers,
corporate IT professionals and 
others deal with ad hoc
data in a myriad of complex formats on a daily basis.
This data is often unpredictable, poorly documented, and
filled with errors, making it extremely difficult
to deal with.  Often, before anything can be done with
ad hoc data one must normalize its shape, eliminate errors
where possible and transform it into a standardized format. 
Once in a standardized format, the data
can be directly loaded into a database or manipulated by
standard, widely available tools.  

The goal of this research is to develop a new platform for
efficient and reliable computing with ad hoc data.
More specifically, we propose the following comprehensive 
research agenda.

\begin{enumerate}
\item We will design, study and implement
a {\em universal ad hoc data description language.}
Our descriptions will be capable of
concisely and accurately describing both the basic syntactic structure
as well as the deeper semantic properties of any ad hoc data source.
We will study of the semantic properties of our description language
and implement a compiler that can
generate parsers and printers 
automatically from data descriptions.

\item We will design, study, and implement  \datatype{}, a 
high-level programming language {\em with 
intrinsic support for 
processing ad hoc data.}  Our programming language will use the
specifications from our universal data description language 
both as directives for
parsing ad hoc data sources and as types for describing
representations of ad hoc data within the programming environment.  
In addition,
a critical facet of our language will be its support for
{\em error-aware computing}.  Our error-aware infrastructure 
will allow programmers to conveniently
verify correctness of data relative to a description, or 
alternatively, to detect data errors and handle them in domain-specific
ways.  Finally, we will be sure
our language design is founded on
strong programming language principles by
studying its type system and metatheory extensively. 

\item We will evaluate \datatype's design by implementing 
applications in at least three completely different domains.  First, we will
develop tools for cleaning, normalizing, analyzing and mining
telecommunications data supplied by AT\&T.  This will allow
us to evaluate the performance of our tools on truly
massive data sets (gigabytes or even terabytes in size).  
Our research in this context also has the 
long-term potential of making
a broad impact on society by improving the reliability of network
monitoring and fraud detection tools
used in the telecommunications industry.  Second, we 
will develop tools for processing data sources of 
interest to computational biologists and genomics researchers.  
In particular, we will collaborate with
Olga Troyanskaya from Princeton's Lewis-Sigler Institute for 
Integrative Genomics and Steven Kleinstein, program coordinator of Princeton's
Picasso project for interdisciplinary research.
Together, we will produce documentation and tools for the
ad hoc data of most use to computational biologists.
Third, we will study formats for medical data including
formats for lab results, medical diagnoses and clinical notes.
Medical data is often rife with errors and it is estimated that
up to 300,000 people in the US die each year because doctors 
do not have correct patient information~\cite{rudlin:pc}.
We will develop precise \datatype{} descriptions for medical data formats and
explore techniques for creating tools capable of handling erroneous data.

% This collaboration will allow us to make a broad impact on
% research in the natural sciences.
% Troyanskaya is in the process of developing Magic~\cite{magic}, a database 
% that stores information on gene-gene interactions
% and needs to load that database with information extracted from
% a variety of other sources including, among others,
% the Gene Ontology database GO~\cite{go}.  Kleinstein is currently working on
% a simulator for understanding immune-system responses and is in need of tools for
% loading ad hoc data into his simulator. 

\item We will engage Princeton's undergraduate students in 
a series of independent projects based around the the development and use of
\datatype.  Potential projects include (1) cross-disciplinary
research in computer science and biology by building the ad hoc
data processing tools needed for biological research at 
Princeton, (2) cross-disciplinary research in computer science and medicine
by writing precise \datatype{} descriptions for medical data, and creating
tools for 
error detection and correction (3) building a graphical, structured editor 
so non-programmers can
describe, parse, print, edit and search data any ad hoc data source
at a high level of abstraction,
(4) interfacing \datatype{} with other common languages including
Java, C\#, Perl and Python,
(5) tuning performance on massive data sets, and
(6) heuristics for automatically guessing data 
descriptions for simple tabular data.  Three
Princeton undergraduates Mark Daly, Jon Epstein and Michael Ten-Pow
have already expressed interest in these projects for the upcoming year.
\end{enumerate}


% We have already begun to develop a system called \pads{} (Processing
% Ad hoc Data Sources) that helps to address some of these concerns.  It
% includes a high-level, declarative language for describing ad hoc data
% formats and it is possible to generate tools for parsing data,
% printing statistical summaries of data and transforming data into
% \xml.  We propose to improve and generalize our work in several
% important ways.

% \begin{enumerate}
% \item The current \pads{} system is far from a universal
% data processing platform.  There are many commonly-used
% data formats \pads{} cannot currently handle.  We propose
% a number of innovative ways of extending the \pads{} description language
% and compiler to alleviate these deficiencies.
% \item The current \pads{} tool-generation infrastructure is brittle and
% inflexible, and the generated tools have sub-par performance.  We propose
% to improve the tool-generation infrastructure by augmenting \pads{}
% with a system of user-defined attributes that can communicate
% semantic information to the tool generator and implementing a mechanism for 
% application-specific customization.
% \item The \pads{} description language has no specification of its 
% meaning or properties.  We propose to define a formal semantics for 
% \pads{}, prove properties of its data processing algorithms and
% use our formal model to re-evaluate, debug and generalize our 
% implementation.  
% \item In addition to our work on using \pads{} to implement applications 
% in the networking and telecommunications domains, part of our mission
% is to have a broad impact on researchers who do data processing
% in the natural sciences, including physics, biology and chemistry.
% We have already developed a partnership with
% Olga Troyanskaya in Princeton's Lewis-Sigler Institute for 
% Integrative Genomics
% and are studying her data processing problems.  We believe that 
% we have a tremendous
% opportunity to make an impact on the productivity of 
% genomics researchers at Princeton
% and elsewhere by supplying them with easy-to-use tools produced by our system.
% \item To extend and support our relationship with the Genomics Institute,
% and to have a broad impact on interdisciplinary education,
% we plan to develop undergraduate research projects in which
% computer science majors use \pads{} to help biologists 
% with their data processing problems.  
% \end{enumerate}

Overall, our research will combine novel language design ideas and
strong theoretical analysis with a serious implementation.  We will
deliver a programming system with the potential to substantially
increase the overall productivity of data analysts, 
information systems administrators
and programmers who
deal with ad hoc data regularly.  If funded, 
our research also has the potential to have  a
broad impact on the telecommunications industry, on
research in computational biology and other sciences,
and on error detection and correction in 
medical data, potentially helping save the lives of
the over 300,000 people per year that die because their doctors
receive incorrect data~\cite{rudlin:pc}.

\subsubsection{Experience and prior work with ad hoc data.}
\datatype's language design and implementation will build upon the experience
we have gained designing and implementing the \pads{}
system~\cite{fisher+:pads,pads-website}.  The \pads{} system 
is freely available software we have developed that allows
users to write \C-like type declarations and interpret them as
descriptions of ad hoc data.  The \pads{} compiler generates libraries
for parsing and printing from these descriptions.  
While \pads{} is an excellent first step in the design of
a language for processing ad hoc data, the experience we have
gained developing and using it demonstrates clearly that 
it has many deficiencies and drawbacks.  Our current research 
will correct these problems with a vastly improved
design and implementation.

We would like to highlight two core differences between \pads{} and 
\datatype{} designs.  The first critical difference is that 
\datatype's data description language will be based on
polymorphic, recursive and dependent datatypes inspired by
functional programming languages such as ML and Haskell.  In contrast,
\pads{} specifications are based on \C{} structs, arrays
and unions.  By adding recursion, a feature not present in
\pads, we will be able to write descriptions for a
number of important tree-structured formats used by computational 
biologists~\cite{geneontology,newick}.  
By adding polymorphism, another feature not present in \pads,
we will be able to build reusable specification
libraries that will help make descriptions more concise and 
programmers more productive.  In addition, our initial
experiments indicate that \datatype{} descriptions are anywhere from
10\% to 40\% more concise than the current \pads{} descriptions.  Most
importantly, \datatype{} descriptions will be a much better fit for the
light-weight data transformation language we propose to develop.  

The second critical difference is that {\em \pads{} is not a programming
language.}  It merely generates libraries that can be used by \C{}
programmers.  \C{} is a very low-level language that makes
transforming ad hoc data awkward, cumbersome and potentially
error-prone.  More importantly, it provides no intrinsic support for
dealing with the errors that appear in ad hoc data.  In addition, \C's
type system and operational model provide no support for checking the
rich invariants found in ad hoc data either at run time or at compile
time.  In contrast, \datatype{} will be a high-level language with an
elegant and convenient syntax for data-driven programming, intrinsic
support for handling errors, intrinsic mechanisms for checking data
invariants at run time and a sophisticated type system for enforcing
data invariants at compile time.  Overall,
programs written in \datatype{} will be more concise, more reliable,
easier to understand,
easier to maintain, and easier to evolve as data formats evolve
than programs written in \C.

However, the \pads{} implementation will certainly not go unused.  It 
is a well-engineered
and efficient parser generator with support for massive data sources.
We plan to implement \datatype{} directly on top of \pads{}, extending \pads{}
with new functionality where necessary to support the novel features 
of \datatype.  Using \pads{} directly as a back-end for our
new system will greatly speed up our development process.
Overall, due to our past experience with \pads{} and
our ability to leverage the current implementation in our new design,
we are able, and in fact uniquely qualified, to take on this crucial
research challenge.

\subsubsection{Current solutions to the problem of ad hoc data.}

Given the importance of ad hoc data, it is perhaps surprising that
more tools do not exist to solve it.  The vast majority of tools simply assume
that the data they manipulate {\em is in the right format from the
beginning.}  If it is not, it is up to the user to get the data in the
correct format themselves --- he or she receives little or no help
with the problem.  For instance, \xml{} and relational databases expect
their inputs are already in \xml{} or standard formats such as CSV
(Comma-Separated Values).  Similarly, languages for transforming
\xml{} such as XDuce~\cite{hosoya+:xduce-journal}, 
Cduce~\cite{benzaken+:cduce}, and 
Xtatic~\cite{gapeyev+:XtaticRuntime}, to name just a few,
assume (of course!) their inputs are \xml.
\datatype{} is completely complementary to this work.
\datatype{} is designed to be used on the buggy, non-uniform
ad hoc data that we {\em wish} was formatted as well-formed 
\xml, but to our dismay {\em is not}.
One can easily imagine building cooperative systems 
in which \datatype{} is used as a front-end to translate
ad hoc data into \xml{} and an \xml-based tool is used as a back-end.
The same kind of cooperative relationship might hold with the 
Harmony~\cite{foster+:lenses}, a tool for synchronizing 
disparate views of the same logical data source.  One can 
imagine using \datatype{} as a front end that translates data into 
a format Harmony can understand whenceforth Harmony uses
its technology for synchronization.  

One must also not confuse the goals and achievements of systems such as
ASN.1~\cite{asn} and ASDL~\cite{asdl} with our proposed 
\datatype{} system.  ASN.1 
allows the user to specify a logical in-memory
representation and generates an ASN.1 physical, on-disk format. 
\datatype{} is intended to help with the {\em inverse} problem.
\datatype{} allows users to specify arbitrary, physical, on-disk 
formats and generates
a parser that translates the given on-disc format into a 
logical in-memory representation.  \datatype{} could help translate
ad hoc data into the ASN.1 on-disk format --- ASN.1 cannot do this
itself.

% The development of \datatype{} is
% completely complementary to research on relational and semi-structured
% databases and on research in \xml{} programming.
% These systems assume that their inputs are \xml{} (of course!)
% as \datatype will be explicitly designed to help with the
% problem of loading one of these databases with data that {\em is not
% currently in the expected format.}

One might wonder why we do not choose to use standard compiler tools based on
regular expressions or context-free grammars like Lex and Yacc.
First, regular expressions and context-free grammars, while excellent
formalisms for describing programming language syntax, are not ideal
for describing the sort of ad hoc data we discuss in this proposal.
One reason is that regular expressions and context free grammars
do not have the dependency and semantic constraints that occur commonly
in ad hoc data formats.  For instance, one often reads a number
$i$ that expresses the fact that the following data will contain
a sequence of $i$ records.  It is not easy to express such constraints
in Lex or Yacc.  A second reason
is that Yacc, for instance, supports non-deterministic choice
between grammar rules.  Our experience indicates that
this feature is not necessary for describing ad hoc data.
Moreover, it leads to ambiguous grammars and
complex, hard-to-understand error messages.
Anyone who has tried to debug the hundreds of shift-reduce
and reduce-reduce errors that inevitably come up when designing 
parsers using Yacc will know what I mean!
A third reason is that these tools only support ASCII data sources
and provide no help building in-memory representations for the programmer
or data transformations.  Consequently, in our experience, programmers
{\em never} use Lex and Yacc to build parsers for ad hoc data.
% For instance, many ad hoc data sources we have studied include 
% Lex and Yacc are
% both over- and under- kill.  Overkill because the division into a
% lexer and a context free grammar is not necessary for many ad hoc data
% sources, and under-kill in that such systems require the user to build
% in-memory representations manually and support only ASCII sources.  

The most common tools programmers use to manipulate ad hoc data
are programming languages such as Perl and C.  
Unfortunately, writing parsers, transformations and printers this way
is tedious and error-prone.  Programmers rarely insert all of the
necessary error-handling code and if they do, the error-handling
code completely dominates the parser, obscuring the main algorithm
for parsing correct data.  In contrast, the \datatype{} compiler
will automatically insert all necessary error detection and recovery code.
This dramatically improves the reliability, and especially security,
if that is a factor, of the system.  Moreover, Perl and C parsing
and printing programs are very hard to understand and modify
after-the-fact.  The original parser-writer's hard-won
understanding of the data ends up embedded in parsing code, making
long-term maintenance difficult for the original writers and sharing
the knowledge with others nearly impossible.  Again, in contrast,
\datatype{} descriptions, are high-level specifications that are
uncluttered with algorithmic details, are easy to read and 
easy to change.

The most closely related work includes 
\erlang{}'s bit syntax~\cite{erlang} and the
the \packettypes{}~\cite{sigcomm00} and
\datascript{} languages~\cite{gpce02}, 
all of which allow declarative descriptions of physical data.  
These projects were motivated by parsing networking protocols,
\textsc{TCP/IP} packets, and \java{} jar-files, respectively.  Like
\pads{} and \datatype, these languages have a type-directed approach to
describing ad hoc data and permit the user to define semantic constraints.
In contrast to our proposed
work, these systems handle only binary data,
are missing features such as recursion or polymorphism, and assume the data is
error-free or halt parsing if an error is detected. 
Parsing non-binary data poses additional challenges because of the need
to handle delimiter values and to express richer termination conditions
on sequences of data.  In general, it is often the case that
ASCII data formats have a different structure and properties from binary
networking data.  Recursion is essential for parsing hierarchical
data.  While hierarchical data does not seem to appear in networking 
applications (and hence recursion is
unnecessary in \erlang, \packettypes{} and \datascript),
it is common in data used by scientists.  Our analysis also
indicates that polymorphism is useful for defining reusable
format abstractions and libraries.  Finally, and most 
importantly, since ad hoc data sources
such as medical records are often rife with errors
of all different kinds, support for processing error-filled data is
essential.  We propose to include such support in \datatype.

\paragraph*{Proposal Structure.}
In the following section, we will explain the characteristics
of ad hoc data in more detail.  This should help readers further understand
the immense challenges and risks posed by ad hoc data.  
In Section~\ref{sec:datatype-overview}, we
explain our vision of how \datatype{} can be used to help deal with these
challenges.  In Section~\ref{sec:universal}, we will outline a proposal
for a universal data description language and in Section~\ref{sec:language}
we will describe our plan for language support for computing with
ad hoc data.  In Section~\ref{sec:plan}, we make our research plan precise
and in Section~\ref{sec:impact}, we describe its broad impact.
%Section~\ref{sec:related} describes related work and
Section~\ref{sec:results} describes the PI's and senior personel's
results from prior research.

\subsection{The Challenges of Ad Hoc Data}
\label{sec:challenges}

There are vast amounts of useful data stored in traditional databases
and \xml{} formats, but there is just as much in ad hoc formats.
\figref{figure:data-sources} provides some information on ad hoc data
formats from the networking and telecommunications domain at AT\&T and
ad hoc data formats used by computational biologists at Princeton.
They include ASCII, binary, and Cobol data formats, with both fixed
and variable-width records arranged in linear sequences or in tree- or
DAG-shaped hierarchies.  The data sources range in size from
relatively small files up to Netflow
applications, which can be produced at over one GB per second.  Common
errors include undocumented, corrupted and missing data.

% We hope that these examples give the reader a beginning sense of the
% nature and pervasiveness of ad hoc data sources.  However, we cannot
% emphasize enough just how pervasive ad hoc data is and to what degree
% it infiltrates computer systems, businesses, government, and
% scientific research.  Remember, just about any program that spits out
% useful information in a format other than \xml{}, \textsc{html},
% \textsc{jpeg}, \textsc{mpeg} or a few others is generating ad hoc
% data.  Hence, most programmers deal with ad hoc data on a individual
% basis regularly.  The financial industry is awash with ad hoc data.
% Scientific instruments in biology, astrophysics and chemistry output
% ad hoc data.  Most daunting of all is perhaps our national
% health-care system.  Recently, Newt Gingrich and Hilary Clinton have
% jointly proposed we consolidate and integrate our healthcare
% records from across all systems in the United States
% in the next 10 years.  Such a massive project will undoubtedly
% require technical solutions at many levels.  At the lowest levels, we
% will need a reliable software to collect and transfer health care data
% from legacy systems into modern formats.  The \datatype{} system we
% propose will be an invaluable aid in the development of reliable software
% for this very task.

\begin{figure*}
\begin{center}
\begin{tabular}{@{}|l|l|l|l|}
\hline
Name: Use                           & Representation    
%& Size
           & Common Problems \\ \hline\hline
%Web server logs (CLF):                & Fixed-column      
%& $\leq$12GB/week 
%& Race conditions on log entry\\ 
%Measuring web workloads               & ASCII records     
%&                             
%& Unexpected values\\ \hline
AT\&T provisioning data (\dibbler{}): & Variable-width    
%& 2.2GB/week 
& Unexpected values \\ 
Monitoring service activation         & ASCII records     
%&            
& Corrupted data feeds \\ \hline
Call detail: Fraud detection                   & Fixed-width binary records  
%&\appr{}7GB/day 
&  Undocumented data\\\hline 
AT\&T billing data (\ningaui{}):      & Cobol  
%& \appr{}4000 files/day, 
& Unexpected values\\ 
Monitoring billing process   &                             
%& 250-300GB/day    
& Corrupted data feeds \\ \hline
IP backbone data (\darkstar{})  & ASCII  
%& $\ge$ 15 sources  
& Multiple missing-value rep's \\
Network Monitoring:  &        
%& \appr{}15 GB/day              
& Undocumented data \\ \hline
Netflow:                               & Data-dependent number of   
%& $\ge$1Gigabit/second  
& Missed packets\\ 
Network Monitoring        & Fixed-width binary records  
& \\ \hline
Gene ontology data:        & Variable-width ASCII records 
%& ? 
&  \\
Gene-gene correlations & in DAG-shaped hiearchy 
%&  
& \\\hline
Newick data:                          & Fixed-width ASCII record 
%& ? 
& Human entry errors \\
Immune system response simulation & in tree-shaped hierarchy 
%& 
& \\
\hline
HL7:                 
Medical lab results 
& Variable-width ASCII records 
& Human entry errors \\
&& Undocumented data \\
&& Wrong data encodings \\ \hline
CPT codes:
Medical diagnoses & Floating point numbers & Lack of encoding agreement\\
\hline
SnowMed: Medical clinic notes
& keyword tags \& free text
& 
\\ \hline
\end{tabular}
\caption{Selected ad hoc data sources from networking and telecommunications,
biological sciences and medical domains.}
\label{figure:data-sources}
\end{center}
\end{figure*}


% In this domain, it is possible to categorize sources of ad hoc data broadly
% as {\em online data} and {\em offline data}.  Online data
% is data sent over the wire that networking software actively
% reads, interprets and reacts to.  For instance, servers continuously
% listen for requests and react promptly to clients.  Intrusion detection 
% systems and performance evaluation systems
% monitor network activity continuously and signal administrators
% when they detect problems or anomalies.  On the other hand, offline data
% does not necessarily have to be processed in real time.  Offline
% data comes in the form of web server
% logs~\cite{wpp}, netflows capturing internet traffic~\cite{netflow},
% log files characterizing IP backbone resource utilization and telephony
% call detail data~\cite{hancock-toplas}, to name just a few.

% While offline and online ad hoc data have many features in common,
% they also have a few differences.  Software dealing with online data
% is amongst the most vulnerable software that we deploy on a network.
% Extraordinary lengths must be taken to ensure attackers, who can
% supply this software with unexpected data and can reach it in
% real-time, cannot exploit errors in processing code to take control of
% servers, performance monitors or intrusion detection software itself.
% A cautionary example of the dangers of online ad hoc data processors
% is the Ethereal system~\cite{ethereal}.  Ethereal is used by network
% administrators for monitoring, analyzing and troubleshooting networks.
% Unfortunately, like most network software, users have found a number
% of vulnerabilities in the software, and moreover many of these
% vulnerabilities are directly related to the mundane components of the
% system that parse ad hoc data as opposed to the parts of the system that
% perform higher-level tasks.  For instance, in March 2004, Stefan Esser
% posted an advisory on 13 different buffer overflow attacks on
% Ethereal~\cite{etherealvulnerabilities}.  Of the 13, 9 attacks
% occurred during parsing.  Therefore, in addition to all the problems
% associated with processing offline data, those who deal with
% online data must be cognizant of the substantial security risks
% inherent in each line of code they write.

Processing all this ad hoc data is challenging for a variety of reasons. 
First, ad hoc data typically arrives ``as is'': the analyst or system
that receives it can only say ``thank you,'' not request a more
convenient format.  Second, documentation for the format may not exist
at all, or it may be out of date.  A common phenomenon is for a field
in a data source to fall into disuse.  After a while, a new piece of
information becomes interesting, but compatibility issues prevent data
suppliers from modifying the shape of their data, so instead they
hijack the unused field, often failing to update the documentation in
the process.

Third, such data frequently contain errors for a variety of reasons:
malfunctioning equipment, non-standard values to 
indicate ``no data available,'' human error in
entering data, unexpected data values, confusion about
encoding standards, \etc{} The appropriate response
to such errors depends on the application.    
In the case of medical data, errors signal tremendous,
possibly life-threatening danger.  These cases deserve
human attention and immediate action must be taken.
In other applications, it may be possible to
repair the data, while still others either set data aside for a time
until it is convenient for 
a human user to investigate.  In still other cases,
errors are simply discarded.  For some applications, errors in the data can
be the most interesting part because they can signal where two systems
are failing to communicate.  

A fourth challenge is that ad hoc data sources can be high volume:
AT\&T's call-detail stream contains roughly 300~million calls per day
requiring approximately 7GBs of storage space. Although this data is
eventually archived in a database, analysts mine it profitably before
such archiving~\cite{kdd98,kdd99}. More challenging, the \ningaui{}
project at AT\&T accumulates billing data at a rate of 250-300GB/day,
with occasional spurts of 750GBs/day. Netflow data arrives from Cisco
routers at rates over a gigabyte per second~\cite{gigascope}! 
%Such
%volumes mean it must be possible to process the data without loading
%it all into memory at once.

A final challenge is that ad hoc data often needs to be translated
into a new, more useful or more standard format before anything
can be done with it.  Intuitively, this transformation
can be done in three stages.  First, one must generate a parser for
the ad hoc format to read it into a program.  Second,
one must engineer the transformation itself by
filtering unwanted parts, normalizing representations, and
detecting and correcting or deleting erroneous data.  Third,
transformed data must be printed in the new format.

% We would like to emphasize again that
% today, people tend to use \C{} or \perl{} for this task.
% Unfortunately, writing parsers, transformations and printers this way
% is tedious and error-prone, complicated by the lack of documentation,
% convoluted encodings designed to save space, the need to produce
% efficient code, and the need to handle errors robustly to avoid
% corrupting down-stream data.  Moreover, the parser writers' hard-won
% understanding of the data ends up embedded in parsing code, making
% long-term maintenance difficult for the original writers and sharing
% the knowledge with others nearly impossible.

\subsection{\datatype{}:  Taking on the Challenge of Ad Hoc Data}
\label{sec:datatype-overview}

The \datatype{} system will make life easier for data analysts by addressing
each of the concerns outlined in the previous section.
It provides a declarative data description
language that permits analysts to describe the physical layout of
their data, \textit{as it is}.  The language also permits analysts to
describe expected semantic properties of their data so that deviations can
be flagged as errors. The intent is to allow analysts to capture in a
\datatype{} description all that they know about a given data source.
When the data descriptions are coupled with the computational element
of \datatype{}, programmers will have a quick and easy way to
develop efficient and reliable scripts for processing ad hoc data.


\figHeight{architecture-grant}{Processing ad hoc using the \datatype{} system. 
The sequences 010010100100.. and
101101001011.. stand for the input and output ad hoc data
respectively. The notation ``Rep, MD'' stands for the pair of an internal 
ad hoc data representation and its associated metadata including error 
descriptions.}{fig:dsys}{2in}

\datatype{} descriptions are intended to be concise enough to serve as
documentation and flexible enough to describe any ASCII, binary,
Cobol, and mixed data formats.  Ideally, programmers write
a single description for a data source and then develop
many useful software tools based on
the single description.  This one-description/many-tools feature provides
strong incentive for keeping the descriptions current, allowing them
to serve as living documentation for a data source.
  
Figure~\ref{fig:dsys} shows how one builds a typical transformation
tool for processing ad hoc data using \datatype{}.  The first step is
to write down \datatype{} descriptions for the input data and output
data.  In some cases, the input
and output data formats will be the same.  In other cases, \datatype{}
may simply serve as a front-end tool for getting ad hoc into a
programmable system and an output description will not be needed.
Once the descriptions have been defined, the \datatype{} description
compiler will be able to produce customizable libraries for parsing
and printing the data.  The generated parsing code checks all possible
error cases: system errors related to the input file, buffer, or
socket; syntax errors related to deviations in the physical format;
and semantic errors in which the data violates user constraints.
Because these checks appear only in generated code, they do not
clutter the high-level declarative description of the data source.
Moreover, since tools are generated automatically by a compiler rather
than written by hand, they are far more likely to be robust and far
less likely to have dangerous vulnerabilities such as buffer
overflows.  The Ethereal system~\cite{ethereal}, which is used for
monitoring network performance and security, is a cautionary example
of what happens when people write parsers for ad hoc data by hand ---
they are bound to get them wrong.  A recent security analysis~\cite{etherealvulnerabilities} of the
Ethereal system found 13 new vulnerabilities
that allowed a malicious attacker to take control of the system, an
unfortunate situation for software intended in part to improve the
security of a system. Nine of the 13 vulnerabilities
were the result of errors in the
parsing code.


The result of a parse is a pair consisting of a {\em canonical
in-memory representation} of the data (denoted ``Rep'' in 
Figure~\ref{fig:dsys}) and meta-data for the parse 
(denoted ``MD'' in 
Figure~\ref{fig:dsys}).  We
refer to the meta-data as the {\em parse descriptor}. The parse
descriptor may hold a variety of bits of information including
the position of the data in the file and a characterization
of the possible errors in the data.  There are several
different sorts of errors that may arise.  Syntactic errors occur
when a parser cannot read a valid item of the right type from the file 
(eg: a parser attempting to read an integer finds the character '?'
instead).  Semantic errors occur when a parser can read an item
but it does not satisfy the appropriate semantic condition (eg: the
parse finds an integer in the file, but the integer is zero when it
should be greater than zero).  

The computational part of the \datatype{} language will form the glue between
the input data format and the output format.  Transformations written
\datatype{} will be able to handle error-free data using a concise 
notation.  In addition, programmers will have the freedom to
query the parse descriptor coupled with the data representation. 
This structure allows programmers
to respond to errors in application-specific ways.

With such huge datasets, performance is critical. We will 
investigate techniques for addressing performance in a 
number of ways.  First, we will
compile the data description to \pads{} rather than simply interpret
it. \pads{} itself is compiled into efficient \C{} code.  Second, we
will support lazy processing of data streams for very large data
sources.  The lazy stream-reading system will read manageable chunks
of a large data source into the run-time environment and flush them
when they are no longer needed.  Third, we will investigate techniques
for folding together multiple transformations (\ie{} {\em deforestation}~\cite{wadler:deforestation})
and guaranteeing that transformations need only make a single pass
over the data source.
% Finally, we will exploit another
% portion of the underlying \pads{} implementation: the \textit{parsing
% masks}.  \pads{} parsing masks allow data analysts to choose which
% semantic conditions to check at run-time, permitting them to specify
% all known properties in the source description without forcing all
% users of that description to pay the run-time cost of checking them.
% \datatype{} programmers will be given access to this functionality as
% well.

% We propose to extend and improve the current \pads{}\ prototype by engaging
% in three intertwined research efforts:

% \begin{enumerate}
% \item Our experience with real-world data has revealed that
% the current \pads{}\ system is unable to process many common
% data formats.  We propose a number of extensions to \pads{} 
% that will greatly improve its expressive power and enable
% us to handle many more important ad hoc formats.
% \item While \pads{}\ can automatically generate tools for providing
% statistical data summaries, querying ad hoc data and generating
% \xml, the tool generation process is extremely brittle, inflexible
% and results in low-performance software.  We propose to redesign
% the automatic tool generation process using a principled methodology
% based upon a novel \pads{}-attribute system that can communicate
% semantic information across tools.  Our new methodology will
% improve the flexibility and reliability of the \pads{} system.
% \item There is no formal description of the \pads{} language
% and no semantics for the compiler or tools.  We propose to
% formalize the language and analyze the semantics of the
% compiler.  Our formal analysis may find bugs or flaws in the
% \pads{} system and will improve our confidence in the implementation.
% \end{enumerate}

% \noindent
% In addition, part of our mission will be to use \pads{} to provide
% data processing tools for researchers in the natural sciences including
% biology, chemistry and physics.  

% In building the \pads{} system, we faced challenges on two fronts:
% designing the data description language and
% crafting the generated libraries and tools.
% In the rest of the paper, 
% we describe each of these designs.
% We also give examples to show how useful they have been at AT\&T.


\subsection{Towards a Universal Ad Hoc Data Description Language}
\label{sec:universal}

In order to give the reader a sense of our proposed data
description language and some of the challenges, we
will take a look at an example of ad hoc data: summaries of
phone order information produced by the \dibbler{} project.
%To track AT\&T's provisioning process, the \dibbler{} project compiles
%weekly summaries of the state of certain types of phone service orders.  
These ASCII summaries store the summary date and one record per order.
Each order record contains a header followed by a sequence of events.
The header has 13 pipe separated fields: the order number, AT\&T's
internal order number, the order version, four different telephone
numbers associated with the order, the zip code of the order, a
billing identifier, the order type, a measure of the complexity of the
order, an unused field, and the source of the order data.  Many of
these fields are optional, in which case nothing appears between the
pipe characters.  The billing identifier may not be available at the
time of processing, in which case the system generates a unique
identifier, and prefixes this value with the string ``no\_ii'' to
indicate the number was generated. The event sequence represents the
various states a service order goes through; it is represented as a
new-line terminated, pipe separated list of state, timestamp pairs.
There are over 400 distinct states that an order may go through during
provisioning.  The sequence is sorted in order of increasing timestamps. 
\figref{figure:dibbler-records} shows a small example of
this format.
%156 different states for one order
%-rw-r--r--    1 angusm   dibbler   2187472314 Jun  9  2003 /fs/dibblerd/tlf/data/out_sum.stream
%2171.364u 31.379s 40:41.54 90.2% 0+0k 2+0io 2pf+0w
%53 had trailing t or } after zip code
%It may be apparent from this paragraph that English is a poor
%language for describing data formats!


\begin{figure*}
\begin{small}
%\begin{center}
\begin{verbatim}
0|1005022800
9152|9152|1|9735551212|0||9085551212|07988|no_ii152272|EDTF_6|0|APRL1|DUO|10|1000295291
9153|9153|1|0|0|0|0||152268|LOC_6|0|FRDW1|DUO|LOC_CRTE|1001476800|LOC_OS_10|1001649601
\end{verbatim}
\caption{Tiny example of \dibbler{} provisioning data.}
\label{figure:dibbler-records}
%\end{center}
\end{small}
\end{figure*}

\suppressfloats


\begin{figure}
\begin {code}
\input{dibblerml}
\end{code}
\caption{\datatype{} description for \dibbler{} provisioning data.}
\label{figure:dibblerml}
\end{figure}


A \datatype{} description will specify the physical layout and 
semantic properties of an ad hoc data source. 
The language provides a type-based model:
basic types describe atomic data, while
structured types describe compound data built from simpler pieces.
Examples of base
types include 8-bit unsigned integers (\cd{Puint8}), 32-bit
integers (\cd{Pint32}), binary 32-bit integers (\cd{Pbint32}),
dates (\cd{Pdate}), strings (\cd{Pstring}),
and IP addresses (\cd{Pip}).  Semantic conditions for such base types
include checking that the resulting number fits in the indicated
space, \ie, 16-bits for \cd{Pint16}.  
%
% By themselves, these base types
% do not provide sufficient information to allow parsing because they do
% not specify how the data is coded, \ie{}, in ASCII, EBCDIC, or binary.
% To resolve this ambiguity, \datatype{} will allow users to set an
% ambient coding discipline.  By default, it will use ASCII.  In addition to
% these built-in types, we propose to allow users to define their own new base 
% types to specify more specialized forms of atomic data.  We
% will need to discover a convenience and effective way to do this.
%
Base types (and other types) may be parameterized by values.  
This mechanism serves
both to reduce the number of base types and to permit the format and
properties of later portions of the data to depend upon earlier
portions.  For example, the base type \cd{Puint16_FW(3)} specifies
an unsigned two byte integer physically represented by exactly three
characters, while the type \cd{Pstring(' ')} describes a string
terminated by a space.  The \datatype{} implementation 
will exploit the fact that we have already implemented many,
many such base types in the underlying \pads{} backend.  

To describe more complex data, \datatype{} provides a collection of
structured types loosely based on the type structure of functional
programming languages such as Haskell and ML.  
\figref{figure:dibblerml} gives a \datatype{} description 
for the \dibbler{} phone order summaries in our
proposed syntax.  Overall, the data
description is a sequence of type definitions.
It is probably easiest to understand the data source by
reading these descriptions bottom up.

The last type definition \cd{source} is intended to be
a definition of an entire \dibbler{} data source.  The
type description states that a \cd{source} is a
\cd{summery\_ header} followed by a sequence of objects
made up of an \cd{order\_header} followed by \cd{events}.
The tuple type constructor (\cd{T1 * T2}) and the
array type constructor (\cd{T Parray(sep,term)})
both specify sequences of objects in a data source.
The \cd{Parray} type depends upon two value parameters,
(\cd{sep} and \cd{term}).
The first parameter describes the syntactic separators 
that may be found between elements of the array.  In this
case \cd{NL} (the newline character) may be found between
each element of the array.  In other words, once 
the \cd{summary\_header} has been parsed, each line
of the data source will contain an 
\cd{order\_header} and \cd{events}.  The second parameter
is the terminator for the array.  In this case,
the terminator is the end-of-file marker.  We will also
support a variety of other termination conditions for arrays as necessary.

% Different cases in the datatype describe
% different alternatives in the ad hoc data.  Recursion can be used to
% specify flat representations of tree-like data.  Dependency makes
% it possible for later parsing to depend upon earlier parsing.
% There will also be (dependent) records, tuples and arrays for describing sequences of data
% items.  Singleton types will be used to describing literal characters that must
% appear in the ad hoc data source. Each of
% these types can have an associated predicate that indicates whether a
% value calculated from the physical specification is indeed a legal
% value for the type.  For example, a predicate might require that two
% fields of a record are related or that the elements of a
% sequence are in increasing order.  Programmers can specify such
% predicates using \datatype{} expressions and functions, written in an
% ML-like syntax.  Finally, \datatype will allow programmers to
% define their own new type abbreviations.

% \begin{figure}
% {\small
% \input{dibbler_new}
% }
% \caption{\pads{} description for \dibbler{} provisioning data.}
% \label{figure:dibbler}
% \end{figure}



The definition of \cd{events} indicates that
this part of the \dibbler{} data will contain a sequence
of \cd{event}s separated by verticle bars and terminated by a newline.
Each \cd{event} is a string terminated by a veriticle bar,
followed by a verticle bar and ending with an unsigned
32-bit integer.  The interesting part of this sequence is the
presence of the type \cd{'|'}.  In type-theoretic terms, this is
a {\em singleton type}.  It states that one should
 expect exactly the character \cd{'|'} in the input stream at this point.
Other singletons appear in the summary header type as \cd{"0|"}
and NL (the newline character).

The type \cd{order\_header} is a record type that indicates
the data format involves the sequence of items described by
the fields of the record.  Notice that there are two different
sorts of fields: anonymous fields containing directives to parse
a particular character (\cd{'|'}), like the singleton types,
and fields with names.  The second named field,
\cd{att\_order\_num}, reveals two other proposed features of 
\datatype: dependency and constraints.  Here,
\cd{att\_order\_num} is constrained to be less than
\cd{order\_num}, the value parsed in an earlier field.
This is a relatively simple constraint on the correctness of the
ad hoc data format.  In practice, constraints can become very rich
involving properties such as sortedness of records in an array,
definitions of expected characters,
restrictions on date and time ranges, constraints on IP address
domains, restrictions on phone number area codes and virtually 
infinite variety of other possibilities. 

The last interesting feature in the \dibbler{} example is the
datatype definition of \cd{dib\_ramp}.  It describes
two alternatives for a portion of data, either an integer alone
or the fixed string \cd{"no\_ii"} followed by an integer.
In order to parse data in this format, the parser will
first attempt to parse the first branch and only if it
fails will it attempt to parse the second branch.

\begin{figure}
\begin{code}
val COMMA  = ','  
val COLON  = ':'  
val SEMI   = ';'  
\mbox{}
type entry = Pstring(COLON) * COLON * Pfloat32
\mbox{}
datatype tree =
    Tree of '(' * tree Parray(COMMA,')') * "):" * Puint32
  | Tip of entry
\mbox{}
type trees = (tree * SEMI) Parray(NL,EOF)
\mbox{}
{\rm Tiny data fragment with type trees: }
\mbox{}
(((erHomoC:0.28006,erCaelC:0.22089):0.40998,(erHomoA:0.32304,
(erpCaelC:0.58815,((erHomoB:0.5807,erCaelB:0.23569):0.03586,
erCaelA:0.38272):0.06516):0.03492):0.14265):0.63594,(TRXHomo:0.65866,
TRXSacch:0.38791):0.32147,TRXEcoli:0.57336);
\end{code}
\caption{Simplified Tree-shaped Newick Data}
\label{figure:newick}
\end{figure}

A second interesting example of ad hoc data comes courtesy of
Steven Kleinstein, program coordinator of Princeton's Picasso project for
interdisciplinary research in computational sciences.  
Kleinstein is in the process of 
building a simulator to study immune response.  Data
needed for his simulations comes in a Newick format, which is
a flat representation of trees and used by many biologists~\cite{newick}.  
In Kleinstein's Newick format (simplified
here for expository purposes), leaves of the
tree are string labels followed by a colon and a number.
A parent node in the tree introduces a collection of
children by placing a sequence of trees within parens.
Following the parens is a colon and a number, as is the case
for the leaf node (incidentally, the numbers are called ``distances''
and represent the number of genetic mutations that separate the
child from the parent).  Each line of a file may contain
a different tree, terminated by a semi-colon.

\figref{figure:dibbler-records} gives a description of Newick
and a bit of example data.
Despite the relative complexity of the structure of the data, 
the description is remarkably concise.
Notice that the data type definition of \cd{tree} is recursive ---
there appears to be no effective description of this data
source without it.  

While we have designed a basic syntax for our universal data description
language there remain many questions to answer. Here are a couple
to give the reader a sense of the research challenges we face.
\begin{itemize}
\item What range of ad hoc data descriptions can and can't we describe?
Can we describe DAG-structured data such as the gene ontology
data used by Troyanskaya?  Can we describe more general structures
involving pointers (DNS data is compressed using pointers)?  What
additional support is required?
\item Can we develop reusable
specification libraries for common items like trees, DAGs, lists and tables
akin to the data structure libraries found in
conventional programming languages?
\item How do we compile \datatype{} descriptions to \pads{}?  What
extensions to \pads{} are necessary to support \datatype{} features such as
recursion and polymorphism? How do we maximize efficiency of generated parsers
over massive data sets?
\item How do we provide a formal semantics for \datatype{} descriptions?
\end{itemize}

\subsection{The \datatype{} Transformation Language}
\label{sec:language}

Often, the first thing one wants to do with ad hoc data 
is to transform it into some standard form, such as \xml{}
or one of the standard database formats.  This way,
the ad hoc data can be read directly into the database
and archived there.  Once the data has made its way into a database, 
users can write standard XQuery or SQL to extract the information 
they need.  Since maintaining the integrity of information
within a database is crucial, conversion to a standard format
is usually accompanied by a data cleaning phase that detects
and attempts to correct any errors it finds in the data source.
This phase may also filter out unneeded data, convert dates,
times and other information with a variety of formats into
a canonical form, rearrange the order of elements,
and otherwise massage or transform the data.
We intend to design \datatype{} 
to support writing clear, reliable and efficient programs to
accomplish these data transformation tasks.
Overall, the language will be functional and make
heavy use of pattern matching.  We believe this design will
allow programmers to quickly write concise, correct and maintainable
programs.  

The first innovative feature of \datatype{} will be
that the polymorphic, dependent
data descriptions (\cd{source}, \cd{entry}, etc.)
will be used both as directives for parsing and
as types in the programming language.  These
types will describe the data structures that represent
ad hoc data once it has been
read into the run-time system.  Since \datatype{} descriptions
are parameterized by values and depend upon complex semantic constraints
this leads to a rich type structure for our programming language.
A central research problem will involve developing the type system
and its type inference algorithm.

A second key feature is the intrinsic support the programming language provides
{\em error-aware computing}.  As mentioned repeatedly in this proposal,
errors are an intrinsic part of ad hoc data.  In fact, for some 
applications, errors are the most interesting part of the data 
as they indicate where software or
hardware may be failing.  The \datatype{} programming infrastructure
will help programmers deal with errors as they see fit.
As we mentioned in previous
sections, the parsers generated from \datatype{} data descriptions
will produce pairs of internal objects composed of
a data representation and a parse descriptor (PD)
that carries metadata concerning errors that may present in the 
representation.
We will refer to the pair of ad hoc data representation and corresponding PD
as an {\em ad hoc value}.

One of the crucial design principles of our language will be
that programmers never write PDs themselves; the
PDs are always constructed automatically by the
compiler system.  This important principle allows the compiler
to guarantee that an ad hoc value's PD always accurately 
describes the representation.
It will be impossible for a programmer to corrupt the relationship
between representation and PD accidentally.  

As a first cut, the PD for any ad hoc
value may indicate one of the following things:
\begin{itemize}
\item {\tt G}: this is \cd{G}ood data; it contains no syntactic or 
semantic errors, not even nested deeply within its structure.
\item {\tt B}: this is \cd{B}ad data; it is not even syntactically correct
\item {\tt S}: this is syntactically valid data, but a \cd{S}emantic 
constraint does not hold.
\item {\tt R}: the top-level structure of this data is good, but some
(\cd{R}ecursive) substructure is either syntactically or semantically invalid.
\item {\tt U}: this data may be good, but I am \cd{U}nsure.  The programmer 
will have to check the substructure herself.  This last descriptor
is important for processing massive data streams where the system reads the
stream lazily and cannot know in advance whether or not the stream
as a whole will be good.
\end{itemize}

\noindent
Although programmers cannot construct a parse descriptor and pair it with a
representation to create a value, the programmer may use pattern matching
to extract and read a value's parse descriptor.  Notice that
when the programmer determines that a value's parse descriptor is
\cd{G}ood, this implies the entire substructure is error free
and need not be checked further for errors.  This 
greatly facilitates programming with data that is expected
to be error-free in the common case ---  a programmer can simply check 
the top-level PD and if it is indeed \cd{G}ood,
she need not clutter the rest of her code with error-checking
preteens.  In addition, when the top-level PD is \cd{G}ood,
the value representation might perhaps be optimized as the
PDs for the substructures are unnecessary.  Of course, we
need to do much more research before we will understand how to 
represent and compute with ad hoc data values most efficiently.


\begin{figure}
\begin{code}
open Sirius (* introduce \dibbler type definitions. See Figure \ref{figure:dibblerml}. *) 
\mbox{}
type streams = entry stream * entry stream
\mbox{}
fun splitEntry (e:entry * streams) : streams =
  case e of
    (entry<<G>>, (good, bad)) => (entry::good, bad)
  | (entry<<_>>, (good, bad)) => (good, entry::bad)
\mbox{}    
fun splitSource (s:source) : streams =
    let (hdr,Parray(entries,_,_)) = s in 
    stream\_fold splitEntry (nil, nil) entries
\end{code}
\caption{Error filter for \dibbler{} data}
\label{figure:newick-clean}
\end{figure}


\figref{figure:newick-clean}{} presents a very simple \datatype{}
program in a syntax we are in the process of designing.  The
intent here is to give the reader a basic idea of the initial direction
for our design --- it is by no means well worked out.  Much additional
research is required.  This program does a very simple and common
transformation.  It takes the \dibbler{} data source and
walks through all of the entries in the source checking for
entries with errors.  The good entries are placed in
one output stream and the bad entries are placed in another.
The idea is that the good entries may then be further processed
or directly loaded into a database without corrupting
the valuable data therein.  A human might examine
the bad entries off-line to determine the cause of errors
and to figure out how to fix the corrupted entries.

The \cd{splitEntry} function describes how to check an entry
to determine whether it is \cd{G}ood (syntactically and semantically
valid) or not.  
Here, \cd{pat<<pdpat>>} is a pattern that the programmer uses
to extract information about the parse descriptor from a value. 
\cd{pdpat} is the pattern for PDs and \cd{pat} is a pattern for 
the value's substructures.  

The \cd{splitSource} function pattern matches against the
input source, extracting the stream of entries, and iteratively
applying the \cd{splitEntry} function.  In this example,
\cd{Parray(entry,\_,\_)} is a pattern for array values.
An array value is a stream (in this case \cd{entry})
coupled with a separator and a terminator.  Also in this example,
we assume \cd{fold\_stream} 
iteratively applies a function to a stream.  
%We intend to
%scour the existing research on stream processing to help us
%develop a stream programming model for \datatype. 

% The function \cd{app} is responsible for reading in
% data from the file descriptors it is given,
% splitting the data into good and bad 
% and writing the data back out to files.
% The process of reading and writing is dependent on the
% type descriptions of the ad hoc data.  Here, we write
% \cd{read::T} to read a data source formatted according to \cd{T}
% and \cd{write::T} to write a format according to \cd{T}.
% The read function will parse a file and generates an ad hoc data value with
% type \cd{T}.  The write function will write an ad hoc data value with
% type \cd{T} to a file.

% This example does not focus particularly on the type structure of the
% programming language, but since the \datatype{} descriptions
% are usually dependent and often involve very rich constraints,
% we have much research to do on the structure of the \datatype{} type
% system and type inference algorithm.

We have only fleshed out a preliminary 
syntax for the language to give the reader a feel for the
direction of our initial research.  There are a tremendous number of
questions to be answered:

\begin{itemize}
\item What sort of operations are available for reading, writing,
merging, splitting and computing over streams of data?
\item Is our design of parse descriptors effective and useful?
What additional information should they contain?
\item How, in general, do we write reliable and maintainable
programs in this language? What kind of recurring
programming paradigms help us program in the presence of errors?
\item Is it possible to write safe, {\em generic} programs
that operate correctly over data in {\em any} format whatsoever?
For instance, can we write a completely generic tool that
can transform any data format into \xml?  \pads{} has some
partly-generic tools, but these tools are written in 
low-level, heavily-macro-ized C that is not safe
and not really generic as it only handles data shaped as a header and
list of records.
\item How do we optimize performance of the system?  Can we provide 
programmer support in the form of a guarantee
that transformations need only make a single pass over massive sets?
What representation of parse descriptors is most efficient?
\item How do we design the type system for language.  The descriptions
often involve complex dependent constraints.  Is it possible to check these
constraints statically? How? To what extent can we support type inference?
\item How do we define the semantics of our transformation language?
\item Even more speculatively, given two related ad hoc data formats,
can we generate the programs that transform data from one format to 
the other?  Can we generate part of the transformation and have a programmer
help us with the rest?  
%\item Can we provide high-level, visual support to 
%non-programmers that allows them to parse, edit, print, search and transform
%arbitrary ad hoc data sources?  Can these tools help the productivity of
%a biologist, astrophysicist or financial analyst with little or no
%andknowledge of programming languages?
\end{itemize}

\subsection{Specific Research Plan}
\label{sec:plan}

The previous subsections have argued
that ad hoc data can be found across all areas of science,
industry and government.  We desperately require new tools for processing
this data, which comes in an infinite variety of shapes and styles,
and is often poorly documented and filled with errors.  We hope to
have given the reader a flavor of the
universal data description language we propose for describing
ad hoc data as well as the structure of
the \datatype{} programming language, which we plan to use to
manipulate, analyze and transform ad hoc data.
Our ultimate goals for the data description language are (1) to make it
as expressive as possible, enabling descriptions of
any data format, (2) to keep the notation lightweight, 
concise and easy to understand at a glance, and (3)
to be a good fit with the data processing and
transformation components of the \datatype{} language.
Our ultimate goals for the \datatype{} programming language
are (1) to make it easy and convenient to write reliable
scripts for processing any ad hoc data source, (2)
to support programming in the presence of the errors that
almost invariably arise in ad hoc data sources and (3)
to support efficient processing of massive data sets.
In this subsection, we briefly outline the major steps
we will take to realize our language design goals.

\paragraph*{Design of basic type structure (Year 1).}
We will first design the concrete syntax of the data description,
including the polymorphic, recursive and dependent datatype specifications 
we have proposed.
As indicated in the previous section, we already have worked
out the general ideas behind the basic design.  Now we need to make 
it precise, test it on example applications, refine the design
and define the semantics.  

% We have not mentioned polymorphic definitions yet in any detail,
% but our experience with several dataformats indicates
% they will be very useful for specification reuse.  
% For example, in the common format for web server logs~\cite{wpp}, optional
% values are represented as either a single dash (if the item does not
% appear) or the item itself (if it does appear).  Currently, for every
% different sort of optional value (\ie ``dash or integer'' vs. ``dash
% or string''), one must write down a separate specification.  It should
% be possible to eliminate this sort of redundancy and reduce the size
% of format specifications using polymorphic types (types parameterized
% by other types).  In our previous examples, we used
% {\cd{Popt}} type constructors (``nothing or something'') pervasively.  
% With polymorphism,
% these sorts of constructors would not need to be built in to the
% system.  A user could define new constructors of this ilk 
% whenever they choose.

%In addition, we will be developing a syntax for the more mundane
%elements of a language: tuples, records, arrays
%(parameterized) type definitions, base types etc.

\noindent
{\bf Implementation of the basic type structure. (Year 1)}
In order to implement our data description language,
we will compile the basic types into our old system, \pads.
In order to compile the recursive and polymorphic
elements of our design, 
two features our experience with real-world data sources
has shown are invaluable,
we will have to extend the underlying \pads{}
implementation.  
% These extensions to \pads{}
% are nontrivial.  In particular,
% we need to transform \pads{} current ``descent'' parser into
% a ``recursive descent'' parser.  
% There are also a number of changes that must be made to
% the generated libraries to handle recursion.  
% We will have to make similar sorts of extensions
% to \pads{} to handle polymorphism as well.  However,
% once the extensions to \pads{} are complete,
% we believe we will be able to compile recursive data types
% into specifications based on \pads{} structs and unions.
% However, the optimal compilation strategy is certainly
% an important research question.


\noindent
{\bf Novel Data Formats and Descriptions. (Years 1-3)}
As we develop \datatype, we will be constantly
on the lookout for new forms of ad hoc data.  
When we find dataformats we cannot easily describe,
we will research new ways to handle them cleanly and concisely.
Already, we that we cannot easily describe DAG-structured
and data that shows up in the Gene Ontology (GO)
Project~\cite{geneontology} used by Olga
Troyanskaya (Princeton Lewis-Sigler Institute for Integrative
Genomics).  Nor can we handle the pointers used to compress
domain names in DNS.
% For instance, we recently introduced to the , a data repository used by Olga
% Troyanskaya (Princeton Lewis-Sigler Institute for Integrative
% Genomics) and other biologists who study protein-protein interactions.
% One component of the GO data repository classifies reactions by
% placing them in a DAG-structured hierarchy.  In order
% to specify and represent this data internally in a 
% space-efficient manner, we cannot simply rely on
% recursive datatype descriptions as they will
% generate trees of data internally.  Instead,
% we believe we need a second mechanism which will allow us to specify
% a pointer from one part of the data representation to another.
% Another similar example occurs in
% part of the DNS packet format.  This format compresses
% host names by using pointers to point back to common
% hostname suffixes. 
We plan to consider these and similar data sources in more detail
in order to come up with more general specification mechanisms still.
As we come across other novel data sources, we will do the same.


% \paragraph*{Advanced Features: Multi-stage Data Processing.}
% Many data sources require multi-stage processing.  For instance,
% security-sensitive data may be encrypted and high-volume data
% is often compressed.  On the way into the system, a first 
% pass must decrypt or decompress the data before a second pass
% does the parsing work on the underlying format.  On the way out of the system, 
% it may be necessary to apply reverse transforms.  

% One solution
% to this problem might be to let auxiliary passes through the data
% remain outside the \datatype{} system.  Unfortunately, this solution
% leaves us in a situation in which \datatype{} descriptions 
% are not self-contained
% definitions of the data format in question.  Consequently some of the value
% of \datatype{} as documentation is lost.  
% In addition, some data formats~\cite{korn+:delta,korn+:data-format} are not 
% uniformly encrypted or compressed.
% The data may contain subsections that are encrypted, and more troublesome,
% the encryption or compression algorithms may be data-dependent so
% disentangling the compression and encryption stages 
% involves just the sort of parsing that \pads{} was built for.

% We hope to be able to facilitate multi-stage data processing 
% directly in \datatype.  As a
% first step, we will add staging directives with the form 
% \cd{preprocess(args) \Pthen T} to the language of types.
% The command \cd{preprocess(args)} performs some preprocessing step, be it
% decryption, decompression or otherwise.  The results of the preprocessing
% will then be fed into the parser generated from the type \cd{T}.
% The \Pthen\ type will be a first-class type and therefore it will compose
% with any other PADS specification.  For instance, it will be straightforward to
% decompress and then decrypt:  

% \cd{decompress(length) \Pthen decrypt(key, length) \Pthen T}.  

% \Pthen\ statements are designed to compose with other PADS features, making
% it easy to do data-dependent processing.

% \paragraph*{Advanced Features:  Integrating Multiple Data Repositories.}
% Sometimes, a single logical data source is represented
% as several distinct, concrete repositories.   This is the case
% in the GO data source used by Troyanskaya. 
% In GO,  data is split into four disjoint
% files: a molecular function file, a biological process file,
% a cellular component file and a term definitions file.

% % begin BS

% We believe that the right way to describe such data sources
% is to introduce a notion of {\em data module} into \pads{}.
% In this case, a data source as a whole may link
% a series of data modules together.  One of the goals
% of this design would be to allow enough flexibility that
% the whole data source could be processed together or a single module
% (such as for molecular function module) could be processed on its own.
% However, it remains unclear if this is indeed the right processing model;
% more research and experimentation is necessary to determine the
% pros and cons of such a set up.

% % end BS


\noindent
{\bf \datatype{} transformation language design. (Years 1-2)}
We will begin by designing the basic syntax of commands and operations
for the language.  The initial design will resemble the examples
given previously in the paper.  As we gain more experience with further
examples and experimentation, the design will undoubtedly evolve.
Initially, we will develop a {\em dynamically-typed} language in which
the data descriptions (in addition to directing parsing and printing)
are used as the {\em contracts} we have seen in
Eiffel~\cite{eiffel}, and most recently in the functional
programming languages investigated by Findler and 
Felleisen~\cite{findler+:contracts}.  Intuitively, a contract
is simply a requirement that a data value match a specification.
In our case, the specifications are the \datatype{} descriptions.
The run-time system will dynamically 
check that the value satisfies the contract/data description.
This initial phase will result in a complete scripting language
for rapid prototyping work involving ad hoc data.  
%We need further research
%to understand how useful and efficient such a dynamically typed design can be.


\noindent
{\bf \datatype{} type system and inference algorithm. (Years 2-3)}
The second phase of the language design will involve augmenting 
\datatype with a static type system.  Development of a dependent
type system and inference algorithm for the language will be a
significant research challenge.  However, we are confident we will be
able to make use of recent work on dependent type systems,
particularly work by Zenger~\cite{zenger:indexed-types} and Xi and
Pfenning~\cite{xi+:dml}.  Of course, to meet our goals, we will have
to extend the previous authors work substantially in order to deal
with the sophisticated constraint systems that appear in ad hoc data.
The PI has extensive experience with dependent type
systems~\cite{walker:security,smith+:alias-types,walker+:capabilities,mandelbaum+:refinements,ahmed+:hierarchical,tan+:tcs04,}
and we will take advantage of this experience to meet our goals.


\noindent
{\bf Applications.  (Years 1-3)}
Throughout the duration of our grant,
we propose to study applications
in networking and telecommunications, in genomics and
microbiology, and in medical domains including processors for 
lab reports and medical diagnoses. 


\noindent
{\bf Formalization and Semantic Analysis. (Years 1-3)}
As far as we are aware, no data description language
anything like \datatype{}
(including \pads~\cite{fisher+:pads}, 
\packettypes~\cite{sigcomm00}, 
\datascript~\cite{gpce02}, and \blt~\cite{eger:blt})
has been given a formal semantics.  
We propose to ameliorate this situation by
formalizing the core features of the current language
and analyzing the semantics of our extensions as we develop them.
We intend to investigate denotational semantics for descriptions both as sets
of parsable strings and as parser functions. 
We also intend to investigate the metatheoretic properties of the 
transformation language.
% We will also formalize the dynamic and static semantics of the 
% computational portion of \datatype.  We will investigate
% its metatheoretic properties and prove well-typed programs 
% ``don't go wrong.''  In our case, this means that PDs always
% accurately describe their corresponding data representations and
% consequently programmers
% will never fall into the trap of using bogus data that could not be 
% properly parsed.

%We believe the formalization will provide another dimension along which we can
%evaluate our tools, will help us find bugs in the implementation and 
%may suggest ways to generalize or streamline the \pads{} system.

% We believe that the best way to model \datatype{} is as a dependent
% type theory with dependent record types ($\Sigma x{:}\tau.\tau'$) to model
% the dependent records we propose, set types 
% ($\{x{:}\tau \; | \; P(x) \}$) to model
% semantic constraints, sum and recursive types to model
% datatypes and type functions
% ($\lambda x{:}\tau.\tau'$ and $\lambda \alpha{::}\kappa.\tau'$) 
% to model parameterized \pads{}\ types.  
% In order to give a semantics to the \datatype{} description
% compiler, we will explore using
% a denotational semantics in which {\em types} are interpreted 
% {\em as data processors}:  $\lsem \tau \rsem = f$
% where the data processor $f$ is a function that maps bitstreams into 
% data structures in the lambda calculus. 

% We will also formalize the dynamic and static semantics of the 
% computational portion of \datatype.  We will investigate
% its metatheoretic properties and prove well-typed programs 
% ``don't go wrong.''  In our case, this means that PDs always
% accurately describe their corresponding data representations and
% consequently programmers
% will never fall into the trap of using bogus data that could not be 
% properly parsed.


\subsection{Broader Impacts}
\label{sec:impact}


\noindent
{\bf Supporting Research in the Natural Sciences.}
If this research is funded, part of our mission will be
to provide support for the data processing needs of biologists,
chemists and physicists at Princeton University first and 
the broader natural sciences community later.  As mentioned,
we have already begun to work with Olga Troyanskaya
and Steve Kleinstein to understand their needs.
As is the case with many computational biologists,
Troyanskaya analyzes genomics data that is provided to her in ad hoc
formats.  For instance, Troyanskaya reports that she
and her students have spent
substantial blocks of time (weeks and months)
building parsers to collect and integrate
this data.  This wastes Troyanskaya's valuable time, which could otherwise be
used on the challenging and important algorithmic problems for which
she is a world expert.
If this research is
funded, we will be able to provide data descriptions, which serve as precise
data format
documentation, and a suite of useful tools for Troyanskaya, Kleinstein
and their colleagues will be able to use to boost their productivity
substantially.  

% Troyanskaya has pointed us to several data
% repositories~\cite{grid,bind,geneontology} that she and her colleagues
% commonly use in their scientific investigations.  
% In fact, many of our proposed extensions to \pads{}\ are motivated directly by
% our investigation of these data sources.
% If this research is
% funded, we will be able to provide a suite of tools that Troyanskaya
% and her colleagues at Princeton's Genomics Institute and the wider
% genomics community will be able to use to boost their productivity
% substantially.  Part of our contribution will be a series of PADS
% descriptions for these formats and the analysis and querying tools we
% can generate automatically from .  A second important contribution
% will be a visual interface built on top of PADS that allows scientists
% to browse data represented in ad hoc data formats without having to
% know anything about programming or parsing.  All of our software will
% be freely available to academics via the Web.


\noindent
{\bf Undergraduate Research.}
Princeton University has a very active undergraduate research program.
Princeton students usually do at least one semester of
Junior Independent Work and one semester of Senior Independent Work. 
Many students also do a year-long
Junior and Senior Theses.  We believe that PADS is an excellent
platform for undergraduate independent work and we have plans for
a number of undergraduate projects, as discussed in 
section~\ref{ssec:intro}.  Overall, we hope to link
undergraduate independent work with our effort to
produce tools for biologists and medical information systems,
thereby exposing students to these emerging interdisciplinary fields.

\noindent
{\bf Industrial and Governmental Impact.}
Today's most important industries and governmental services
run on information.  As different companies, NGOs,
and governmental groups implement buy, sell
and exchange data, they are invariably frustrated by the time and expense
it takes to extract {\em information} from the low-level
formats in which their data is represented.  
When it comes to ad hoc data on a grand scale,
perhaps the most daunting challenge will come from the national
health-care system.  Recently, Newt Gingrich and Hilary Clinton have
jointly proposed we consolidate and integrate our national healthcare
records in the next 10 years.  Such a massive project will undoubtedly
require technical solutions at many levels.  At the lowest levels, we
will need a reliable software to collect and transfer health care data
from legacy systems into modern formats.  The \datatype{} system we
propose will be an invaluable aid in the development of this software.
We hope it will eventually make electronic medical information systems
more reliable and better integrated, thereby helping 
to save the lives of the hundreds of thousands of people that die
each year because their doctors receive erroneous data~\cite{rudlin:pc}.

\subsection{Results from Prior NSF Support}
\label{sec:results}

\paragraph*{Kathleen Fisher, Senior Personnel} 
Kathleen Fisher is a senior researcher at AT\&T Labs,
where she has spent the past nine years working on projects
related to managing massive amounts of ad hoc data.
In the Hancock project~\cite{kdd00,hancock-toplas}, she helped design and implement a C-based
domain-specific programming language for processing massive  
transaction streams.  Hancock programs make it easy to build
and maintain profiles of the entities described in such streams. 
AT\&T uses these profiles to detect fraud and target marketing.
In the PADS/C project~\cite{fisher+:pads}, 
Kathleen has helped design and implement
a data description language for describing the physical layout
and semantic properties of ad hoc data.  From these descriptions,
the PADS system produces a parser and a collection of tools for
manipulating the associated data.  PADS is used at AT\&T to
vet and clean network monitoring data before loading it into
relational databases.  

Fisher is an active proponent of increasing the
role of women and minorities in computing and has
obtained NSF funding to support increased involvement of women
in computer science (NSF 0243337, ACM Special Projects: 
Travel Grants for Faculty at Minority/Female Institutions to Attend
FCRC'03, Co-PI).  This grant was committed to improving the representation of women and
minorities in computer science. To that end, we solicited applications
for travel grants from faculty members at undergraduate institutions
with large minority and/or female enrollments to attend FCRC '03, an
umbrella meeting with 16 constituent conferences and many associated
workshops and tutorials.  The organizers of the constituent meetings agreed to waive
the registration fees for all program participants. 
Descriptions of the many meetings that
comprised FCRC '03 are available from the FCRC '03 web site 
\url{http://www.acm.org/sigs/conferences/fcrc/}.  We
received 56 applications, and were able to award 49
fellowships.




% This program exposed faculty members to state-of-the-art research in
% computer science, to provide them with materials and training that
% will help them improve their existing curricula and/or introduce new
% curricula, and to establish contacts with other faculty at peer
% institutions.  As part of the program, we organized an evening panel
% discussion on the topic of Computer Science Research: Recruiting and
% Retaining Women and Minorities.  Panelists included Jan Cuny
% (University of Oregon), Leah Jamieson (Purdue University), William
% Aspray (University of Indiana), and Ann Quiroz Gates (The University
% of Texas at El Paso).  

% Through evaluation forms and correspondence, the participants
% indicated that they were very happy with their experiences at
% FCRC '03. They said that they would attend a similar meeting again,
% were funding made available. The participants also indicated that the
% materials presented would have a strong positive influence on their
% future teaching and research.  They indicated a strong desire for more
% opportunities to network with the other fellows at the meeting.

%\input{walker-priors}

\paragraph*{David Walker, PI} (NSF CCR-0238328 CAREER: Programming Languages for Secure and Reliable Component Software
Systems, PI)
The goal of Walker's career award is to develop new programming language
technology that will improve the security and reliability of component software systems.  
One specific thrust of this work has involved the development of
type systems for ensure the safety of low-level mobile code.
One of the most crucial and difficult problems in this domain
is development of techniques for ensuring memory safety --
memory-safety guarantees 
provide a strong foundation on top of  which
richer security mechanisms can be implemented.  
To this end, he has developed
logic-based type systems that can detect memory errors involving
stack-allocated data~\cite{ahmed+:stack,jia+:stack} and heap or region-allocated
data~\cite{ahmed+:hierarchical-storage}.  In addition to studying memory safety
properties, Walker has shown how to use related type-theoretic techniques
to enforce adherence to very general software
protocols~\cite{mandelbaum+:refinements}.  This research builds 
upon his foundational graduate work on typed assembly language 
(TAL)~\cite{morrisett+:tal,morrisett+:journal-stal}.

% Recently, he has turned his expertise in type theory and type systems
% on the problem of describing ad hoc data sources used in the
% \pads{} project~\cite{fisher+:pads-semantics}.  

Walker and his students have also developed new techniques for defining,
implementing and reasoning about run-time enforcement of program properties.
For example, he developed a rich new theory of security monitors as formal
automata that transform untrusted program behavior as they 
execute~\cite{ligatti+:edit-automata,ligatti+:renewal}.
This theory allows security
architects to model a variety of different sorts of run-time
enforcement mechanisms, to prove that certain mechanisms can or cannot
enforce various security properties, and to compare the power of
different classes of security monitors.    
Security monitors may be implemented using aspect-oriented 
programming languages, consequently Walker has engaged in a 
broad
language-theoretic study of the semantics of these 
languages~\cite{walker+:aspects,dantas+:harmless-advice,ligatti+:aspectsjournal,dantas+:polyaml}.
Finally, he has implemented new language support for
security monitors in an extension to Java~\cite{bauer+:polymer}.

% Walker's security monitoring language is a 
% form of aspect-oriented programming language.
% In order to better understand aspect-oriented technologies and their
% potential impact on security, Walker formalized and proved
% safe the {\em first} higher-order, strongly-typed calculus of 
% aspects and several 
% variants
% These calculi form the basis for development of sound aspect-oriented
% programming languages and principles for reasoning about them.

Finally, Walker's career grant allowed him to be a leader in
programming languages and security education. In 2004, he organized
a 10-day summer school on software security 
attended by over 70 participants~\cite{summerschool04}.  In 2005, he is 
in the process of
organizing a second summer school on reliable computing~\cite{summerschool05}. 
 He has also written a
chapter of a new textbook on type systems about to enter its
second printing~\cite{walker:attapl}.



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{References Cited}
{\bibliographystyle{abbrv}
 \small\bibliography{pads}
} \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Biographical Sketch}
%\input{dpw-bio}

They are in separate files now (see cv/lastname-cv.tex).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Budget}

The budget pages (no longer necessary because of fastlane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Current and Pending Support (NSF Form 1239)}

The current-and-pending pages (no longer necessary because of fastlane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Facilities, Equipment and Other Resources (NSF Form 1363)}

The facility page (no longer necessary because of fastlane).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Special Information and Supplementary Documentation}

We may consider asking AT\&{}T to provide a support letter.

\end{document}


