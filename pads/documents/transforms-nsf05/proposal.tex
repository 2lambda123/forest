\documentclass[11pt]{article}
\usepackage{times,url}
\usepackage{code} 
%\usepackage{proof}
%\usepackage{newcode}
\usepackage{epsfig}

\input{defs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\voffset             0in    %  top vertical offset
\hoffset             0in    %  left horizontal offset
\oddsidemargin       0pt    %  Left margin on odd-numbered pages.
\evensidemargin      0pt    %  Left margin on even-numbered pages.
\topmargin           0pt    %  Nominal distance from top of page to top of
\headheight          0pt    %  Height of box containing running head.
\headsep             0pt    %  Space between running head and text.
\textwidth         6.5in    %  Width of text on page
\textheight          9in    %  Height of text on page
\setlength{\parskip}{.05in}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\textfraction}{0.1}
\renewcommand{\baselinestretch}{1.01}
%\input{macro}  %%%  other macro definitions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\setcounter{page}{1}
\pagenumbering{roman}
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{NSF 05-518 \datatype{}: Language Support for Processing Ad Hoc Data Sources}
%%%%%%%
\input{summary}
\newpage
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Table of Contents}
% \tableofcontents\
% \vspace*{.1in} 
% \begin{center}
% {\Large\bf\em{}This is a place-holder. Fastlane generates this page automatically.}
% \end{center} 
% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{arabic}
\setcounter{page}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Description}

\subsection{Introduction}
\label{ssec:intro}

An {\em ad hoc data format} is any nonstandard data format for which
parsing, querying, analysis or transformation tools are not readily
available.  \xml{} is not an ad hoc data format --- there are hundreds
of tools for reading, querying, and transforming \xml{}.  However,
computer programmers, financial analysts,
computational biologists, chemists and physicists,
healthcare and airline information systems,
corporate IT professionals and 
others deal with deal ad hoc
data in a myriad of complex formats on a daily basis.
This data is often unpredictable, poorly documented, and
filled with errors, making it extremely difficult
to deal with.  Often, before anything can be done with
ad hoc data one must clense it of as many errors as possible,
normalize its shape and transform it into a standardized format. 
Once in a standardized format, the data
can be directly loaded into a database or manipulated by
standard, widely available tools.  

The goal of this research is to develop a new platform for
efficient and reliable computing with ad hoc data.
More specifically, we propose the following comprehensive 
research agenda.

\begin{enumerate}
\item We will design, study and implement
a {\em universal ad hoc data description language.}
Our descriptions will be capable of
concisely and accurately describing both the basic syntactic structure
as well as the deeper semantic properties of any ad hoc data source.
We will study of the semantic properties of our description language
and implement a compiler that can
generate a parser automatically from a data description.

\item We will design, study, and implement  \datatype{}, a 
high-level programming language {\em with 
intrinsic support for 
processing ad hoc data.}  Our programming language will use the
rich data descriptions both as directives for
parsing ad hoc data sources and as types for describing
representations of ad hoc data within the programming environment.  
In addition,
a critical facet of our language will be its support for
{\em error-aware computing}.  Our error-aware infrastructure 
will allow programmers to conveniently
verify correctness of data relative to a description or 
alternatively detect data errors and handle them in domain-specific
ways.  Finally, we will be sure
our language design is founded on
strong programming language principles by
studying its type system and metatheory extensively. 

\item We will evaluate \datatype's design by implementing 
applications in at least two completely different domains.  First, we will
develop tools for cleaning, normalizing, analyzing and mining
telecommunications data supplied by AT\&T.  This will allow
us to evaluate the performance of our tools on the truly
massive data sets (gigabytes or even terabytes in size)  found in
industry.  Our research in this context also has the 
long-term potential of making
a broad impact on society by improving the reliability of network
monitoring and fraud detection tools
used in the telecommunications industry.  Second, we 
will develop tools for processing data sources of 
interest to computational biologists, genomics researchers 
at Princeton and in the
broader research community.  In particular, we will collaborate with
Olga Troyanskaya from Princeton's Lewis-Sigler Institute for 
Integrative Genomics and Steven Kleinstein, head of Princeton's
Picasso project for interdisciplinary research.
This collaboration will allow to make a broad impact on
research in the natural sciences:  Our general-purpose, reuseable
and reliable tools will help boost the productivity of biologists, 
chemists and physicists.
% Troyanskaya is in the process of developing Magic~\cite{magic}, a database 
% that stores information on gene-gene interactions
% and needs to load that database with information extracted from
% a variety of other sources including, among others,
% the Gene Ontology database GO~\cite{go}.  Kleinstein is currently working on
% a simulator for understanding immune-system responses and is in need of tools for
% loading ad hoc data into his simulator. 

\item We will engage Princeton's undergraduate students in 
a series of independent projects based around the the development and use of
\datatype.  Potential projects include (1) cross disciplinary
research in computer science and biology by building the ad hoc
data processing tools needed for biological research at 
Princeton, (2) building a visual interface so non-programmers can
describe, parse, print, and edit data at a high level of abstraction,
(3) research in compiler and parser optimization for massive data sets 
(4) research in techniques for automatically guessing effective data 
descriptions for simple tabular data, and several more.  Three
Princeton undergraduates Mark Daly, Jon Epstein and Michael Ten-Pow
have already expressed interest in these projects for the coming year.
\end{enumerate}


% We have already begun to develop a system called \pads{} (Processing
% Ad hoc Data Sources) that helps to address some of these concerns.  It
% includes a high-level, declarative language for describing ad hoc data
% formats and it is possible to generate tools for parsing data,
% printing statistical summaries of data and transforming data into
% \xml.  We propose to improve and generalize our work in several
% important ways.

% \begin{enumerate}
% \item The current \pads{} system is far from a universal
% data processing platform.  There are many commonly-used
% data formats \pads{} cannot currently handle.  We propose
% a number of innovative ways of extending the \pads{} description language
% and compiler to alleviate these deficiencies.
% \item The current \pads{} tool-generation infrastructure is brittle and
% inflexible, and the generated tools have sub-par performance.  We propose
% to improve the tool-generation infrastructure by augmenting \pads{}
% with a system of user-defined attributes that can communicate
% semantic information to the tool generator and implementing a mechanism for 
% application-specific customization.
% \item The \pads{} description language has no specification of its 
% meaning or properties.  We propose to define a formal semantics for 
% \pads{}, prove properties of its data processing algorithms and
% use our formal model to re-evaluate, debug and generalize our 
% implementation.  
% \item In addition to our work on using \pads{} to implement applications 
% in the networking and telecommunications domains, part of our mission
% is to have a broad impact on researchers who do data processing
% in the natural sciences, including physics, biology and chemistry.
% We have already developed a partnership with
% Olga Troyanskaya in Princeton's Lewis-Sigler Institute for 
% Integrative Genomics
% and are studying her data processing problems.  We believe that 
% we have a tremendous
% opportunity to make an impact on the productivity of 
% genomics researchers at Princeton
% and elsewhere by supplying them with easy-to-use tools produced by our system.
% \item To extend and support our relationship with the Genomics Institute,
% and to have a broad impact on interdisciplinary education,
% we plan to develop undergraduate research projects in which
% computer science majors use \pads{} to help biologists 
% with their data processing problems.  
% \end{enumerate}

Overall, our research will combine novel language design ideas and
strong theoretical analysis with a serious implementation.  We will
deliver a programming system with the potential to substantially
increase the overall productivity of data analysts, 
information systems administrators
and programmers who
deal with ad hoc data regularly.  Finally, our research has the potential
to have  a
broad impact on the telecommunications industry and on
research in interdisciplinary computational science,
particularly genomics and microbiology where we will focus
our initial efforts.

\subsubsection{Experience and prior work with ad hoc data.}
\datatype's language design and implementation will build upon the experience
we have gained designing and implementing the \pads{}
system~\cite{fisher+:pads,pads-website}.  The \pads{} system 
is freely available software we have developed that allows
users to write \C-like type declarations and interpret them as
descriptions of ad hoc data.  The \pads{} compiler generates libraries
for parsing and printing from these descriptions.  
While \pads{} is an excellent first step in the design of
a language for processing ad hoc data, the experience we have
gained developing and using it demonstrates clearly that 
it has many deficiencies and drawbacks.  Our current research 
will correct these problems with a vastly improved
design and implementation.

The two most substantial differences between \pads{} and 
\datatype{} go to the very core of the two designs.
The first critical difference is that 
\datatype's data description language will be based on
polymorphic, recursive datatypes as found in functional programming languages
as opposed to \C{} structs, arrays
and unions.  By adding recursion, a feature not present in the original
\pads, our new design will allow us to write descriptions for a
number of important tree-structured formats used by computational 
biologists~\cite{go,newick}.  In addition, our initial
experiments indicate that \datatype{} descriptions are anywhere from
10\% to 40\% more concise than the current \pads{} descriptions.  Most
importantly, \datatype{} description are a much better fit for the
light-weight data transformation language we propose to develop.  

The second critical difference is that {\em \pads{} is not a programming
language.}  It merely generates libraries that can be used by \C{}
programmers.  \C{} is a very low-level language that makes
transforming ad hoc data awkward, cumbersome and potentially
error-prone.  More importantly, it provides no intrinsic support for
dealing with the errors that appear in ad hoc data.  In addition, \C's
type system and operational model provide no support for checking the
rich invariants found in ad hoc data either at run time or at compile
time.  In contrast, \datatype{} will be a high-level language with an
elegant and convenient syntax for data-driven programming, intrinsic
support for handling errors, intrinsic mechanisms for checking data
invariants at run time and a sophisticated type system for enforcing
data invariants at compile time.  Overall,
programs written in \datatype{} will be more concise, more reliable,
easier to understand,
easier to maintain, and easier to evolve as data formats evolve
than programs written in \C.

However, the \pads{} implementation will certainly not go unused.  It is a well-engineered
and efficient parser generator with excellent support for massive data sources.
We plan to implement \datatype{} directly on top of \pads{}, extending \pads{}
with new functionality where necessary to support the novel features 
of \datatype.  Using \pads{} directly as a back-end for our
new system will greatly speed up our development process.
Overall, due to our past experience with \pads{} and
our ability to leverage the current implementation in our new design,
we are able, and in fact uniquely qualified, to take on the crucial
research challenge we are proposing here.
  
%We will give and additional comparison of
%our proposed system, \datatype{} and its relation to \pads{} in
%the related work section at
%the end of this proposal.  

\paragraph*{Proposal Structure.}
In the rest of this proposal, we will explain the immense
challenges and risks posed by ad hoc data in more detail.  We will
explain how our proposed system as a whole can deal with them.   
After this introductory section, we will describe the
key components of our new research agenda and explain how our research
will make a broad impact on disciplines well outside the bounds of
traditional computer science and on interdisciplinary education as well.

\subsection{The Challenges of Ad Hoc Data}

There are vast amounts of useful data stored in traditional databases
and \xml{} formats, but there is just as much in ad hoc formats.
\figref{figure:data-sources} provides some information on ad hoc data
formats from the networking and telecommunications domain at AT\&T and
ad hoc data formats used by computational biologists at Princeton.
They include ASCII, binary, and Cobol data formats, with both fixed
and variable-width records arranged in linear sequences or in tree- or
DAG-shaped hierarchies.  The data sources range in size from
relatively small files up to network applications, such as web server
logs, which can be produced at a rate of 12GB per week, to Netflow
applications, which can be produced at over one GB per second.  Common
errors include undocumented data, corrupted data, missing data, and
multiple missing-value representations.

We hope that these examples give the reader a beginning sense of the
nature and pervasiveness of ad hoc data sources.  However, we cannot
emphasize enough just how pervasive ad hoc data is and to what degree
it infiltrates computer systems, businesses, government, and
scientific research.  Remember, just about any program that spits out
useful information in a format other than \xml{}, \textsc{html},
\textsc{jpeg}, \textsc{mpeg} or a few others is generating ad hoc
data.  Hence, most programmers deal with ad hoc data on a individual
basis regularly.  The financial industry is awash with ad hoc data.
Scientific instruments in biology, astrophysics and chemistry output
ad hoc data.  Most daunting of all is perhaps our national
health-care system.  Recently, Newt Gingrich and Hilary Clinton have
jointly proposed we consolidate and integrate our healthcare
records from across all systems in the United States
in the next 10 years.  Such a massive project will undoubtedly
require technical solutions at many levels.  At the lowest levels, we
will need a reliable software to collect and transfer health care data
from legacy systems into modern formats.  The \datatype{} system we
propose will be an invaluable aid in the development of reliable software
for this very task.

\begin{figure*}
\begin{center}
\begin{tabular}{@{}|l|l|l|l|}
\hline
Name: Use                           & Representation    
%& Size
           & Common Errors \\ \hline\hline
Web server logs (CLF):                & Fixed-column      
%& $\leq$12GB/week 
& Race conditions on log entry\\ 
Measuring web workloads               & ASCII records     
%&                             
& Unexpected values\\ \hline
AT\&T provisioning data (\dibbler{}): & Variable-width    
%& 2.2GB/week 
& Unexpected values \\ 
Monitoring service activation         & ASCII records     
%&            
& Corrupted data feeds \\ \hline
Call detail:                   & Fixed-width       
%&\appr{}7GB/day 
&  Undocumented data\\
Fraud detection 
                                      & binary records  
%& 
& \\ \hline 
AT\&T billing data (\ningaui{}):      & Cobol  
%& \appr{}4000 files/day, 
& Unexpected values\\ 
Monitoring billing process   &                             
%& 250-300GB/day    
& Corrupted data feeds \\ \hline
IP backbone data (\darkstar{})  & ASCII  
%& $\ge$ 15 sources  
& Multiple missing-value rep's \\
Network Monitoring:  &        
%& \appr{}15 GB/day              
& Undocumented data \\ \hline
Netflow                               & Data-dependent      
%& $\ge$1Gigabit/second  
& Missed packets\\ 
Network Monitoring:        & number of   
%&                       
& \\
                                      & fixed-width 
%&
& \\
                                      & binary records 
%&
& \\ \hline
Gene Ontology data:        & Variable-width ASCII records 
%& ? 
&  \\
Gene-gene correlations in Magic & in DAG-shaped hiearchy 
%&  
& \\
database &
%& 
& \\\hline
Newick data                          & Fixed-width ASCII record 
%& ? 
& Manual entry errors \\
Immune system response simulation & in tree-shaped hierarchy 
%& 
& \\
\hline
\end{tabular}
\caption{Selected ad hoc data sources.}
\label{figure:data-sources}
\end{center}
\end{figure*}


% In this domain, it is possible to categorize sources of ad hoc data broadly
% as {\em online data} and {\em offline data}.  Online data
% is data sent over the wire that networking software actively
% reads, interprets and reacts to.  For instance, servers continuously
% listen for requests and react promptly to clients.  Intrusion detection 
% systems and performance evaluation systems
% monitor network activity continuously and signal administrators
% when they detect problems or anomalies.  On the other hand, offline data
% does not necessarily have to be processed in real time.  Offline
% data comes in the form of web server
% logs~\cite{wpp}, netflows capturing internet traffic~\cite{netflow},
% log files characterizing IP backbone resource utilization and telephony
% call detail data~\cite{hancock-toplas}, to name just a few.

% While offline and online ad hoc data have many features in common,
% they also have a few differences.  Software dealing with online data
% is amongst the most vulnerable software that we deploy on a network.
% Extraordinary lengths must be taken to ensure attackers, who can
% supply this software with unexpected data and can reach it in
% real-time, cannot exploit errors in processing code to take control of
% servers, performance monitors or intrusion detection software itself.
% A cautionary example of the dangers of online ad hoc data processors
% is the Ethereal system~\cite{ethereal}.  Ethereal is used by network
% administrators for monitoring, analyzing and troubleshooting networks.
% Unfortunately, like most network software, users have found a number
% of vulnerabilities in the software, and moreover many of these
% vulnerabilities are directly related to the mundane components of the
% system that parse ad hoc data as opposed to the parts of the system that
% perform higher-level tasks.  For instance, in March 2004, Stefan Esser
% posted an advisory on 13 different buffer overflow attacks on
% Ethereal~\cite{etherealvulnerabilities}.  Of the 13, 9 attacks
% occurred during parsing.  Therefore, in addition to all the problems
% associated with processing offline data, those who deal with
% online data must be cognizant of the substantial security risks
% inherent in each line of code they write.

Processing all this ad hoc data is challenging for a variety of reasons. 
First, ad hoc data typically arrives ``as is'': the analyst or system
that receives it can only say ``thank you,'' not request a more
convenient format.  Second, documentation for the format may not exist
at all, or it may be out of date.  A common phenomenon is for a field
in a data source to fall into disuse.  After a while, a new piece of
information becomes interesting, but compatibility issues prevent data
suppliers from modifying the shape of their data, so instead they
hijack the unused field, often failing to update the documentation in
the process.

Third, such data frequently contain errors for a variety of reasons:
malfunctioning equipment, race conditions on log entry~\cite{wpp},
non-standard values to indicate ``no data available,'' human error in
entering data, unexpected data values, \etc{} The appropriate response
to such errors depends on the application.  Some applications require
the data to be error free: if an error is detected, processing needs
to stop immediately and a human must be alerted.  Other applications
can repair the data, while still others either set data aside for
a human user to investigate or simply discard the erroneous
or unexpected values altogether.  For some applications, errors in the data can
be the most interesting part because they can signal where two systems
are failing to communicate.

A fourth challenge is that ad hoc data sources can be high volume:
AT\&T's call-detail stream contains roughly 300~million calls per day
requiring approximately 7GBs of storage space. Although this data is
eventually archived in a database, analysts mine it profitably before
such archiving~\cite{kdd98,kdd99}. More challenging, the \ningaui{}
project at AT\&T accumulates billing data at a rate of 250-300GB/day,
with occasional spurts of 750GBs/day. Netflow data arrives from Cisco
routers at rates over a gigabyte per second~\cite{gigascope}! Such
volumes mean it must be possible to process the data without loading
it all into memory at once.

A final challenge is that ad hoc data often needs to be translated
into a new, more useful or more standard format before anything
can be done with it.  Intuitively, this transformation
can be done in three stages.  First, one must generate a parser for
the ad hoc format to read it into a program.  Second,
one must engineer the transformation itself by
filtering unwanted parts, normalizing representations, and
detecting and correcting or deleting erroneous data.  Third,
transformed data must be printed in the new format that can be processed by
standards tools.

Today, people tend to use \C{} or \perl{} for this task.
Unfortunately, writing parsers, transformations and printers this way
is tedious and error-prone, complicated by the lack of documentation,
convoluted encodings designed to save space, the need to produce
efficient code, and the need to handle errors robustly to avoid
corrupting down-stream data.  Moreover, the parser writers' hard-won
understanding of the data ends up embedded in parsing code, making
long-term maintenance difficult for the original writers and sharing
the knowledge with others nearly impossible.

\subsubsection{\datatype{}:  Taking on the Challenge of Ad Hoc Data}

The \datatype{} system will make life easier for data analysts by addressing
each of the concerns outlined in the previous section.
It provides a declarative data description
language that permits analysts to describe the physical layout of
their data, \textit{as it is}.  The language also permits analysts to
describe expected semantic properties of their data so that deviations can
be flagged as errors. The intent is to allow analysts to capture in a
\datatype{} description all that they know about a given data source.
When the data descriptions are coupled with the computational element
of \datatype{}, programmers will have a quick and easy way to
develop efficient and reliable scripts for processing ad hoc data.


\figHeight{architecture-grant}{Processing ad hoc using the \datatype{} system.  Green-shaded components are written by
the programmer. The sequences 010010100100.. and
101101001011.. stand for the input and output ad hoc data
respectively. The words ``Rep, MD'' stand for the pair of internal 
data representation and its associated metadata including error 
descriptions.}{fig:dsys}{2in}

\datatype{} descriptions are intended to be concise enough to serve as
documentation and flexible enough to describe any ASCII, binary,
Cobol, and mixed data formats.  Ideally, programmers write
a single description for a data source and then develop
many useful software tools based on
the single description.  This one-description/many-tools feature provides
strong incentive for keeping the descriptions current, allowing them
to serve as living documentation for a data source.
  
Figure~\ref{fig:dsys} shows how one builds a typical transformation
tool for processing ad hoc data using \datatype{}.  The first step is
to write down \datatype{} descriptions for the input data and output
data.  In some cases, such as when a tool is simply doing {\em data
cleaning}, the process of error detection and correction, the input
and output data formats will be the same.  In other cases, \datatype{}
may simply serve as a front-end tool for getting ad hoc into a
programmable system and an output description will not be needed.
Once the descriptions have been defined, the \datatype{} description
compiler will be able to produce customizable libraries for parsing
and printing the data.  The generated parsing code checks all possible
error cases: system errors related to the input file, buffer, or
socket; syntax errors related to deviations in the physical format;
and semantic errors in which the data violates user constraints.
Because these checks appear only in generated code, they do not
clutter the high-level declarative description of the data source.
Moreover, since tools are generated automatically by a compiler rather
than written by hand, they are far more likely to be robust and far
less likely to have dangerous vulnerabilities such as buffer
overflows.  The Ethereal system~\cite{ethereal}, which is used for
monitoring network performance and security, is a cautionary example
of what happens when people write parsers for ad hoc data by hand ---
they are bound to get it wrong.  A recent security analysis of the
Ethereal system~\cite{ethereal-attack} found 13 new vulnerabilities
that allowed a malicious attacker to take control of the system, an
unfortunate situation for software intended in part to improve the
security of a system. Nine of 13 were buffer overruns found in the
parsing code.


The result of a parse is a pair consisting of a canonical
in-memory representation of the data and meta-data for the parse.  We
refer to the meta-data as the {\em parse descriptor}. The parse
descriptor may hold a variety of bits of information including
the position of the data in the file and a characterization
of the possible errors in the data.  There are several
different sorts of errors that may arise.  Syntactic errors occur
when a parser cannot read a valid item of the right type from the file 
(eg: a parser attempting to read an integer finds the character '?'
instead).  Semantic errors occur when a parser can read an item
but it does not satisfy the appropriate semantic condition (eg: the
parse finds an integer in the file, but the integer is $-1$ when it
should be greater than zero).  

The computational part of the \datatype{} language will form the glue between
the input data format and the output format.  Transformations written
\datatype{} will be able to handle error-free data using a concise 
notation.  In addition, programmers will have the freedom to
query the parse descriptor coupled with the data representation. 
This structure allows programmers
to respond to errors in application-specific ways.

With such huge datasets, performance is critical. The \datatype{}
system will address performance in a number of ways.  First, we will
compile the data description to \pads{} rather than simply interpret
it. \pads{} itself is compiled into efficient \C{} code.  Second, we
will support lazy processing of data streams for very large data
sources.  The lazy stream-reading system will read manageable chunks
of a large data source into the run-time environment and flush them
when they are no longer needed.  Finally, we will exploit another
portion of the underlying \pads{} implementation: the \textit{parsing
masks}.  \pads{} parsing masks allow data analysts to choose which
semantic conditions to check at run-time, permitting them to specify
all known properties in the source description without forcing all
users of that description to pay the run-time cost of checking them.
\datatype{} programmers will be given access to this functionality as
well.

% We propose to extend and improve the current \pads{}\ prototype by engaging
% in three intertwined research efforts:

% \begin{enumerate}
% \item Our experience with real-world data has revealed that
% the current \pads{}\ system is unable to process many common
% data formats.  We propose a number of extensions to \pads{} 
% that will greatly improve its expressive power and enable
% us to handle many more important ad hoc formats.
% \item While \pads{}\ can automatically generate tools for providing
% statistical data summaries, querying ad hoc data and generating
% \xml, the tool generation process is extremely brittle, inflexible
% and results in low-performance software.  We propose to redesign
% the automatic tool generation process using a principled methodology
% based upon a novel \pads{}-attribute system that can communicate
% semantic information across tools.  Our new methodology will
% improve the flexibility and reliability of the \pads{} system.
% \item There is no formal description of the \pads{} language
% and no semantics for the compiler or tools.  We propose to
% formalize the language and analyze the semantics of the
% compiler.  Our formal analysis may find bugs or flaws in the
% \pads{} system and will improve our confidence in the implementation.
% \end{enumerate}

% \noindent
% In addition, part of our mission will be to use \pads{} to provide
% data processing tools for researchers in the natural sciences including
% biology, chemistry and physics.  

% In building the \pads{} system, we faced challenges on two fronts:
% designing the data description language and
% crafting the generated libraries and tools.
% In the rest of the paper, 
% we describe each of these designs.
% We also give examples to show how useful they have been at AT\&T.


\subsection{Towards a Universal Ad Hoc Data Description Language}

In order to give the reader a sense of our proposed data
description language and some of the challenges, we
will take a look at an example of ad hoc data:
a tiny fragment of provisioning data from AT\&T and show
how our proposed \datatype{} language can be used to describe it.
In the telecommunications industry, the term \textit{provisioning} refers to 
the steps necessary to convert an order for phone service into the actual 
service.  
To track AT\&T's provisioning process, the \dibbler{} project compiles
weekly summaries of the state of certain types of phone service orders.  
These ASCII summaries store the summary date and one record per order.
Each order record contains a header followed by a sequence of events.
The header has 13 pipe separated fields: the order number, AT\&T's
internal order number, the order version, four different telephone
numbers associated with the order, the zip code of the order, a
billing identifier, the order type, a measure of the complexity of the
order, an unused field, and the source of the order data.  Many of
these fields are optional, in which case nothing appears between the
pipe characters.  The billing identifier may not be available at the
time of processing, in which case the system generates a unique
identifier, and prefixes this value with the string ``no\_ii'' to
indicate the number was generated. The event sequence represents the
various states a service order goes through; it is represented as a
new-line terminated, pipe separated list of state, timestamp pairs.
There are over 400 distinct states that an order may go through during
provisioning.  The sequence is sorted in order of increasing timestamps. 
\figref{figure:dibbler-records} shows a small example of
this format.
%156 different states for one order
%-rw-r--r--    1 angusm   dibbler   2187472314 Jun  9  2003 /fs/dibblerd/tlf/data/out_sum.stream
%2171.364u 31.379s 40:41.54 90.2% 0+0k 2+0io 2pf+0w
%53 had trailing t or } after zip code
It may be apparent from this paragraph that English is a poor
language for describing data formats!


\begin{figure*}
\begin{small}
%\begin{center}
\begin{verbatim}
0|1005022800
9152|9152|1|9735551212|0||9085551212|07988|no_ii152272|EDTF_6|0|APRL1|DUO|10|1000295291
9153|9153|1|0|0|0|0||152268|LOC_6|0|FRDW1|DUO|LOC_CRTE|1001476800|LOC_OS_10|1001649601
\end{verbatim}
\caption{Tiny example of \dibbler{} provisioning data.}
\label{figure:dibbler-records}
%\end{center}
\end{small}
\end{figure*}

A \datatype{} description specifies the physical layout and 
semantic properties of an ad hoc data source. 
The language provides a type-based model:
basic types describe atomic data such as integers, strings, dates, \etc{}, while
structured types describe compound data built from simpler pieces.
\suppressfloats


\begin{figure}
\begin {code}
\input{dibblerml}
\end{code}
\caption{\datatype{} description for \dibbler{} provisioning data.}
\label{figure:dibblerml}
\end{figure}

The \datatype{} will provide a large collection of broadly useful base
types by exploiting the fact that we have already implemented many,
many such types in the underlying \pads{} backend.  Examples of base
types include 8-bit unsigned integers (\cd{Puint8}), 32-bit
integers (\cd{Pint32}), dates (\cd{Pdate}), strings (\cd{Pstring}),
and IP addresses (\cd{Pip}).  Semantic conditions for such base types
include checking that the resulting number fits in the indicated
space, \ie, 16-bits for \cd{Pint16}.  By themselves, these base types
do not provide sufficient information to allow parsing because they do
not specify how the data is coded, \ie{}, in ASCII, EBCDIC, or binary.
To resolve this ambiguity, \datatype{} will allow users to set an
ambient coding discipline.  By default, it will use ASCII.  In addition to
these built-in types, we propose to allow users to define their own new base 
types to specify more specialized forms of atomic data.  We
will need to discover a convenience and effective way to do this.

Types can be parameterized by values.  This mechanism serves
both to reduce the number of base types and to permit the format and
properties of later portions of the data to depend upon earlier
portions.  For example, the base type \cd{Puint16_FW(3)} specifies
an unsigned two byte integer physically represented by exactly three
characters, while the type \cd{Pstring(' ')} describes a string
terminated by a space.

To describe more complex data, \datatype{} provides a collection of
structured types loosely based on the type structure of functional
programming languages such as Haskell and ML.  
\figref{figure:dibblerml} gives a \datatype{} description for the
\dibbler{} provisioning data written in a syntax we are currently
designing.  We will use this example to illustrate some of the
features of \datatype{} language.  

Overall, the \datatype{} data
description is a sequence of type definitions.
It is probably easiest to understand the data source by
reading these descriptions bottom up.
The last type definition \cd{source} is intended to be
a definition of an entire \dibbler{} data source.  The
type description states that a \cd{source} is a
\cd{summery\_ header} followed by a sequence (\cd{Parray}) of objects
made up of an \cd{order\_header} followed by \cd{events}.
The \cd{Parray} type depends upon two value parameters.
The first parameter describes the syntactic separators 
that may be found between elements of the array.  In this
case \cd{NL} (the newline character) may be found between
each element of the array.  In other words, once 
the \cd{summery\_ header} has been parsed, each line
of the data source will contain an 
\cd{order\_header} and \cd{events}.  The second parameter
is the terminator for the array.  In this case,
the terminator is the end-of-file marker.  The array
is considered completely and successfully parsed when 
the end-of-file marker is reached.

% Different cases in the datatype describe
% different alternatives in the ad hoc data.  Recursion can be used to
% specify flat representations of tree-like data.  Dependency makes
% it possible for later parsing to depend upon earlier parsing.
% There will also be (dependent) records, tuples and arrays for describing sequences of data
% items.  Singleton types will be used to describing literal characters that must
% appear in the ad hoc data source. Each of
% these types can have an associated predicate that indicates whether a
% value calculated from the physical specification is indeed a legal
% value for the type.  For example, a predicate might require that two
% fields of a record are related or that the elements of a
% sequence are in increasing order.  Programmers can specify such
% predicates using \datatype{} expressions and functions, written in an
% ML-like syntax.  Finally, \datatype will allow programmers to
% define their own new type abbreviations.

% \begin{figure}
% {\small
% \input{dibbler_new}
% }
% \caption{\pads{} description for \dibbler{} provisioning data.}
% \label{figure:dibbler}
% \end{figure}



The definitions of \cd{events} indicate that
this part of the \dibbler{} data will contain a sequence
of \cd{event}s separated by verticle bars and terminated by a newline.
Each \cd{event} is a string terminated by a veriticle bar,
followed by a verticle bar and ending with an unsigned
32-bit integer.  The interesting part of this sequence is the
presence of the type \cd{'|'}.  In type-theoretic terms, this is
a {\em singleton type}.  It states that one should
 expect exactly the character \cd{'|'} in the input stream at this point.
Other singletons appear in the summary header type as \cd{"0|"}
and NL (the newline character).

The type \cd{order\_header} is a record type that indicates
the data format involves the sequence of items described by
the fields of the record.  Notice that there are two different
sorts of fields: anonymous fields containing directives to parse
a particular character (\cd{'|'}), like the singleton types,
and fields with names.  The second named field,
\cd{att\_order\_num}, reveals two other proposed features of 
\datatype: dependency and constraints.  Here,
\cd{att\_order\_num} is constrained to be less than
\cd{order\_num}, the value parsed in an earlier field.
This is a relatively simple constraint on the correctness of the
ad hoc data format.  In practice, constraints can become very rich
involving properties such as sortedness of records in an array,
restrictions on date and time ranges, constraints on IP addresses,
restrictions on phone number formats, addresses and virtually 
infinite variety of other possibilities. 

The last interesting feature in the \dibbler{} example is the
datatype definition of \cd{dib\_ramp}.  It describes
two alternatives for portion of data, either an integer alone
or the fixed string \cd{"no\_ii"} followed by an integer.
In order to parse data in this format, the parser will
first attempt to parse the first branch and only if it
sales will it attempt to parse the second branch.

\begin{figure}
\begin{code}
type entry = Pstring(':') * ':' * Puint32
\mbox{}
datatype tree =
    Tree of '(' * tree Parray(',', ')') * "):" * Puint32
  | Tip of entry
\mbox{}
type trees = (tree * ';') Parray(NL,EOF)
\mbox{}
(* Tiny data fragment with type trees:
\mbox{}
(B:10,(A:34,C:15,E:23):4,D:2):12;
((A:4,E:22):4,D:2):11;
\mbox{}
*)
\end{code}
\caption{Simplified Tree-shaped Newick Data}
\label{figure:newick}
\end{figure}

A second interesting example of ad hoc data comes courtesy of
Steven Kleinstein, head of Princeton's Picasso project for
interdisciplinary research in computational sciences.  
Kleinstein is in the process of 
building a simulator to study immune response.  Data
needed for his simulations comes in a Newick format, which is
a flat representation of trees and used by many biologists~\cite{newick}.  
In Kleinstein's Newick format (simplified
here for expository purposes), leaves of the
tree are string labels followed by a colon and a number.
A parent node in the tree introduces a collection of
children by placing a sequence of trees within parens.
Following the parens is a colon and a number, as is the case
for the leaf node (incidentally, the numbers are called ``distances''
and represent the number of genetic mutations that separate the
child from the parent).  Each line of a file may contain
a different tree, terminated by a semi-colon.

\figref{figure:dibbler-records} gives a description of Newick
and a bit of example data.
Despite the relative complexity of the structure of the data, 
the description is remarkably concise.
Notice that the data type definition of \cd{tree} is recursive ---
there appears to be no effective description of this data
source without it.  

\subsection{The \datatype{} Transformation Language}

Often, the first thing one wants to do with ad hoc data 
is to transform it into some standard form, such as \xml{}
or one of the standard database formats.  This way,
the ad hoc data can be read directly into the database
and archived there.  Once the data has made its way into a database, 
users can write standard XQuery or SQL to extract the information 
they need.  Since maintaining the integrity of information
within a database is crucial, conversion to a standard format
is usually accompanied by a data cleaning phase that detects
and attempts to correct any errors it finds in the data source.
This phase may also filter out unneeded data, convert dates,
times and other information with a variety of formats into
a canonical form, rearrange the order of elements,
and otherwise massage or transform the data.

We intend to design \datatype{} 
to support writing clear, reliable and efficient programs to
accomplish these data transformation tasks.
Overall, the language will be functional and make
heavy use of pattern matching.  We believe this design will
allow programmers to quickly and easily write concise and correct
programs.  

The first critical innovative new feature of the language
is that the polymorphic, dependent
data descriptions (\cd{source}, \cd{entry}, etc.)
will be used as types in the programming language.  These
types will describe the data structures that represent
ad hoc data once it has been
parsed and read into the run-time system.  Since \datatype{} descriptions
are parameterized by values and depend upon complex semantic constraints
this leads to a rich type structure for our programming language.
A central research problem will involve developing the type system
and its type inference algorithm.

A second key feature is the intrinsic support the programming language provides
{\em error aware computing}.  As mentioned repeatedly in this proposal,
errors are an intrinsic part of ad hoc data that must be recognized
and dealt with.  In fact, for some applications, errors are the most interesting part of the data as they indicate where software or
hardware may be failing.  The \datatype{} programming infrastructure
will help programmers deal with errors as they see fit.
As we mentioned in previous
sections, the parsers generated from \datatype{} data descriptions
will produce pairs of objects that include
the actual data representation {\em and} a parse descriptor (PD)
that carries metadata concerning the data representation,
including any errors that may present in the data.
We will refer to the pair of ad hoc data representation and corresponding PD
as an {\em ad hoc value}.

One of the crucial design principles of our language will be
that programmers never write PDs themselves; the
PDs are always constructed automatically by the
compiler system.  This important principle allows the compiler
to guarantee that an ad hoc value's PD always accurately 
describes the representation.
It will be impossible for a programmer to corrupt the relationship
between representation and PD accidentally.  

As a first cut, the PD for any ad hoc
value may indicate one of the following things:
\begin{itemize}
\item {\tt G}: this is \cd{G}ood data; it contains no syntactic or 
semantic errors, not even nested deeply within its structure.
\item {\tt B}: this is \cd{B}ad data; it is not even syntactically correct
\item {\tt S}: this is syntactically valid data, but a \cd{S}emantic 
constraint does not hold.
\item {\tt R}: the top-level structure of this data is good, but some
(\cd{R}ecursive) substructure is either syntactically or semantically invalid.
\item {\tt U}: this data may be good, but I am \cd{U}nsure.  The programmer 
will have to check the substructure herself.  This last descriptor
is important for processing massive data streams where the system reads the
stream lazily and cannot know in advance whether or not the stream
as a whole will be good.
\end{itemize}

\noindent
Although programmers cannot construct a parse descriptor and pair it with a
representation to create a value, the programmer may use pattern matching
to extract and read a value's parse descriptor.  Notice that
when the programmer determines that a value's parse descriptor is
\cd{G}ood, this implies the entire substructure is error free
and need not be checked further for errors.  This 
greatly facilitates programming with data that is expected
to be error-free in the common case ---  a programmer can simply check 
the top-level PD and if it is indeed \cd{G}ood,
she need not clutter the rest of her code with error-checking
preteens.  In addition, when the top-level PD is \cd{G}ood,
the value representation might perhaps be optimized as the
PDs for the substructures are unnecessary.  Of course, we
need to do much more research before we will understand how to 
represent and compute with ad hoc data values most efficiently.


\begin{figure}
\begin{code}
(* open file to introduce \dibbler type definitions for
   event, events, source, etc.  See Figure \ref{figure:dibblerml}. *) 
\mbox{}
open Sirius
\mbox{}
type streams = entry stream * entry stream
\mbox{}
fun splitEntry (e:entry * streams) : streams =
  case e of
    (<<entry, G>>, good, bad) => (entry::good, bad)
  | (<<entry, _>>, good, bad) => (good, entry::bad)
\mbox{}    
fun splitSource (s:source) : streams =
    let (hdr,Parray(entries,_,_)) = s in 
    stream\_fold splitEntry (nil, nil) entries
\mbox{}
fun app (fds : fd * fd * fd) : unit =  
  let (fdin,fdgood,fdbad) = fds                in
  let input               = Pread::source fdin in
  let (good,bad)          = splitSource input  in
  Pwrite::(entry Parray(NL,EOF)) (Parray(good,NL,EOF), fdgood);
  Pwrite::(entry Parray(NL,EOF)) (Parray(bad,NL,EOF), fdbad)
\end{code}
\caption{Error filter for \dibbler{} data}
\label{figure:newick-clean}
\end{figure}


\figref{figure:newick-clean} presents a very simple \datatype{}
program in a syntax we are in the process of designing.  The
intent here is to give the reader a basic idea of the initial direction
for our design --- it is by no means well worked out.  Much additional
research is required.  This program does a very simple and common
transformation.  It takes the \dibbler{} data source and
walks through all of the entries in the source checking for
entries with errors.  The good entries are placed into
one output stream and the bad entries are placed into another.
The idea is that the good entries may then be further processed
or directly loaded into a database without corrupting
the valuable data therein.  A human might examine
the bad entries off-line to determine the cause of errors
and to figure out how to fix the corrupted entries.

The command in the program opens the \dibbler{} data description module.
This enters the type definitions into the programming environment
(see~\figref{figure:dibblerml} for the details of these definitions).
Next the \cd{splitEntry} function describes how to check an entry
to determine whether it is \cd{G}ood (syntactically and semantically
valid) or not.  The \cd{G}ood entries are placed in the \cd{good}
stream and the other entries are placed in the \cd{bad} stream.
Here, \cd{<<pat,pdpat>>} is a pattern in which \cd{pdpat} is
a pattern for PDs and \cd{pat} is a pattern for the recursive structure 
of the value.  We do not actually look at the recursive structure of the
value, because once we see the \cd{G} PD associated with an entry
we know that the entire substructure is good.  
In this example, we use the notation \cd{entry::good} to put
an \cd{entry} on the front of the \cd{good} stream. 

The \cd{splitSource} function pattern matches against the
input source, extracting the stream of entries, and iteratively
applying the \cd{splitEntry} function.  In this example,
\cd{Parray(entry,\_,\_)} is a pattern for array values.
An array value is a stream (in this case \cd{entry})
coupled with a separator and a terminator.  Also in this example,
we assume \cd{fold\_stream} 
iteratively applies a function to a stream.  We intend to
scour the existing research on stream processing to help us
develop a stream programming model for \datatype. 

The function \cd{app} is responsible for reading in
streams from the file descriptors it is given,
splitting the streams into good and bad data
and writing the streams back out to files.
The process of reading and writing is dependent on the
type descriptions of the ad hoc data.  Here, we write
\cd{read::T} to read a data source formatted according to \cd{T}
and \cd{write::T} to write a format according to \cd{T}.
The read function generates an ad hoc data value with
type \cd{T} and the write function writes an ad hoc data value with
type \cd{T} to a file.

This example does not focus particularly on the type structure of the
programming language, but since the \datatype{} descriptions
are usually dependent and often involve very rich constraints,
we have much research to do on the structure of the \datatype{} type
system and type inference algorithm.

\subsubsection{Specific Research Plan}

The previous subsections should convince the reader 
that ad hoc data can be found across all areas of science,
industry and government.  We desperately require new tools for processing
this data, which comes in an infinite variety of shapes and styles,
and is often poorly documented and filled with errors.  We also hope to
have given the reader a flavor of the
universal data description language we propose to describe
all kinds of ad hoc data as well as the structure of
the \datatype{} programming language, which we plan to use to
manipulate, analyze and transform ad hoc data.
Our ultimate goals for the data description language are (1) to make it
as expressive as possible, enabling descriptions of
any data format, (2) to keep the notation lightweight, 
concise and easy to understand at a glance, and (3)
to be a good fit with the data processing and
transformation components of the \datatype{} language.
Our ultimate goals for the \datatype{} programming language
are (1) to make it easy and convenient to write reliable
scripts for processing any ad hoc data source, (2)
to support programming in the presence of the errors that
almost invariably arise in ad hoc data sources and (3)
to support efficient processing of massive data sets.
In this subsection, we briefly outline the major steps
we will take to realize our language design goals.

\paragraph*{Design of basic type structure including 
polymorphic, recursive and dependent datatype specifications.}
We will first design the concrete syntax of the data description.
As indicated in the previous section, we already have worked
out the general ideas behind the basic design.  Now we need to make 
it precise and fill in the details.  The central novelty
here will be the definition of the recursive, polymorphic
and dependent datatype specifications.

We have not mentioned polymorphic definitions yet in any detail,
but our experience with several dataformats indicates
they will be very useful for specification reuse.  
For example, in the common format for web server logs~\cite{wpp}, optional
values are represented as either a single dash (if the item does not
appear) or the item itself (if it does appear).  Currently, for every
different sort of optional value (\ie ``dash or integer'' vs. ``dash
or string''), one must write down a separate specification.  It should
be possible to eliminate this sort of redundancy and reduce the size
of format specifications using polymorphic types (types parameterized
by other types).  In our previous examples, we used
{\cd{Popt}} type constructors, which were intended to be
``nothing or something'' pervasively.  With polymorphism,
these sorts of constructors would not need to be built in to the
system.  A user could defind new constructors of this ilk 
whenever they chose to.

In addition, we will be developing a syntax for the more mundane
elements of a language: tuples, records, arrays
(parameterized) type definitions, base types etc.

\paragraph*{Implementation of the basic type structure.}
In order to implement our data description language,
we will compile the basic types into our old system, \pads.
In order to compile the recursive and polymorphic
elements of our design, 
two features our experience with real-world data sources
has shown are invaluable,
we will have to extend the underlying \pads{}
implementation.  These extensions to \pads{}
are nontrivial.  In particular,
we need to transform \pads{} current ``descent'' parser into
a ``recursive descent'' parser.  
There are also a number of changes that must be made to
the generated libraries to handle recursion.  
We will have to make similar sorts of extensions
to \pads{} to handle polymorphism as well.  However,
once the extensions to \pads{} are complete,
we believe we will be able to compile recursive data types
into specifications based on \pads{} structs and unions.
However, the optimal compilation strategy is certainly
an important research question.

\paragraph*{Advanced Features:  Data with Pointers.}
Some data formats contain explicit embedded pointers 
from one part of the data source to another.
For example, the Gene Ontology (GO)
Project~\cite{geneontology} is a data repository used by Olga
Troyanskaya (Princeton Lewis-Sigler Institute for Integrative
Genomics) and other biologists who study protein-protein interactions.
One component of the GO data repository classifies reactions by
placing them in a DAG-structured hierarchy.  In order
to specify and represent this data internally in a 
space-efficient manner, we cannot simply rely on
recursive datatype descriptions as they will
generate trees of data internally.  Instead,
we believe we need a second mechanism which will allow us to specify
a pointer from one part of the data representation to another.
Another similar example occurs in
part of the DNS packet format.  This format compresses
host names by using pointers to point back to common
hostname suffixes. 

We plan to consider these and similar data sources in more detail
in order to come up with an effective and general specification mechanism.
If funded, we will do more research on this topic and come up with
a concrete design.

% \paragraph*{Advanced Features: Multi-stage Data Processing.}
% Many data sources require multi-stage processing.  For instance,
% security-sensitive data may be encrypted and high-volume data
% is often compressed.  On the way into the system, a first 
% pass must decrypt or decompress the data before a second pass
% does the parsing work on the underlying format.  On the way out of the system, 
% it may be necessary to apply reverse transforms.  

% One solution
% to this problem might be to let auxiliary passes through the data
% remain outside the \datatype{} system.  Unfortunately, this solution
% leaves us in a situation in which \datatype{} descriptions 
% are not self-contained
% definitions of the data format in question.  Consequently some of the value
% of \datatype{} as documentation is lost.  
% In addition, some data formats~\cite{korn+:delta,korn+:data-format} are not 
% uniformly encrypted or compressed.
% The data may contain subsections that are encrypted, and more troublesome,
% the encryption or compression algorithms may be data-dependent so
% disentangling the compression and encryption stages 
% involves just the sort of parsing that \pads{} was built for.

% We hope to be able to facilitate multi-stage data processing 
% directly in \datatype.  As a
% first step, we will add staging directives with the form 
% \cd{preprocess(args) \Pthen T} to the language of types.
% The command \cd{preprocess(args)} performs some preprocessing step, be it
% decryption, decompression or otherwise.  The results of the preprocessing
% will then be fed into the parser generated from the type \cd{T}.
% The \Pthen\ type will be a first-class type and therefore it will compose
% with any other PADS specification.  For instance, it will be straightforward to
% decompress and then decrypt:  

% \cd{decompress(length) \Pthen decrypt(key, length) \Pthen T}.  

% \Pthen\ statements are designed to compose with other PADS features, making
% it easy to do data-dependent processing.

% \paragraph*{Advanced Features:  Integrating Multiple Data Repositories.}
% Sometimes, a single logical data source is represented
% as several distinct, concrete repositories.   This is the case
% in the GO data source used by Troyanskaya. 
% In GO,  data is split into four disjoint
% files: a molecular function file, a biological process file,
% a cellular component file and a term definitions file.

% % begin BS

% We believe that the right way to describe such data sources
% is to introduce a notion of {\em data module} into \pads{}.
% In this case, a data source as a whole may link
% a series of data modules together.  One of the goals
% of this design would be to allow enough flexibility that
% the whole data source could be processed together or a single module
% (such as for molecular function module) could be processed on its own.
% However, it remains unclear if this is indeed the right processing model;
% more research and experimentation is necessary to determine the
% pros and cons of such a set up.

% % end BS

\paragraph*{\datatype{} transformation language design}
We will begin by designing the basic syntax of commands and operations
for the language.  The initial design will resemble the examples
given previously in the paper.  As we gain more experience with further
examples and experimentation, the design will undoubtedly evolve.

Initially, we will develop a {\em dynamically-typed} language in which
the data descriptions (in addition to directing parsing and printing)
are used as {\em contracts} in the sense of 
Effel~\cite{},
a variety of other object-oriented languages and most recently Findler and 
Felleisen~\cite{findler+:contracts}.  In other words, when 
a programmer ascribes a type to an ad hoc
value (\eg{} \cd{(e : T)}), the run-time system will dynamically 
check that the value computed by \cd{e} satisfies
the dependent constraints given in \cd{T}.  

This initial phase may turn into a complete scripting language
for rapid prototyping work involving ad hoc data.  However, the
pervasive dynamic checking may hinder performance.  Moreover,
larger programs will need to be organized around static type 
structure.

\paragraph*{\datatype{} type system and inference algorithm.}
The second phase of the language design will involve augmenting the
original design with a static type system.  Development of a dependent
type system and inference algorithm for the language will be a
significant research challenge.  However, we are confident we will be
able to make use of recent work on dependent type systems,
particularly work by Zenger~\cite{zenger:indexed-types} and Xi and
Pfenning~\cite{xi+:dml}.  Of course, to meet our goals, we will have
to extend the previous authors work substantially in order to deal
with the sophisticated constraint systems that appear in ad hoc data.
The PI has extensive experience with dependent type
systems~\cite{walker:popl00,smith+:alias,walker+:capabilities,mandelbaum+:effective-refinements,ahmed+:hierarchical,tan+:tcs04}
and we will take advantage of this experience to meet our goals.

\paragraph*{Applications}
We plan to use our prototype systems both on applications
in networking and telecommunications from AT\&T and in genomics and
microbiology at Princeton.  The massive data sets (on the order of
100s of gigabytes) provided to us by AT\&T will be an
invaluable aid in stress testing the performance of our system.
At Princeton, we have already established
a collaboration with Olga Troyanskaya, who works in Princeton's 
Lewis-Sigler Institute 
for Integrative Genomics and Steven Kleinstein, head of
Princeton's Picasso project for interdisciplinary 
computational science.  We will test the flexibility
and expressiveness on the biological data sources
Troyanskaya and Kleinstein indicate are most important
for their research and the research of other biologists.
We plan to begin with formats used in Troyanskaya's Magic 
database~\cite{magic} and Kleinstein's immune-response system 
simulations.

\paragraph*{Formalization and Semantic Analysis.}
In order to understand the expressiveness and meta-theoretic
properties of our data description language
we plan to formally define its semantics.
Currently, as far as we are aware, no data description language
anything like \datatype{}
(including \pads~\cite{pads}, \packettypes~\cite{packettypes}, and \datascript~\cite{datascript}, and \blt~\cite{blt})
has been given a formal semantics.  
We propose to ameliorate this situation by
formalizing the core features of the current language
and analyzing the semantics of our extensions as we develop them.
%We believe the formalization will provide another dimension along which we can
%evaluate our tools, will help us find bugs in the implementation and 
%may suggest ways to generalize or streamline the \pads{} system.

We believe that the best way to model \datatype{} is as a dependent
type theory with dependent record types ($\Sigma x{:}\tau.\tau'$) to model
the dependent records we propose, set types 
($\{x{:}\tau \; | \; P(x) \}$) to model
semantic constraints, sum and recursive types to model
datatypes and type functions
($\lambda x{:}\tau.\tau'$ and $\lambda \alpha{::}\kappa.\tau'$) 
to model parameterized \pads{}\ types.  
In order to give a semantics to the \datatype{} description
compiler, we will explore using
a denotational semantics in which {\em types} are interpreted 
{\em as data processors}:

\[
\lsem \tau \rsem = f
\]

\noindent
where the data processor $f$ is a function that maps bitstreams into 
data structures in the lambda calculus. 

We will also formalize the dynamic and static semantics of the 
computational portion of \datatype.  We will investigate
its metatheoretic properties and prove well-typed programs 
``don't go wrong.''  In our case, this means that PDs always
accurately describe their corresponding data representations and
consequently programmers
will never fall into the trap of using bogus data that could not be 
properly parsed.


\subsection{Broader Impacts}
\label{ssec:impact}

If funded, this project will have three major broad impacts.

\paragraph*{Supporting Research in the Natural Sciences}
If this research is funded, part of our mission will be
to provide support for the data processing needs of biologists,
chemists and physicists at Princeton University first and 
the broader natural sciences community later.  As mentioned,
we have already begun to work with Olga Troyanskaya
and Steve Kleinstein to understand their needs.
As is the case with many computational biologists,
Troyanskaya analyzes genomics data that is provided to her in ad hoc
formats.  In the past, Troyanskaya and her students have spent
substantial blocks of time (weeks and months)
building parsers to collect and integrate
this data.  This wastes Troyanskaya's valuable time, which could otherwise be
used on the challenging and important algorithmic problems for which
she is a world expert.
If this research is
funded, we will be able to provide data descriptions and
a suite of useful tools that Troyanskaya
and her colleagues at Princeton's Genomics Institute and the wider
genomics community will be able to use to boost their productivity
substantially.  

% Troyanskaya has pointed us to several data
% repositories~\cite{grid,bind,geneontology} that she and her colleagues
% commonly use in their scientific investigations.  
% In fact, many of our proposed extensions to \pads{}\ are motivated directly by
% our investigation of these data sources.
% If this research is
% funded, we will be able to provide a suite of tools that Troyanskaya
% and her colleagues at Princeton's Genomics Institute and the wider
% genomics community will be able to use to boost their productivity
% substantially.  Part of our contribution will be a series of PADS
% descriptions for these formats and the analysis and querying tools we
% can generate automatically from .  A second important contribution
% will be a visual interface built on top of PADS that allows scientists
% to browse data represented in ad hoc data formats without having to
% know anything about programming or parsing.  All of our software will
% be freely available to academics via the Web.

\paragraph*{Undergraduate Research}
Princeton University has a very active undergraduate research program.
Princeton students usually do at least one semester of
Junior Independent Work and one semester of Senior Independent Work. 
Many students also do a year-long
Junior and Senior Theses.  We believe that PADS is an excellent
platform for undergraduate independent work and we plan to link
undergraduate independent work with our interdisciplinary effort to
produce tools for biologists.
More specifically, we will recruit undergraduates to help us
build our PADS visualization tool and to build
specific data processing tools for the biological data formats
used in Princeton's Genomics Institute.
This will introduce students both to interesting and novel
computer science research and also allow them to build useful
tools for biological applications. 

\paragraph*{Industrial and Governmental Impact}
Today's most important industries and governmental services
run on information.  As different companies, nonprofit organizations,
and governmental groups implement data repositories buy, sell
and exchange data, they are invariably frustrated by the time and expense
it takes to extract {\em information} from the low-level
formats in which their data is represented.  
When it comes to ad hoc data on a grand scale,
perhaps the most daunting challenge will come from the national
health-care system.  Recently, Newt Gingrich and Hilary Clinton have
jointly proposed we consolidate and integrate our national healthcare
records in the next 10 years.  Such a massive project will undoubtedly
require technical solutions at many levels.  At the lowest levels, we
will need a reliable software to collect and transfer health care data
from legacy systems into modern formats.  The \datatype{} system we
propose will be an invaluable aid in the development of this software.

Moreover, no matter how much time we sink into standards committees
and standardization of data formats, these efforts will always lag
behind.  There will always be data in legacy formats that must be
understood, updated and translated into a more modern form.  We
believe our research on ad hoc data management and transformation has
the potential for a tremendous long-term impact on data management
across all parts of our society.

\subsection{Comparison with Other Research}
\label{ssec:related}

Given the importance of ad hoc data, it is perhaps surprising that
more tools do not exist to solve it.  Many, many tools simply assumes
that the data they manipulate is in the right format from the
beginning.  If it is not, it is up to the user to get the data in the
correct format themselves --- he or she receives little or no help
with the problem. For instance, \xml{} and relational databases expect
their inputs are already in \xml{} or standard formats such as CSV
(Comma-Separated Values).  The development of \datatype{} is
completely complementary to research on relational and semi-structured
databases as \datatype will be explicitly designed to help with the
problem of loading one of these databases with data that is not
currently in the expected format.

One might wonder why we do not choose to use tools based on
regular expressions or context-free grammars like Lex and Yacc.
First, regular expressions and context-free grammars, while excellent
formalisms for describing programming language syntax, are not ideal
for describing the sort of ad hoc data we have discussed in this proposal.
The main reason for this is that regular expressions and context free grammars
do not support dependency and do not support deep semantic constraints
that are important for ensuring data integrity.
For instance, many ad hoc data sources we have studied include
 
Lex and Yacc are
both over- and under- kill.  Overkill because the division into a
lexer and a context free grammar is not necessary for many ad hoc data
sources, and under-kill in that such systems require the user to build
in-memory representations manually, support only ASCII sources, and
don't provide extra tools.  ASN.1~\cite{asn} and related
systems~\cite{asdl} allow the user to specify an in-memory
representation and generate an on-disk format, but this doesn't help
when given a particular on-disk format.  Existing ad hoc description
languages~\cite{gpce02,sigcomm00,erlang} are steps in the right
direction, but they focus on binary, error-free data and they do not
provide auxiliary tools.


There are many tools for describing data formats. For example,
\textsc{ASN.1}~\cite{asn} and \textsc{ASDL}~\cite{asdl} are both
systems for declaratively describing data and then generating
libraries for manipulating that data.  In contrast to \pads{},
however, both of these systems specify the {\em logical\/} representation
and automatically generate a {\em physical\/} representation.
Although useful for many purposes, this technology does not help
process data that arrives in predetermined, ad hoc formats.

Lex and yacc-based tools generate parsers from declarative
descriptions, but they require users to write both a lexer and a
grammar and to construct the in-memory representations by hand.  In
addition, they only work for ASCII data, they do not easily
accommodate data-dependent parsing, and they do not provide auxiliary
services.

More closely related work includes \erlang{}'s bit syntax~\cite{erlang} and
the \packettypes{}~\cite{sigcomm00} and
\datascript{} languages~\cite{gpce02}, 
all of which allow declarative descriptions of physical data.  These projects were motivated by parsing protocols,
\textsc{TCP/IP} packets, and \java{} jar-files, respectively.  Like
\pads{}, these languages have a type-directed approach to
describing ad hoc data and permit the user to define semantic constraints.
In contrast to our
work, these systems handle only binary data and assume the data is
error-free or halt parsing if an error is detected. 
Parsing non-binary data poses additional challenges because of the need
to handle delimiter values and to express richer termination conditions
on sequences of data. These systems also
focus exclusively on the parsing/printing problem, whereas we have 
leveraged the declarative nature of
our data descriptions to build additional useful tools.


Recently, a standardization effort has been started whose stated goals are quite similar to those of the \pads{} project~\cite{dfdl}. The description
language seems to be \xml{} based, but at the moment, more details are 
not available.

\subsection{Results from Prior NSF Support}
\label{ssec:results}

\paragraph*{Kathleen Fisher, Senior Personnel} 
Kathleen Fisher is a senior industry researcher at AT\&T.
She has not applied for previous general-purpose grants from NSF.
However, she is active proponent of increasing the
role of women and minorities in computing and has
obtained NSF funding to support increased involvement of women
in computer science as indicated in her biographical sketch
and detailed below.

(NSF 0243337, ACM Special Projects: 
Travel Grants for Faculty at Minority/Female Institutions to Attend
FCRC'03, Co-PI) This grant was committed to improving the representation of women and
minorities in computer science. To that end, we solicited applications
for travel grants from faculty members at undergraduate institutions
with large minority and/or female enrollments to attend FCRC '03, an
umbrella meeting with 16 constituent conferences and many associated
workshops and tutorials.  The organizers of the constituent meetings agreed to waive
the registration fees for all program participants. 
Descriptions of the many meetings that
comprised FCRC '03 are available from the FCRC '03 web site 
\url{http://www.acm.org/sigs/conferences/fcrc/}.  We
received 56 applications, and were able to award 49
fellowships.

This program exposed faculty members to state-of-the-art research in
computer science, to provide them with materials and training that
will help them improve their existing curricula and/or introduce new
curricula, and to establish contacts with other faculty at peer
institutions.  As part of the program, we organized an evening panel
discussion on the topic of Computer Science Research: Recruiting and
Retaining Women and Minorities.  Panelists included Jan Cuny
(University of Oregon), Leah Jamieson (Purdue University), William
Aspray (University of Indiana), and Ann Quiroz Gates (The University
of Texas at El Paso).  

Through evaluation forms and correspondence, the participants
indicated that they were very happy with their experiences at
FCRC '03. They said that they would attend a similar meeting again,
were funding made available. The participants also indicated that the
materials presented would have a strong positive influence on their
future teaching and research.  They indicated a strong desire for more
opportunities to network with the other fellows at the meeting.

%\input{walker-priors}

\paragraph*{David Walker, PI} (NSF CCR-0238328 CAREER: Programming Languages for Secure and Reliable Component Software
Systems, PI)
The goal of Walker's career award is to develop new programming language
technology that will improve the security and reliability of component software systems.
More specifically, Walker and his students have begun to develop new techniques for defining,
implementing and reasoning about run-time enforcement of program properties.
For example, he developed a rich new theory of security monitors as formal
automata that transform untrusted program behavior as they 
execute~\cite{ligatti+:edit-automata}.
This theory allows security
architects to model a variety of different sorts of run-time
enforcement mechanisms, to prove that certain mechanisms can or cannot
enforce various security properties, and to compare the power of
different classes of security monitors.    Recently, Walker 
has used the theory to prove the surprising new result that powerful run-time
program monitors can enforce certain kinds of liveness properties~\cite{ligatti+:renewal}.  
In addition, he has implemented the theory as an extension to Java and demonstrated
how to build compositional security monitors that can be applied to oblivious third-party 
software~\cite{bauer+:polymer}.

Walker's security monitoring language is a form of aspect-oriented programming language.
In order to better understand aspect-oriented technologies and their
potential impact on security, Walker formalized and proved
safe the {\em first} higher-order, strongly-typed calculus of 
aspects~\cite{walker+:aspects}.  This calculus defines
both static typing rules and the execution behavior of aspect-oriented
programs.  Consequently, it may
serve as a starting point for analysis of deeper properties of programs.
Recently, he has used the calculus to study the design of a
program analysis that determines the effect of security monitors on
the code they monitor~\cite{dantas+:harmless-advice}.   The analysis
can guarantee that a security monitor does not interfere with the 
original application, which greatly increases a user's incentive
to apply security patches.

To complement his work on run-time monitoring programs, Walker has also
developed several type systems to ensure basic type and memory safety conditions
for low-level programs.  Basic type- and memory-safety guarantees provide a foundation on which
richer security mechanisms can be implemented.  More specifically, he
has extended his earlier work on typed assembly language (TAL)~\cite{morrisett+:tal,morrisett+:journal-stal} with
logic-based type systems that can detect memory errors involving
stack-allocated data~\cite{ahmed+:stack} and heap or region-allocated
data~\cite{ahmed+:hierarchical-storage}.  In addition to studying memory safety
properties, Walker has shown how to use related type-theoretic techniques
to enforce adherence to very general software
protocols~\cite{mandelbaum+:refinements}.  

Finally, Walker's career grant allowed him to be a leader in
programming languages and security education. In 2004, he organized
a 10-day summer school on software security 
attended by over 70 participants~\cite{summerschool04}.  In 2005, he is in the process of
organizing a second summer school on high-assurance software and reliable computing.  He has also written a
chapter of a new textbook on type systems~\cite{walker:attapl}.



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{References Cited}
{\bibliographystyle{abbrv}
 \small\bibliography{pads}
} \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Biographical Sketch}
%\input{dpw-bio}

They are in separate files now (see cv/lastname-cv.tex).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Budget}

The budget pages (no longer necessary because of fastlane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Current and Pending Support (NSF Form 1239)}

The current-and-pending pages (no longer necessary because of fastlane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Facilities, Equipment and Other Resources (NSF Form 1363)}

The facility page (no longer necessary because of fastlane).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Special Information and Supplementary Documentation}

We may consider asking AT\&{}T to provide a support letter.

\end{document}


