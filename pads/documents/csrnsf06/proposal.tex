\documentclass[11pt]{article}
\usepackage{times,url}
\usepackage{code} 
%\usepackage{proof}
%\usepackage{newcode}
%\usepackage{epsfig}
\usepackage{psfig}

\input{defs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\voffset             0in    %  top vertical offset
\hoffset             0in    %  left horizontal offset
\oddsidemargin       0pt    %  Left margin on odd-numbered pages.
\evensidemargin      0pt    %  Left margin on even-numbered pages.
\topmargin           0pt    %  Nominal distance from top of page to top of
\headheight          0pt    %  Height of box containing running head.
\headsep             0pt    %  Space between running head and text.
\textwidth         6.5in    %  Width of text on page
\textheight          9in    %  Height of text on page
\setlength{\parskip}{.05in}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\textfraction}{0.1}
\renewcommand{\baselinestretch}{1.01}
%\input{macro}  %%%  other macro definitions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\setcounter{page}{1}
\pagenumbering{roman}
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{NSF 05-629 Project Summary:  \\
Language Support for Generation of Systems Monitoring Tools}
%%%%%%%
\input{summary}
\newpage
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Table of Contents}
% \tableofcontents\
% \vspace*{.1in} 
% \begin{center}
% {\Large\bf\em{}This is a place-holder. Fastlane generates this page automatically.}
% \end{center} 
% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{arabic}
\setcounter{page}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Description}

\subsection{Introduction}
\label{ssec:intro}

Complex systems must be {\em monitored} to proactively find problems,
record/archive system health, oversee system operation, detect
malicious processes or security violations and perform a myriad of other tasks.
The {\em monitoring subsystems} 
that perform these tasks take a wide range of forms, 
from embedded sensors that monitor physical processes to
intrusion detection systems that monitor flows to a single
organization to large-scale application/machine monitoring systems
that monitor the health and performance of Grid and distributed
systems spanning hundreds to thousands of nodes and tens to hundreds
of sites.  However, the construction and maintenance of monitoring systems
is not normally the top priority for systems implementers.  Even in the 
case of sensor
networks, where the primary function of the sensor is to monitor some
aspect of its environment, the data from the sensor is fed to a much
larger system solving an overall problem.

% Some form of monitoring is now commonly used across a wide range of
% systems, from embedded sensors that monitor physical processes to
% intrusion detection systems that monitor flows to a single
% organization to large-scale application/machine monitoring systems
% that monitor the health and performance of Grid and distributed
% systems spanning hundreds to thousands of nodes and tens to hundreds
% of sites. Monitoring can be used to find problems, record/archive
% system health, oversee system operation, and other tasks, but rarely
% is it the focus of an entire system. Even in the case of sensor
% networks, where the primary function of the sensor is to monitor some
% aspect of its environment, the data from the sensor is fed to a much
% larger system solving an overall problem.

Understandably, with monitoring at the periphery of most system, or
added after the initial system design, it suffers in many respects --
its code may not be as well-tested as the core code of the system, it
may not be regularly updated/maintained as necessary, the formats it
uses may not have been designed for longevity and evolvability, and
the data produced by the monitoring system may not have any associated
pre-existing tools for its manipulation. All of these problems can
lead to monitoring being less useful than it could otherwise be, and
in the case where the monitoring code itself is incorrect, it can be
the cause of security problems.

Part of the problem with building reliable, secure and extensible
monitoring systems is the diversity
of the data.  Since construction of monitoring subsystems is often
somewhat of an afterthought, the monitoring software usually
must deal with a plethora of preexisting data formats.  
Moreover, since monitoring is not initially a system architect's
top priority, these formats are often poorly documented and
rarely adhere to any sort of standard structure.  After
initial design of formats, some parts evolve and other parts decay
--- some data fields are added and others fall into disuse or
are hijacked for completely different purposes.  When formats are
optimized for space considerations, their structure and evolution
becomes even more complex.  
Even if the current system designer puts great thought 
into the monitoring subsystem, they often cannot avoid the fact that
some of the data come from poorly structured legacy systems and devices.


% Part of the problem with building monitoring systems is the diversity
% of the data. For any number of reasons, it may be impractical or
% unreasonable to use formats that have their own parsers and associated
% support tools. For example, some systems may be too small or have too
% limited bandwidth to natively use XML, while others may have such high
% data rates that conversion to/from other formats exceeds the
% processing capability of the system. Others may use more compact
% representations, or have so much archived data that conversion to
% other formats may exceed the storage space available to them.


% In order to do anything with such data, system implementers typically
% build ``one-off'' tools for parsing, printing, transforming and
% visualizing it.  Because of it's nonstandard form, there are no
% preexisting tools to perform these tasks.  Building these tools takes
% time and effort and as with any quick hack or ``one-off'' tool, they
% tend to be unreliable, insecure, hard to maintain and difficult to
% evolve.

One might suggest we should simply teach system implementers a standard
format for communicating systems information and produce a collection of 
reliable, reusable tools for processing that sort of data -- a sort of
XML for systems implementers.  While this might 
sound attractive in theory, it will never happen in practice.  We 
will always be constrained by 
the formats processed by pre-existing software tools and devices.
In addition, somebody, somewhere will
introduce that new data format that they simply intended for themselves, but
that escapes into the wild and becomes widely used.  There is as much
chance of moving to a single standard data format
as there is of moving to a single standard programming language.


% Even if system architects showed more foresight when it comes
% to the design of data formats, For any number of reasons, it may be impractical or
% unreasonable to use formats that have their own parsers and associated
% support tools. For example, some systems may be too small or have too
% limited bandwidth to natively use XML, while others may have such high
% data rates that conversion to/from other formats exceeds the
% processing capability of the system. Others may use more compact
% representations, or have so much archived data that conversion to
% other formats may exceed the storage space available to them.

We refer to the kind of data often seen in monitoring systems 
as having an {\em ad hoc data format},
by which we mean that it is a nonstandard data format for which
parsing, querying, analysis or transformation tools are not readily
available.  \xml{} is not an ad hoc data format --- there are hundreds
of tools for reading, querying, and transforming \xml{}.  However,
network administrators, data analysts, computer scientists,
biologists, chemists, and physicists deal with ad hoc data in a myriad
of complex formats on a daily basis.  This data, which is often
unpredictable, poorly documented, and filled with errors poses
tremendous challenges to its users and the software that manipulates
it.  

Our goal is to improve the monitoring process and alleviate the
burden, risk and confusion associated with ad hoc data by developing a
universal data processing system capable of

\begin{enumerate}

\item concisely and accurately describing any ad hoc data source at an
easy-to-understand, high-level of abstraction, and

\item automatically generating a suite of highly reliable and
efficient data processing tools that make understanding, manipulating
and transforming ad hoc data an effortless task.

\item using this support for ad hoc data processing to build
general-purpose monitoring tools that can be easily configured to
display, archive, and query ad hoc data

\item providing automatic and efficient distributed data gathering, so
that Grid and distributed systems monitoring can be provided in the
same framework

\end{enumerate}

\subsection{Introduction}
\label{ssec:intro}

An {\em ad hoc data format} is any nonstandard data format for which
parsing, querying, analysis or transformation tools are not readily
available.  \xml{} is not an ad hoc data format --- there are hundreds
of tools for reading, querying, and transforming \xml{}.  However,
network administrators,
data analysts, computer scientists,
biologists, chemists, and physicists deal with ad hoc
data in a myriad of complex formats on a daily basis.
This data, which is often unpredictable, poorly documented,
and filled with errors
poses tremendous challenges to its users and the software
that manipulates it.  
Our goal is to alleviate the burden, risk and confusion associated
with ad hoc data by developing a universal data processing system
capable of 

\begin{enumerate}
\item concisely and accurately describing any ad hoc data source at an 
easy-to-understand, high-level of abstraction, and
\item automatically generating a suite of highly reliable and
efficient data processing tools that make understanding, manipulating 
and transforming ad hoc data an effortless task.
\end{enumerate}

Together with his collaborators, the PI has already 
begun to develop a system called \pads{} (Processing
Ad hoc Data Sources)~\cite{padsproj,fisher+:700,launchpads} that 
helps to address some of these concerns.
It includes a high-level, declarative language for describing
ad hoc data formats and it is possible to generate tools for
parsing, printing, querying, generating histograms and 
statistical summaries of data, and transforming 
data into \xml.  An online demo~\cite{padsproj} illustrates
our progress to date.
Our preliminary work has demonstrated
there is much research to do in this area.  In particular, if funded we 
will pursue research on three fronts:

\begin{enumerate}
\item Extend the preliminary system with new features we have discovered are indispensible to
scientists who work with ad hoc data.  In particular, the preliminary system only operates
on single files.  We will extend it so it may operate over and integrate collections of files,
either local or distributed across a network.  We will also augment PADS so that
programmers may write simple, high level and easy-to-understand PADS transforms that can fix data errors
and filter, standardize, coerce, compress, or santize data fields.
\item  Improve automatic tool generation.  We will research new tool-generation architectures
that allow anyone to add new tools to the tool-set, which is currently is currently fixed.
We will also provide mechanisms that allow data descriptions to communicate useful semantic information
to downstream tools.
\item Develop the semantic theory of PADS.  We have a partial PADS parsing semantics based on dependent type
theory but we will extend this semantics to cover our new features and generalize it so it may
describe other components of the systems including printers and other tools.
\item In order to have a broad impact across research in the sciences,
we will develop descriptions and tools for researchers in
physics, biology and chemistry.
More specifically, we have already developed a partnership with
Olga Troyanskaya in Princeton's Lewis-Sigler Institute for 
Integrative Genomics.  Her Magic database system~\cite{magic}
reads several data sources
and applies a Baysian analysis in order to discover gene function.
Through the use of PADS, we will transform Magic into a truly generic
system by allowing Magic to import any data source -- to add new sources,
all a user will have to do is write a simple, high-level PADS description.
\item To extend and support our relationship with the Genomics Institute,
and to have a broad impact on interdisciplinary education,
we plan to develop undergraduate research projects in which
computer science majors use \pads{} to help biologists 
with their data processing problems.  
\end{enumerate}


Overall, our research combines novel language design, high-performance
systems engineering and theoretical analysis.  It will substantially
increase the overall productivity of data analysts, researchers and
software architects who deal with ad hoc data regularly, and it will
improve the security and reliability of the software they produce.
Finally, our research will also have a broad impact on research in the
natural sciences, where ad hoc data is pervasive, and on
interdisciplinary computer science.

In the rest of this introduction, we will explain the immense
challenges and risks posed by ad hoc data in more detail.  We will
explain how our proposed system as a whole can deal with them.  We
will then describe some of the features of \pads{} we have already
implemented.  After this introductory section, we will describe the
key components of our new research agenda and explain how our research
will make a broad impact on disciplines well outside the bounds of
traditional computer science and on interdisciplinary education.

\subsubsection{The Challenges of Ad Hoc Data}

The systems software that monitor ....

There are vast amounts of useful data stored in
traditional databases and \xml{} formats, but there is just as much in
ad hoc formats.  \figref{figure:data-sources} provides some information
on ad hoc data formats from several different domains ranging from genomics
to cosmology to networking to finance to internal corporate billing information.  
They include ASCII, binary, and Cobol data formats, with
both fixed and variable-width records, ranging in size from
relatively small files through network applications which process over
a gigabyte per second.  Common errors include undocumented data,
corrupted data, missing data, and multiple missing-value
representations.


\begin{figure*}
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
Name \& Use   &  Representation              &Size           & Common Errors \\ \hline\hline
Web server logs (CLF): &  Fixed-column  & $\leq$12GB/week & Race conditions \\ 
Measuring web workloads         &  ASCII records &                 & on log entry \\
                                &                &                 & Unexpected values\\ \hline
CoMon data:              &  Geographically & 600 MB/day & Race conditions on\\ 
Monitor \& troubleshoot  &  distributed    & collected from           & log entry \\
PlanetLab infrastructure &  ASCII records  & \appr{}400-450 machines           & \\\hline
AT\&T provisioning data (\dibbler{}): & Variable-width  & 2.2GB/week & Unexpected values \\ 
Monitoring service activation & ASCII records  &            & Corrupted data feeds \\ \hline
Phone call detail:   &  Fixed-width   &\appr{}7GB/day &  Undocumented data\\ 
Fraud detection & binary records & & \\  \hline 
AT\&T billing data (\ningaui{}): & Various Cobol  & \appr{}4000 files/day, & Unexpected values\\ 
Monitoring billing process   & data formats            & 250-300GB/day    & Corrupted data feeds \\ \hline
IP backbone data (\darkstar{})  & ASCII  & $\ge$ 15 sources  & Multiple missing-value \\
Monitoring network performance  &        & \appr{}15 GB/day              & representations  \\ 
                                &        &                               & Undocumented data \\\hline
Netflow       & Data-dependent \# of     & $\ge$1Gigabit/second  & Missed packets\\ 
Monitoring network performance              &  fixed-width    &                       & \\ 
               & binary records & & \\ \hline
\end{tabular}
\caption{Selected ad hoc data sources for system monitoring. }
\label{figure:data-sources}
\end{center}
\end{figure*}


% \begin{figure*}
% \begin{center}
% \begin{tabular}{@{}|l|l|l|l|l|}
% \hline
% Name: Use                           & Representation    & Processing Problems \\ \hline\hline
% Gene Ontology (GO)~\cite{geneontology}:                  & Variable-width    & White-space ambiguities \\
% Gene Product Information 	      & ASCII records &  \\ \hline
% SDSS/Reglens Data~\cite{mandelbaum+:reglens}:                & Floating point numbers, & Repeated multiplicative error \\
% Weak gravitational lensing analysis   & among others & \\ \hline
% Web server logs (CLF):                & Fixed-column      & Race conditions on log entry\\ 
% Measuring web workloads               & ASCII records     & Unexpected values\\ \hline
% AT\&T Call detail data:                          & Fixed-width       & Undocumented data\\
% Phone call fraud detection            & binary records  & \\ \hline 
% AT\&T billing data:                 & Cobol             &  Unexpected values\\ 
% Monitoring billing process          &                   & Corrupted data feeds \\ \hline
% Newick:   Immune                    & Fixed-width ASCII records & None \\ 
% system response simulation          & in tree-shaped hierarchy &\\ \hline                                
% OPRA:                               & Mixed binary \& ASCII records 
%                                                        & 100-page informal \\
% Options-market transactions         & with data-dependent unions & documentation \\ \hline
% Palm PDA:                           & Mixed binary \& character & No high-level  \\
% Device synchronization              & with data-dependent constraints & documentation available\\ \hline
% \end{tabular}
% \caption{Selected ad hoc data sources.}
% \label{figure:data-sources}
% \end{center}
% \end{figure*}


Processing ad hoc data is challenging for a variety of further reasons. 
First, ad hoc data typically arrives ``as is'': the analysts
who receive it can only say ``thank you,'' not request a more
convenient format.  Second, documentation for the format may not exist
at all, or it may be out of date.  A common phenomenon is for a field
in a data source to fall into disuse.  After a while, a new piece of
information becomes interesting, but compatibility issues prevent data
suppliers from modifying the shape of their data, so instead they
hijack the unused field, often failing to update the documentation in
the process.

Third, such data frequently contain errors, for a variety of reasons:
malfunctioning equipment, race conditions on log entry~\cite{wpp},
non-standard values to indicate ``no data available,'' human error in
entering data, unexpected data values, \etc{} The appropriate response
to such errors depends on the application.  Some applications require
the data to be error free: if an error is detected, processing needs
to stop immediately and a human must be alerted.  Other applications
can repair the data, while still others can simply discard erroneous
or unexpected values.  For some applications, errors in the data can
be the most interesting part because they can signal where two systems
are failing to communicate.

Fourth, online systems that process ad hoc data
are highly susceptible to attack from malicious outsiders.
For example, intrusion detection systems
and performance evaluation systems that monitor network activity 
may be sent erroneous packets or other data that cause buffer overflows
and allow attackers to take control of, evade, dismantle or corrupt these
systems.  A cautionary example of the dangers of online ad hoc data
processors is the Ethereal system~\cite{ethereal}. Ethereal is used by network administrators for monitoring, analyzing
and troubleshooting networks. Unfortunately, like most network software, users have found a number of
vulnerabilities in the software, and moreover many of these vulnerabilities are directly related to the mundane
components of the system that parse ad hoc data as opposed to the parts of the system that perform
higher-level tasks. For instance, in March 2004, Stefan Esser posted an advisory on 13 different buffer over-
flow attacks on Ethereal~\cite{etherealvulnerabilities}. Of the 13, 9 attacks occurred during parsing.

Fifth, ad hoc data sources can be high volume:
AT\&T's call-detail stream contains roughly 300~million calls per day
requiring approximately 7GBs of storage space. Although this data is
eventually archived in a database, analysts mine it profitably before
such archiving~\cite{kdd98,kdd99}. More challenging, the \ningaui{}
project at AT\&T accumulates billing data at a rate of 250-300GB/day,
with occasional spurts of 750GBs/day. Netflow data arrives from Cisco
routers at rates over a gigabyte per second~\cite{gigascope}! Such
volumes mean performance is critical and it certainly
must be possible to process the data without loading
it all into memory at once.

Finally, before anything can be done with an ad hoc data source,
someone has to produce a suitable parser for it.  Today, people tend
to use \C{} or \perl{} for this task.  Unfortunately, writing parsers
this way is tedious and error-prone, complicated by the lack of
documentation, convoluted encodings designed to save space, the need
to produce efficient code, and the need to handle errors robustly to
avoid corrupting down-stream data.  Moreover, the parser writers'
hard-won understanding of the data ends up embedded in parsing code,
making long-term maintenance difficult for the original writers and
sharing the knowledge with others nearly impossible.

\subsubsection{\pads{}:  Taking on the Challenge of Ad Hoc Data}

The \pads{} system makes life easier for data analysts by addressing
each of the concerns outlined in the previous section.
It provides a declarative data description
language that permits analysts to describe the physical layout of
their data, \textit{as it is}.  The language also permits analysts to
describe expected semantic properties of their data so that deviations can
be flagged as errors. The intent is to allow analysts to capture in a
\pads{} description all that they know about a given data source
and to provide the analysts with a library of useful routines in exchange. 


\pads{} descriptions are intended to be concise enough to serve as
documentation and flexible enough to describe many ASCII, binary,
Cobol, and mixed data formats.  In addition, useful software tools
can be generated from the descriptions and this feature provides
strong incentive for keeping the descriptions current, allowing them
to serve as living documentation.  

Given a \pads{} description, the \pads{} compiler produces
customizable \C{} libraries and tools for parsing, manipulating, and
summarizing the data.  The core \C{} library includes functions for
reading the data, writing it back out in its original form, writing it
into a canonical \xml{} form, pretty printing it in forms suitable for
loading into a relational database, and accumulating statistical
properties.  An auxiliary library provides an instance of the data API
for Galax~\cite{galax,galaxmanual}, an implementation of XQuery.  This
library allows users to query data with a \pads{} description as if
the data were in \xml{} without having to convert to \xml, which often
can avoid an 8-10 times blow-up in space requirements.  

The declarative nature of \pads{} descriptions facilitates the
insertion of error handling code.  The generated parsing code checks
all possible error cases: system errors related to the input file,
buffer, or socket; syntax errors related to deviations in the physical
format; and semantic errors in which the data violates user
constraints.  Because these checks appear only in generated code, they
do not clutter the high-level declarative description of the data
source.  Moreover, since tools are generated
automatically by a compiler rather than written by hand, 
they are far more likely to be robust
and far less likely to have dangerous vulnerabilities such as
buffer overflows.  Moreover, as formats change and are updated,
one can make a small change in the high-level description
and the compiler will takes care of the rest.  It is extremely unlikely
new vulnerabilities or buffer overflows will be added to the code
base over time as formats change.

The result of a parse is a pair consisting of a canonical
in-memory {\em representation} of the data and a {\em parse descriptor}. 
The parse
descriptor precisely characterizes both the syntactic and the semantic
errors that occurred during parsing.  This structure allows analysts
to choose how to respond to errors in application-specific ways.

% With such huge datasets, performance is critical. The \pads{} system
% addresses performance in a number of ways.  First, we compile the
% \pads{} description rather than simply interpret it to reduce run-time
% overhead.  Second, the generated parser provides multiple entry
% points, so the data consumer can choose the appropriate level of
% granularity for reading the data into memory to accommodate very large
% data sources.  Finally, we parameterize library functions by
% \textit{masks}, which allow data analysts to choose which semantic
% conditions to check at run-time, permitting them to specify all known
% properties in the source description without forcing all users of that
% description to pay the run-time cost of checking them.

% Given the importance of the problem, it is perhaps surprising that
% more tools do not exist to solve it.  \xml{} and relational databases
% only help with data already in well-behaved formats.  Lex and Yacc are
% both over- and under- kill.  Overkill because the division into a
% lexer and a context free grammar is not necessary for many ad hoc data
% sources, and under-kill in that such systems require the user to build
% in-memory representations manually, support only ASCII sources, and
% don't provide extra tools.  ASN.1~\cite{asn} and related
% systems~\cite{asdl} allow the user to specify an in-memory
% representation and generate an on-disk format, but this doesn't help
% when given a particular on-disk format.  Existing ad hoc description
% languages~\cite{gpce02,sigcomm00,erlang} are steps in the right
% direction, but they focus on binary, error-free data and they do not
% provide auxiliary tools.

% We propose to extend and improve the current \pads{}\ prototype by engaging
% in three intertwined research efforts:

% \begin{enumerate}
% \item Our experience with real-world data has revealed that
% the current \pads{}\ system is unable to process many common
% data formats.  We propose a number of extensions to \pads{} 
% that will greatly improve its expressive power and enable
% us to handle many more important ad hoc formats.
% \item While \pads{}\ can automatically generate tools for providing
% statistical data summaries, querying ad hoc data and generating
% \xml, the tool generation process is extremely brittle, inflexible
% and results in low-performance software.  We propose to redesign
% the automatic tool generation process using a principled methodology
% based upon a novel \pads{}-attribute system that can communicate
% semantic information across tools.  Our new methodology will
% improve the flexibility and reliability of the \pads{} system.
% \item There is no formal description of the \pads{} language
% and no semantics for the compiler or tools.  We propose to
% formalize the language and analyze the semantics of the
% compiler.  Our formal analysis may find bugs or flaws in the
% \pads{} system and will improve our confidence in the implementation.
% \end{enumerate}

% \noindent
% In addition, part of our mission will be to use \pads{} to provide
% data processing tools for researchers in the natural sciences including
% biology, chemistry and physics.  

% In building the \pads{} system, we faced challenges on two fronts:
% designing the data description language and
% crafting the generated libraries and tools.
% In the rest of the paper, 
% we describe each of these designs.
% We also give examples to show how useful they have been at AT\&T.


\subsubsection{The Current \pads{} Language Infrastructure}

% \figref{figure:data-sources} summarizes some of the sources we have
% worked with.  They include ASCII, binary, and Cobol data formats, with
% both fixed and variable-width records, ranging in size from
% relatively small files through network applications which process over
% a gigabyte per second.  Common errors include undocumented data,
% corrupted data, missing data, and multiple missing-value
% representations.


% \subsection{Common Log Format}
% Web servers use the Common Log Format (CLF) to log client
% requests~\cite{wpp}.  Researchers use such logs to measure
% properties of web workloads and to evaluate protocol changes
% by "replaying" the user activity recorded in the log.
% This ASCII format consists of a sequence of
% records, each of which has seven fields: the host name or IP address
% of the client making the request, the account associated with the
% request on the client side, the name the user provided for
% authentication, the time of the request, the actual request, the
% \textsc{http} response code, and the number of bytes returned as a
% result of the request.  The actual request has three parts: the
% request method (\eg, \texttt{GET}, \texttt{PUT}), the requested
% \textsc{uri}, and the protocol version.  In addition, the second and
% third fields are often recorded only as a '-' character to indicate
% the server did not record the actual data.  \figref{figure:clf-records}
% shows a couple of typical records.


To understand further how \pads{} can be used,
let us look at an example of ad hoc data:
a tiny fragment of data in the Common Log Format (CLF) that web
servers use to log client requests~\cite{wpp}.  
Systems researchers use such logs to measure
properties of web workloads and to evaluate protocol changes
by ``replaying'' the user activity recorded in the log.
This ASCII format consists of a sequence of
records, each of which has seven fields: the host name or IP address
of the client making the request, the account associated with the
request on the client side, the name the user provided for
authentication, the time of the request, the actual request, the
\textsc{http} response code, and the number of bytes returned as a
result of the request.  The actual request has three parts: the
request method (\eg, \texttt{GET}, \texttt{PUT}), the requested
\textsc{uri}, and the protocol version.  In addition, the second and
third fields are often recorded only as a '-' character to indicate
the server did not record the actual data.  \figref{figure:clf-records}
shows a couple of typical records.



\begin{figure*}
\begin{footnotesize}
%\begin{center}
\begin{verbatim}
207.136.97.49 - - [15/Oct/1997:18:46:51 -0700] "GET /tk/p.txt HTTP/1.0" 200 30
234.200.68.71 - - [15/Oct/1997:18:53:33 -0700] "GET /tr/img/gift.gif HTTP/1.0" 200 409
240.142.174.15 - - [15/Oct/1997:18:39:25 -0700] "GET /tr/img/wool.gif HTTP/1.0" 404 178
188.168.121.58 - - [16/Oct/1997:12:59:35 -0700] "GET / HTTP/1.0" 200 3082
tj62.aol.com - - [16/Oct/1997:14:32:22 -0700] "POST /spt/dd@grp.org/cfm HTTP/1.0" 200 941
214.201.210.19 ekf - [17/Oct/1997:10:08:23 -0700] "GET /img/new.gif HTTP/1.0" 304 -
\end{verbatim}
\caption{Tiny example of Common Log Format records. }
\label{figure:clf-records}
%\end{center}
\end{footnotesize}
\end{figure*}

With this example, we can examine how to use \pads{} to describe 
the {\em physical layout} and 
{\em semantic properties} of our ad hoc data source. 
The language provides a type-based model:
basic types describe atomic data such as integers, characters, 
strings, dates, urls, \etc, while
structured types describe compound data built from simpler pieces.

In a bit more detail,
the \pads{} library provides a collection of broadly useful base
types.  Examples include 8-bit unsigned integers (\cd{Puint8}), 32-bit
integers (\cd{Pint32}), dates (\cd{Pdate}), strings (\cd{Pstring}),
and IP addresses (\cd{Pip}).  Semantic conditions for such base types
include checking that the resulting number fits in the indicated
space, \ie, 16-bits for \cd{Pint16}.  By themselves, these base types
do not provide sufficient information to allow parsing because they do
not specify how the data is coded, \ie{}, in ASCII, EBCDIC, or binary.
To resolve this ambiguity, \pads{} uses the \textit{ambient} coding,
which the programmer can set.  By default, \pads{} uses ASCII.  
% To
% specify a particular coding, the description writer can select base
% types which indicate the coding to use.  Examples of such types
% include ASCII 32-bit integers (\cd{Pa_int32}), binary bytes
% (\cd{Pb_int8}), and EBCDIC characters (\cd{Pe_char}).  In addition to
% these types, users can define their own base types to specify more
% specialized forms of atomic data.

To describe more complex data, \pads{} provides a collection of
structured types loosely based on \C{}'s type structure.  In
particular, \pads{} has \kw{Pstruct}s, \kw{Punion}s, and \kw{Parray}s
to describe record-like structures, alternatives, and sequences,
respectively.  \kw{Penum}s describe a fixed collection of literals,
while \kw{Popt}s provide convenient syntax for optional data.  Each of
these types can have an associated predicate that indicates whether a
value calculated from the physical specification is indeed a legal
value for the type.  For example, a predicate might require that two
fields of a \kw{Pstruct} are related or that the elements of a
sequence are in increasing order.  Programmers can specify such
predicates using \pads{} expressions and functions, written using a
\C{}-like syntax.  Finally, \pads{} \kw{Ptypedef}s can be used to
define new types that add further constraints to existing types.

\pads{} types can be parameterized by values.  This mechanism serves
both to reduce the number of base types and to permit the format and
properties of later portions of the data to depend upon earlier
portions.  For example, the base type \cd{Puint_FW(:x:)} specifies
an unsigned integer physically represented by exactly \cd{x}
characters, where \cd{x} is a value that has been read earlier in the
parse.  The type \cd{Pstring(:SPACE:)} describes a string
terminated by a space (when \texttt{SPACE} is defined to be \texttt{' '}).  
Parameters can be used with compound types like arrays and unions to
specify the size of an array or which branch of a union should be
taken.  This parameterization is what makes PADS a {\em dependently-typed}
language and substantially different from languages based on
context-free grammars or regular expressions.

\figref{figure:clf} gives a \pads{} description for the Common Log Format
data.  
We will use this example to illustrate some of the basic
features of the current \pads{} language.  
In \pads{} descriptions, types are declared before they are used, 
so the type that describes the totality of the data source appears 
at the bottom of the description.  
In this case,
the type \texttt{clf\_t}  describes the entirety of the
CLF data source (the \texttt{Psource} type qualifier indicates
this fact explicitly).  

\kw{Pstruct}s describe fixed sequences of data with unrelated types.
In the CLF description, the type declaration for
\cd{version_t} illustrates a simple \kw{Pstruct}. It starts with a 
string literal that matches the constant \cd{HTTP/} in the data source.  It 
then has two unsigned integers recording the major and minor version numbers
separated by the literal character \kw{'.'}.  \pads{} supports character, string,
and regular expression literals, which are interpreted with the ambient character 
encoding. The type \cd{request_t} 
similarly describes the request portion of a CLF record.  In addition
to physical format information, this \kw{Pstruct} includes a semantic constraint
on the \cd{version} field.  Specifically, it requires that obsolete methods
\cd{LINK} and \cd{UNLINK} occur only under HTTP/1.1.  This constraint illustrates
the use of predicate functions and the fact 
that earlier fields are in scope during the processing of later fields, as the 
constraint
refers to both the \cd{meth} and \cd{version} fields in the \kw{Pstruct}.

\kw{Punion}s describe variation in the data format.  For example, the
\cd{client_t} type in the CLF description indicates that the first
field in a CLF record can be either an IP address or a hostname.
During parsing, the branches of a \kw{Punion} are tried in order; the
first branch that parses without error is taken.  The \cd{auth_id_t}
type illustrates the use of a constraint: the branch \cd{unauthorized}
is chosen only if the parsed character is a dash.  \pads{} also
supports a \textit{switched} union that uses a selection expression to
determine the branch to parse.  Typically, this expression depends
upon already-parsed portions of the data source.

\pads{} provides \kw{Parray}s to describe varying-length sequences of
data all with the same type.  The \cd{clf_t} declaration  uses a
\kw{Parray} to indicate that a CLF file is a sequence of \cd{entry\_t}
records.  This particular array terminates when the data source is
exhausted. In general, \pads{} provides a rich
collection of array-termination conditions: reaching a maximum size,
finding a terminating literal (including end-of-record), or satisfying a
user-supplied predicate over the already-parsed portion of the \kw{Parray}. 
\pads{} also has convenient syntax for 
specifying separators that appear between elements of an array and
declaring inter-element constraints including sorting.

The
\kw{Penum} type \cd{method_t} describes a collection of data literals.
During parsing, \pads{} interprets these constants using the ambient
character encoding.  The \kw{Ptypedef} \cd{response_t} describes
possible server response codes in CLF data by adding the constraint
that the three-digit integer must be between 100 and 600.

Finally, the \kw{Precord} annotations deserve comment. It
indicates that the annotated type constitutes a record.  
The notion of a record varies depending upon the data encoding.  
ASCII data typically uses new-line characters to delimit 
records, binary sources tend to have fixed-width records, while 
COBOL sources usually store the length of each record before the actual data.
\pads{} supports each of these encodings of records and allows users to define
their own encodings.  By default, \pads{} assumes records are new-line terminated.
Before parsing, however, the user can direct \pads{} to use a different record
definition.
\texttt{Precords} have error-recovery semantics -- if errors 
in the data cause the parser to become seriously confused,
it will attempt to recover to a record boundary.
In practice, we have found this to be a very robust recovery mechanism
for ad hoc data.  

Other examples of \pads{} descriptions and an online demo may be
found at \url{www.padsproj.org}.

\begin{figure}
\begin{small}
\begin{code}
\kw{Punion} client\_t \{
  Pip       ip;      /- 135.207.23.32
  Phostname host;    /- www.research.att.com
\};
\mbox{}
\kw{Punion} auth\_id\_t \{
  Pchar unauthorized : unauthorized == '-';
  Pstring(:' ':) id;
\};
\mbox{}
\kw{Pstruct} version\_t \{
  "HTTP/";
  Puint8 major; '.';
  Puint8 minor;
\};
\mbox{}
\kw{Penum} method\_t \{
    GET,    PUT,  POST,  HEAD,
    DELETE, LINK, UNLINK
\};
\mbox{}
bool chkVersion(version\_t v, method\_t m) \{
  \kw{if} ((v.major == 1) && (v.minor == 1)) \kw{return} true;
  \kw{if} ((m == LINK) || (m == UNLINK)) \kw{return} false;
  \kw{return} true;
\};
\mbox{}
\kw{Pstruct} request\_t \{
  '\\"';   method\_t       meth;
  ' ';    Pstring(:' ':) req\_uri;
  ' ';    version\_t      version :
                  chkVersion(version, meth);
  '\\"';
\};
\mbox{}
\kw{Ptypedef} Puint16\_FW(:3:) response\_t :
         response\_t x => \{ 100 <= x && x < 600\};
\mbox{}
\kw{Precord} \kw{Pstruct} entry\_t \{
         client\_t       client;
   ' ';  auth\_id\_t      remoteID;
   ' ';  auth\_id\_t      auth;
   " ["; Pdate(:']':)   date;
   "] "; request\_t      request;
   ' ';  response\_t     response;
   ' ';  Puint32        length;
\};
\mbox{}
\kw{Psource} \kw{Parray} clf\_t \{
  entry\_t [];
\}
\end{code}
\end{small}
\caption{\pads{} description for Web Log data.}
\label{figure:clf}
\end{figure}


% Returning to the CLF description in \figref{figure:wsl}, the \kw{Penum} type \cd{method_t} describes
% a collection of data literals.  During parsing, \pads{} interprets these
% constants using the ambient character encoding.  The \kw{Ptypedef} 
% \cd{response_t} describes possible server response codes in CLF data by adding
% the constraint that the three-digit integer must be between 100 and 600.

% \setlength{\floatsep}{0pt}
% \setlength{\dblfloatsep}{0pt}
% \setcounter{totalnumber}{4}
% \setcounter{dbltopnumber}{2}
% \begin{figure*}[t!]
% \begin{small}
% \begin{center}
% \begin{code}
% \input{ai.paper}
% \end{code}
% \vskip -2ex
% \caption{Tiny example of web server log data.}
% \label{figure:clf-records}
% \end{center}
% \end{small}
% \end{figure*}


% \begin{figure}
% \input{wsl}
% \caption{\pads{} description for web server log data.}
% \label{figure:wsl}
% \end{figure}

\subsection{Overview of Planned Research}
\label{ssec:sow}

Our central research agenda is divided into three broad subsections,
which we will discuss here; our broader impacts will be discussed in the next 
section.  First, our experience with real-world data has revealed that
the current \pads{}\ system is unable to process certain kinds of
data sources, several of which are quite prevalent.  We 
propose a number of extensions to \pads{} 
that will greatly improve its expressive power and enable
us to handle many more important ad hoc formats.  Second,
while \pads{}\ can automatically generate tools for providing
statistical data summaries, querying ad hoc data and generating
\xml, the tool generation process is brittle and inflexible.  We 
propose to redesign
the automatic tool generation process using a principled methodology
based upon type-directed programming paradigms and
a novel \pads{}-attribute system that can communicate
semantic information from descriptions to tools.  
Third, while we have already begun a
semantic analysis of \pads{} that has turned to be extremely useful in 
practice, we only analyzed a small portion of the PADS system.  
We propose to formalize further elements language, particularly
output processors (printers) and prove critical \pads{} correctness
properties including the coherence of parsing and printing.  
Overall, our research combines novel language
design, high-performance systems engineering and theoretical analysis,
all aimed at solving crucial data processing problems.

\subsection{Monitoring Framework}
\input{monitor}

\subsection{Towards a Universal Data Description System}
\input{newfeatures}

\subsubsection{Formalization and Semantic Analysis}
\input{semantics}

\subsection{Broader Impacts}
\label{ssec:impact}

If funded, this project will have two major broad impacts.

\paragraph*{Supporting Research across the Social and Natural Sciences}
While ad hoc data is prevalent in systems research and network monitoring
infrastructure, it also appears in many other disciplines
across the natural and social sciences. In finance and economics,
data analysts process information about stocks, bonds, options and derivatives,
in cosmology, telescopes and other tools output data about stars, planets and
galaxies, in microbiology and genomics, vast amounts of data concerning
genes and gene products is being accumulated using modern computational techniques.
Figure~\ref{figure:scientific-data-sources} lists a few of the scientific data sources
we have begun to investigate.

\begin{figure*}
\begin{center}
\begin{tabular}{@{}|l|l|l|l|l|}
\hline
Name: Use                           & Representation    & Processing Problems \\ \hline\hline
Gene Ontology (GO)~\cite{geneontology}:                  & Variable-width    & White-space ambiguities \\
Gene Product Information 	      & ASCII records &  \\ \hline
SDSS/Reglens Data~\cite{mandelbaum+:reglens}:                & Floating point numbers, & Repeated multiplicative error \\
Weak gravitational lensing analysis   & among others & \\ \hline
Newick:   Immune                    & Fixed-width ASCII records & None \\ 
system response simulation          & in tree-shaped hierarchy &\\ \hline                                
OPRA:                               & Mixed binary \& ASCII records 
                                                       & 100-page informal \\
Options-market transactions         & with data-dependent unions & documentation \\ \hline
\end{tabular}
\caption{Selected ad hoc scientific data sources.}
\label{figure:scientific-data-sources}
\end{center}
\end{figure*}


In order to have a broad impact across the sciences and social sciences,
we will develop descriptions and tools for researchers in diverse fields ranging from
physics to biology to chemistry and economics.
More specifically, we have already developed a partnership with
Olga Troyanskaya in Princeton's Lewis-Sigler Institute for 
Integrative Genomics.  Her Magic database system~\cite{magic}
reads several data sources
and applies a Baysian analysis in order to discover gene function.
The end goal of Magic is to provide critical information about how
our genes work that may be used in the treatment of widespread diseases,
including cancer.
In the past, Troyanskaya and her students have spent
substantial blocks of time building parsers to collect and integrate
this data.  This wastes her valuable time, which could otherwise be
used on the challenging and important algorithmic problems for which
she is a world expert.  It also limits the scope of Magic --- there are
only so many parsers one can program and maintain by hand.
Through the use of PADS, we will transform Magic into a truly generic
system by allowing Magic to import any data source.  To add new sources,
all a user will have to do is write a simple, high-level PADS description.
By adding data sources to the collection Magic
currently analyses, Magic will be able to make additional and possibly more
accurate predictions.  All of our software will
be freely available to academics via the Web.


% More generally, part of our mission will be
% to provide support for the data processing needs of biologists,
% chemists and physicists at Princeton University and 
% the broader natural sciences community.  
% Part of our contribution will be a series of PADS
% descriptions for these formats and the analysis and querying tools we
% can generate automatically from PADS.  A second important contribution
% will be a visual interface built on top of PADS that allows scientists
% to browse data represented in ad hoc data formats without having to
% know anything about programming or parsing.  

\paragraph*{Undergraduate Research}
Princeton University has a very active undergraduate research program.
Princeton students usually do at least one semester of
Junior Independent Work and one semester of Senior Independent Work. 
Many students also do a year-long
Junior and Senior Theses.  We believe that PADS is an excellent
platform for undergraduate independent work and we plan to link
undergraduate independent work with our interdisciplinary effort to
produce tools for biologists.  We have already recruited Mark Daly,
a Princeton Senior who is doing his undergraduate senior thesis
on a PADS user interface and automatic format inference.
In the future, we will recruit other undergraduates to help us build
specific data processing tools for the biological data formats
used in Princeton's Genomics Institute.
This will introduce students both to interesting and novel
computer science research and also allow them to build useful
tools for biological applications. The PI has a proven track record
when it comes to advancing graduate and undergraduate education
as he has organized two summer schools (2004, 2005) on technology for
secure and reliable programming and mentored several undergraduates,
the latest of which, Rob Simmons, won the Princeton Computer Science Department
Senior Thesis Award.

\subsection{Comparison with Other Research}
\label{ssec:related}

The oldest tools for describing data formats are parser generators such as
Lex and Yacc.  While excellent for parsing programming languages, Lex and Yacc
are too heavyweight for parsing the simpler ad hoc data formats one
runs into in the sciences.   
Unlike PADS, whose syntax is based on types from the well-known C language,
the syntax of Lex and Yacc is somewhat foreign.  Perhaps more importantly,
users must write a lexer, write a
grammar, and construct the in-memory representations by hand.  In
addition, they only work for ASCII data, they do not easily
accommodate data-dependent parsing, and they do not provide auxiliary
services such as automatic XML conversion, stastical analysis and
others.  Some more modern parser generators such as ANTLR~\cite{antlr} alleviate
a few of these problems, but they still do not automatically generate auxiliary tools
useful in processing ad hoc data nor do they provide good support for generating
rich, well-typed in-memory representations (ANTLR's in-memory representations
are very limited when compared with PADS and the extensions we propose).

The closest related work includes domain-specific
languages such as PacketTypes~\cite{sigcomm00} and DataScript~\cite{gpce02} 
for parsing and printing binary data, particularly packets
from common networking protocols such as \textsc{TCP/IP} and also
\java{} jar-files.  Like \pads{}, these languages have a type-directed
approach to describing ad hoc data and permit the user to define
semantic constraints.  In contrast to our work, these systems handle
only binary data and assume the data is error-free or halt parsing if
an error is detected.  Not only are ASCII formats a common part of
many software monitoring systems, parsing non-binary data poses additional
challenges because of the need to handle delimiter values and to
express richer termination conditions on sequences of data. 
PacketTypes and DataScript also focus exclusively on the 
parsing/printing problem,
whereas our research will exploit the declarative nature of our data
descriptions to automatically generate additional useful tools and
programming libraries.  \pads{} substantial 
collection of value-added tools and libraries is 
one of the key incentives that will make it
worth a programmer's while to learn \pads{}
and develop and maintain data descriptions for their applications.

The Binary Format Description language (BFD)~\cite{bfd} is a fragment of
XML that allows programmers to specify binary and ASCII formats.  BFD
is able to convert the raw data into XML-tagged data where it can then be
processed using XML-processing tools.  While this is useful for many
tasks, conversion to XML can be prohibitively expensive:  such conversion
often results in an 8-10 times blowup in data size over the native form.
\pads{}, on the other hand, avoids this blowup by processing data in its 
native form.

Currently, the Global Grid Forum is working on a standard
data-format description language for describing ad hoc data formats,
called DFDL~\cite{dfdl-proposal,dfdl-primer}.  Like \pads{},
DFDL{} has a rich collection of base types and supports a variety of
ambient codings.  Unlike \pads{}, DFDL{} does not support semantic
constraints on types nor dependent types, \eg{}, it is not possible to
specify that the length of an array is determined by some previously parsed field in the
data.  Our practical experience indicates that many ad hoc formats,
particularly binary formats, absolutely require dependent types in their
specifications.  DFDL{} is an annotated subset of XML{} Schema, which means
that the XML{} view of the ad hoc data is implicit in a DFDL{}
description.  DFDL{} is still being specified, so no DFDL-aware
parsers or data analyzers exist yet.  

There are parallels between PADS types and some of the elements of parser
combinator libraries found in languages like
Haskell~\cite{burge:parser-combinators,hutton+:parser-combinators}. 
However, as with most other general-purpose parsing tools, one cannot
simply put together a collection of Haskell's parser combinators and
automatically generate domain-specific programs such as 
an XML converter or a histogram generator, for instance.  

A somewhat different class of languages includes
\textsc{ASN.1}~\cite{asn} and \textsc{ASDL}~\cite{asdl}.  Both of
these systems specify the {\em logical\/} in-memory representation of
data and then automatically generate a {\em physical\/} on-disk
representation.  Although useful for many purposes, this technology
does not help process data that arrives in predetermined, ad hoc
formats.  Another language in this category is the Hierarchical Data
Format 5 (HDF5)~\cite{hdf5}.  This file format allows users to store
scientific data, but it does not help users deal with legacy ad hoc
formats like PADS does.

% There are probably hundreds of tools that one might use if their data were
% in \xml.  However, the point of PADS is to allow scientists whose data is {\em not}
% already in \xml to get work done, particularly when that data contains errors,
% as ad hoc data often does.  Since many processes, machines, programs and other devices
% currently output data and a whole most of

XSugar~\cite{brabrand+:xsugar2005} allows user to specify an
alternative non-XML syntax for XML languages using a context-free
grammar.  This tool automatically generates conversion between XML and
non-XML syntax. It also guarantees that conversion will be invertable.
However, it does not use theory of Galois connections, but instead
introduces a notion of ``ignorable'' grammar components (inferred
based on properties of grammar) and proves bijection modulo these
ignorable elements.  More importantly, since the basis of
interconversion is a context free grammar, unlike PADS, formats that
required dependency may not be expressed.

XDTM~\cite{zhao+:sigmod05,xdtm} uses XML Schema to describe the
locations of a collection of sources spread across a local file system
or distributed across a network of computers.  However, XDTM has no
means of specifying the contents of files, so XDTM and PADS solve
complementary problems.  Nevertheless, the XDTM design may provide
ideas to us as we extend PADS from a single-source system to a
multi-source system. The METS schema~\cite{mets} is similar to XDTM as
it describes metadata for objects in a digital library, including a
hierarchy such objects.

Commercial database products provide support for
parsing data in external formats so the data can be imported into
their database systems, but they typically support a limited number of
formats, \eg{}, COBOL copybooks.  Also, no declarative description of the
original format is exposed to the user for their own use, and they
have fixed methods for coping with erroneous data.  For these reasons,
PADS is complementary to database systems.  We strongly believe that
in the future, commercial database systems could and should support a 
PADS-like description language that allows users to import information from
almost any format.  We hope that our research on PADS will make a broad
impact in this area.

On the theoretical front, the scientific community's understanding of
type-based languages for data description is much less mature.  To the
best of our knowledge, our work on the DDC is the first to provide a
formal interpretation of dependent types as parsers and to study the
properties of these parsers including error correctness and type
safety.  Regular expressions and context-free grammars, the basis for
Lex and Yacc have been well-studied, but they do not have dependency,
a key feature necessary for expressing constraints and parsing ad hoc
scientific data.  {\em Parsing Expression Grammars} (PEGs), studied in
the early seventies~\cite{birman+:parsing}, revitalized more recently
by Ford~\cite{ford:pegs} and implemented using ``packrat parsing''
techniques~\cite{ford:packrat,grimm:packrat}, are somewhat more
similar to PADS recursive descent parsers. However, PADS does not use
packrat parsing techniques as the space overhead is too high for large
data sets.  Moreover, our multiple interpretations of types
in the DDC makes our theory substantially different from the theory of
PEGs.

\subsection{Results from Prior NSF Support}
\label{ssec:results}

\paragraph*{Kathleen Fisher, Senior Personnel} 
Kathleen Fisher is a senior researcher at AT\&T Labs,
where she has spent the past nine years working on projects
related to managing massive amounts of ad hoc network monitoring data.
In the Hancock project~\cite{kdd00,hancock-toplas}, she helped 
design and implement a C-based
domain-specific programming language for processing massive  
transaction streams.  Hancock programs make it easy to build
and maintain profiles of the entities described in such streams. 
AT\&T uses these profiles monitor networks for fraud 
and to better understand user characteristics.
As the primary architect of the PADS project~\cite{fisher+:pads}, 
Fisher has helped design and implement the prototype PADS
data description language that will form a foundation for the work
described in this proposal.  From PADS descriptions,
a compiler currently produces a parser and a collection of tools for
manipulating the associated data.  

Fisher is an active proponent of increasing the role of women and
minorities in computing and has obtained NSF funding to support
increased involvement of women in computer science (NSF 0243337, ACM
Special Projects: Travel Grants for Faculty at Minority/Female
Institutions to Attend FCRC'03, Co-PI).  This grant was committed to
improving the representation of women and minorities in computer
science. To that end, Fisher and her collaborators 
solicited applications for travel grants from
faculty members at undergraduate institutions with large minority
and/or female enrollments to attend FCRC '03, an umbrella meeting with
16 constituent conferences and many associated workshops and
tutorials.  The organizers of the constituent meetings agreed to waive
the registration fees for all program participants.  Descriptions of
the many meetings that comprised FCRC '03 are available from the FCRC
'03 web site \url{http://www.acm.org/sigs/conferences/fcrc/}.  Fisher
received 56 applications, and was able to award 49 fellowships.

\paragraph*{Vivek Pai, Co-PI} Vivek Pai was partially funded by 
an NSF CAREER award, CCR-0093351, Automatic Retargeting of Network
Servers. He is currently Co-PI on CNS-0520053, An Evolvable
Architecture for Next-Generation Internet Services, and PI on
CNS-0519829 Bridging the 10 GHz / 10 Gbit Gap: Whole-system approaches
for scalable networked services.

His CAREER award led to the enhancement of the Flash Web Server to
support SpecWeb99, an industry-standard benchmark not commonly used in
the academic community due to its complexity. The main result of this
work is an improved research server that is at or near the top of the
results for this benchmark for the classes of hardware tested. In the
process, his group developed new kernel performance debugging tools
that identified numerous performance problems in the FreeBSD operating
system. The modifications they developed to address these problems
have been integrated into FreeBSD and have been shipping for over a
year.  The work also lead to a better understanding of server
performance in SMT (simultaneous multithreading) processors, and why
these processors are not seeing the performance gains in practice that
would have been expected from their simulations.

The two more recent awards have just started, but are already starting
to bear fruit. One recent service developed from this, CoBlitz, is a
scalable large-file transfer service that runs over HTTP. CoBlitz is
now being used by the CiteSeer Scientific Literature Digital Library
to globally deliver electronic copies of research publications. It is
also being used by the Fedora Core Linux distribution in delivering
CD-ISO and DVD-ISO images of their OS distribution.

\paragraph*{David Walker, PI} (NSF CCR-0238328 CAREER: Programming Languages for Secure and Reliable Component Software
Systems, PI)
The goal of Walker's career award is to develop new programming language
technology that will improve the security and reliability of 
component software systems.  More specifically, Walker and his students have 
a new theory of security monitors as formal
automata that transform untrusted applications as they 
execute~\cite{ligatti+:edit-automata}.
This theory allows security
architects to model a variety of different sorts of run-time
enforcement mechanisms, to prove that certain mechanisms can or cannot
enforce various security properties, and to compare the power of
different classes of security monitors~\cite{ligatti+:renewal}.
The theory forms the foundation of an expressive and powerful
program monitoring system for Java~\cite{bauer+:polymer}.

Walker's security monitoring language is a form of 
aspect-oriented programming language.
In order to better understand aspect-oriented technologies and their
potential impact on security, Walker formalized and proved
safe the {\em first} typed, functional and 
aspect-oriented programming language~\cite{walker+:aspects}.
The language has been implemented and
extended with facilities for polymorphic
and type-directed programming~\cite{dantas+:polyaml}.  
Recently, Walker has studied program analysis techniques
that can precisely determine the effect security monitors have on
the code they monitor~\cite{dantas+:harmless-advice,dantas+:harmless-popl}.   
The analysis
can guarantee that a security monitor does not interfere with the 
original application, which greatly increases a user's incentive
to apply security patches.

% To complement his work on run-time monitoring programs, Walker has also
% developed several type systems to ensure basic type and memory safety conditions
% for low-level programs.  Basic type- and memory-safety guarantees provide a foundation on which
% richer security mechanisms can be implemented.  More specifically, he
% has extended his earlier work on typed assembly language (TAL)~\cite{morrisett+:tal,morrisett+:journal-stal} with
% logic-based type systems that can detect memory errors involving
% stack-allocated data~\cite{ahmed+:stack,jia+:stack} and heap or region-allocated
% data~\cite{ahmed+:hierarchical-storage}.  In addition to studying memory safety
% properties, Walker has shown how to use related type-theoretic and logical techniques
% to verify programs~\cite{jia+:ilc} and enforce general software
% protocols~\cite{mandelbaum+:refinements}.  

Walker's career grant also allowed him to be a leader in
programming languages and security education. In 2004 and 2005 he
organized a 10-day summer school on software security and reliable
computing, attended by over 100 participants
combined~\cite{summerschool04,summerschool05}.  Also
in 2005, his undergraduate research advisee, Rob Simmons, won the
Princeton Computer Science Department Senior Thesis Award.
He has recently written a
chapter of a new textbook on typed programming 
languages~\cite{walker:attapl}.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{References Cited}

{\bibliographystyle{abbrv}
\bibliography{pads-long,pads,galax,padsdave}
}
%{\bibliographystyle{abbrv}
% \small\bibliography{pads}
%} 

\end{document}


