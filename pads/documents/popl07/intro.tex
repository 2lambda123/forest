\section{Introduction}
\label{sec:intro}
\cut{
{\em
To do:
\begin{itemize}
\item update text to include description of tools generated using the framework.
\item add size column to data source table.
\end{itemize}
}}
%% WHAT IS AD HOC DATA?

An {\em ad hoc} data format is any non-standard data format for which
parsing, querying, analysis, or transformation tools are not readily
available.  Despite the existence of standard
data formats like \xml{}, ad hoc data sources are ubiquitous,
arising in industries as diverse as finance, health care,
transportation, and telecommunications as well as in scientific
domains, such as computational biology and physics.
\figref{figure:data-sources} summarizes a variety of such formats,
including ASCII, binary, and Cobol encodings, with both fixed and
variable-width records arranged in linear sequences and in tree-shaped
hierarchies.  Even a single format can exhibit a great deal of
syntactic variability.  For example, \figref{fig:darkstar-records1}
contains two records from a network-monitoring application.  Note that
each record has a different number of fields (delimited by '$|$') and
that individual fields contain structured values (\eg{},
attribute-value pairs separated by '=' and delimited by ';').

Common characteristics of ad hoc data make it difficult to perform
even basic data-processing tasks.  To start, data analysts typically
have little control over the format of the data they are given.  The
data arrives ``as is,'' and the analysts can only thank the suppliers,
not request a more convenient format.  The documentation accompanying
ad hoc data is often incomplete, inaccurate, or missing entirely,
which makes understanding the data format more difficult.
Understanding the format in some detail is a prerequisite to writing
even a basic parser, which is needed to convert ad hoc data into other
formats, for example, to load data into a database.

Another challenge is the occurrence of errors in ad hoc data.  Common
errors, listed in the third column of \figref{figure:data-sources},
include undocumented fields, corrupted and missing data, and multiple
representations for missing values.  Sources of errors include
malfunctioning equipment, race conditions on log entry~\cite{wpp}, the
presence of non-standard values to indicate ``no data available,'' and
human error when entering data.  A wide range of responses are
possible when errors are detected, from halting processing and
alerting a human operator, to partitioning erroneous from valid
records for examination off-line, to repairing erroneous or unexpected
values.  Erroneous data itself is often more important than error-free
data, because it may indicate, for example, that two systems are
failing to communicate.  However, writing code that reliably handles
both error-free and erroneous data is difficult and tedious.

\begin{figure*}
\begin{center}
\scriptsize
\begin{tabular}{@{}|l|l|l|l|l|}
\hline
Name: Use                           & Representation    & Processing Problems \\ \hline\hline
Gene Ontology (GO)~\cite{geneontology}:                  & Variable-width    & White-space ambiguities \\
Gene Product Information 	      & ASCII records &  \\ \hline
SDSS/Reglens Data~\cite{mandelbaum+:reglens}:                & Floating point numbers, & Repeated multiplicative error \\
Weak gravitational lensing analysis   & among others & \\ \hline
Web server logs (CLF):                & Fixed-column      & Race conditions on log entry\\ 
Measuring web workloads               & ASCII records     & Unexpected values\\ \hline
AT\&T Call detail data:                          & Fixed-width       & Undocumented data\\
Phone call fraud detection            & binary records  & \\ \hline 
AT\&T billing data:                 & Cobol             &  Unexpected values\\ 
Monitoring billing process          &                   & Corrupted data feeds \\ \hline
Newick:   Immune                    & Fixed-width ASCII records & None \\ 
system response simulation          & in tree-shaped hierarchy &\\ \hline                                
OPRA:                               & Mixed binary \& ASCII records 
                                                       & 100-page informal \\
Options-market transactions         & with data-dependent unions & documentation \\ \hline
Palm PDA:                           & Mixed binary \& character & No high-level  \\
Device synchronization              & with data-dependent constraints & documentation available\\ \hline
\end{tabular}
\caption{Selected ad hoc data sources.}
\label{figure:data-sources}
\end{center}
\end{figure*}

% \begin{figure*}
% \begin{center}
% \begin{tabular}{@{}|l|l|l|}
% \hline
% \textbf{Name:} Use & Record Format (Size) 
% %& Size
%            & Common Errors \\ \hline\hline
% \textbf{Web server logs (CLF):}           & Fixed-column ASCII & Race conditions on log entry\\ 
% Measuring Web workloads  & ($\leq$12GB/week)  & Unexpected values\\ \hline
% \textbf{AT\&T provisioning data (\dibbler{}):} & Variable-width ASCII & Unexpected values \\ 
% Monitoring service activation  & (2.2GB/week) & Corrupted data feeds \\ \hline
% \textbf{Call detail:}                   & Fixed-width binary &  Undocumented data\\
% Fraud detection                         &   (\appr{}7GB/day) & \\ \hline 
% \textbf{AT\&T billing data (\ningaui{}):}      & Cobol      & Unexpected values\\ 
% Monitoring billing process  &  ($>$250GB/day) & Corrupted data feeds \\ \hline
% \textbf{IP backbone data (\darkstar{}):}  & ASCII & Multiple representations \\
% {Network Monitoring}       &  ($\ge$ 15 sources,\appr{}15 GB/day)  & of missing values \\
%           & & Undocumented data \\ \hline
% \textbf{Netflow:}               & Data-dependent number of & Missed packets\\ 
% {Network Monitoring}  & fixed-width binary records & \\ 
%                       & ($\ge$1Gigabit/second) & \\ \hline
% \textbf{Gene Ontology data:}    & Variable-width  & \\
% Gene product information & ASCII records & White-space ambiguities\\\hline
% \textbf{Newick data}              & Fixed-width ASCII & Manual entry errors \\
% Immune system response simulation & in tree-shaped hierarchy 
% & \\
% \hline
% \end{tabular}
% \normalsize
% \caption{Selected ad hoc data sources.}
% \label{figure:data-sources}
% \end{center}
% \end{figure*}

\begin{figure*}
  \centering
  \small
\begin{verbatim}
 2:3004092508||5001|dns1=abc.com;dns2=xyz.com|c=slow link;w=lost packets|INTERNATIONAL
 3:|3004097201|5074|dns1=bob.com;dns2=alice.com|src_addr=192.168.0.10;
 dst_addr=192.168.23.10;start_time=1234567890;end_time=1234568000;cycle_time=17412|SPECIAL
\end{verbatim}  
  \caption{Simplified network-monitoring data. We inserted the newline
    after the ';' to improve legibility.}
  \label{fig:darkstar-records1}
\end{figure*}

\cut{
Surprisingly,
few meta-language tools, such as data-description languages or parser
generators, exist to assist in management of ad hoc data.  And
although ad hoc data sources are among the richest for database and
data mining researchers, they often ignore such sources as the work
necessary to clean and vet the data is prohibitively expensive.}

%% %% High-volume
%% The high volume of ad hoc data sources is another challenge. AT\&T's
%% call-detail stream, for example, contains roughly 300~million calls
%% per day requiring approximately 7GBs of storage space.  Although this
%% data is eventually archived in a database, data analysts mine it
%% profitably before such archiving~\cite{kdd98,kdd99}.  More
%% challenging, the \ningaui{} project at AT\&T accumulates billing data
%% at a rate of 250-300GB/day, with occasional spurts of 750GBs/day, and
%% netflow data arrives from Cisco routers at rates over a Gigabit per
%% second~\cite{gigascope}!  Such volumes require that the data be
%% processed without loading it into memory all at once.  Not
%% surprisingly, flexible error-response strategies are especially
%% critical with high-volume sources, so that error detection does not
%% halt or delay normal processing.

%% %% EXISTING SOLUTIONS

%% To manage ad hoc data, analysts typically write custom programs in
%% \C{} or \perl{} to parse and manipulate the data. 
%% Unfortunately, writing such parsers is tedious and error-prone,
%% complicated by the lack of documentation, convoluted encodings
%% designed to save space, and the need to produce efficient code.
%% Handling errors is a no-win situation.  If analysts insert code
%% to detect all possible errors, then the error-related code dominates
%% the rest of the program. If they don't, they run the risk of
%% failing to detect a critical error and possibly corrupting valuable
%% down-stream data.  Moreover, the parser writers'
%% hard-won understanding of the data ends up embedded in parsing code,
%% making long-term maintenance difficult for the original writers and
%% sharing the knowledge with others nearly impossible.

%% \cut{
%% The range of application domains, 
%% the variability and irregularity of ad hoc formats, 
%% the lack of documentation,
%% the prevalence of errors, 
%% the volume of data,
%% and the lack of tools
%% make processing ad hoc data both interesting and challenging.
%% }

%% OUR SOLUTION & CONTRIBUTIONS
\subsection{\padsmlbig{}}

%% Slight change to emphasize generation of many tools.

\padsml{} is a domain specific language and compiler system designed to 
improve the productivity of computational biologists, physicists,
network administrators, healthcare providers
and anyone else who works with ad hoc data.
In order to use the system, one must
write a specification of their data in the \padsml{} language.
This specification will describe the physical format of the data in question
and may also include semantic constraints the user expects to hold.
For the modest effort the user puts into writing this specification,
and they gained a substantial return.  First of all, the description
serves as clear, compact and formally specified documentation of 
the data's structure and properties.  In addition, the \padsml{}
compiler will produce a large suite of robust, end-to-end
data processing tools as well as programming libraries specialized
to the format.  If the user's data sources evolve over time,
he or she may make simple changes to the high-level specification
and recompile to produce new, reliable, up-to-date tools.

%% Our solution to the challenges of ad hoc data is \padsml{}, a
%% declarative language for describing data sources.  With \padsml{}, a
%% data analyst can describe both the physical format and semantic
%% properties of a data source.  From a \padsml{} description, our
%% compiler generates robust data processing tools specific to the
%% description.  Because valuable tools are generated from a description,
%% an analyst is motivated to keep the description up-to-date in response
%% to format changes.  Hence a \padsml{} description serves as accurate,
%% formal, and living documentation of a data format.

\cut{\textbf{***First describe features of \padsml{} types and the formal semantics.}
\textbf{Lift text on types from Sec 2.}}

The design of the 
\padsml{} language is inspired by the type structure of modern
functional programming languages.  More specifically, 
\padsml{} has a rich collection of base
types and provides polymorphic, dependent, and recursive datatypes to
specify the syntactic structure and semantic properties of a data
format.  Together, these features enable users to write concise,
complete, and reusable descriptions of formats from domains like
networking, computational biology, finance, and cosmology.  The
\padsml{} language is described through examples from several domains
in Section~\ref{sec:padsml-overview}.

\cut{\textbf{***Justify embedding in \ocaml{}/\ml{}}} 

We have implemented \padsml{} by compiling descriptions into
\ocaml{}.  In order to do so, we use a
``types as modules'' implementation strategy, in which \padsml{} types
become modules and \padsml{} type constructors become functors. \cut{This
design exploits the strengths of the \ml{} module system to provide
the user with an intuitive and convenient interface to
\padsml{}-generated libraries.}
We were motivated to implement \padsml{} in an \ml{} language, not
only to provide \ml{} programmers with a data-description language,
but also  because we believe the features of \ml{} languages, like pattern
matching and higher-order functions, can more naturally express data
processing and transformation tasks than imperative languages like
\C{} and \java{}.
Section~\ref{sec:padsml-impl} describes our ``types as modules''
strategy and shows how \padsml{}-generated modules together
with functional \ocaml{} code can concisely express
common
data-processing tasks like filtering errors and format transformation.

% Like the parser, the types of the representation and parse descriptor
% produced by the parser are derived automatically from the data
% description. The constructs of \padsml{} are based on the type
% structure of the ML family of languages. Therefore, the compiler can
% map the description into type declarations in ML with little change.

\cut{\textbf{***Then describe the parser.}}
\cut{\textbf{***Then other tools and tool framework}}

A key benefit of our approach is the high return-on-investment that
users derive from describing their data in \padsml{}.  To increase
this return, our goal is to generate automatically a collection of
data analysis and processing tools from each description.  The two core
programming libraries that \padsml{} generates from a description 
are a parser and printer for the data source.  The parser maps raw 
data into two data
structures: a canonical \textit{representation} of the parsed data and
a \textit{parse descriptor}, a meta-data object detailing properties
of the corresponding data representation.  Parse descriptors provide
applications with programmatic access to errors detected during
parsing.  The printer inverts the process, mapping internal data structures
and their corresponding parse descriptors back into into raw data.

In addition to generating robust parsers and printer, our framework permits
developers to easily add their own {\em format-independent} 
tools, based on their expertise,
to our tool suite without modifying the \padsml{} compiler.  A new
tool generator need only match a generic interface, specified as an
\ml{} signature.  Correspondingly, for each \padsml{} description, the
\padsml{} compiler generates a meta-tool (a functor)
that takes a tool generator and specializes it for use with
the particular description.  Section~\ref{sec:gen-tool} describes the
tool framework and gives examples of three format-independent 
tools that we have
implemented: a data printer useful for description debugging,
an accumulator that keeps track of error information for
each type in a data source and a formatter that transforms a data
source into XML.

\cut{\textbf{***THEN compare to \padsc{} in one paragraph only}}
\padsml{} has evolved from previous work on
\padsc{}~\footnote{We refer to the original
\pads{} language as \padsc{} to distinguish it from \padsml{}.}~\cite{fisher+:pads}, but
\padsml{} differs from \padsc{} in five significant ways.  First, it
is targeted at the \ml{} family of languages.  Using \ml{} as the host
language simplifies implementation of many data processing tasks, like
data transformations, which benefit from \ml{}'s pattern matching and
high level of abstraction.  Second, unlike \padsc{} types, \padsml{}
types may be parameterized by other types, resulting in more concise
and elegant descriptions though code reuse.  ML-style datatypes and 
anonymous nested tuples also help improve readability and compactness
of descriptions.  Third, \padsml{} provides
significantly better support for development of new tool generators.
In particular, \padsml{} provides a generic interface against which
tool generators can be written.  In \padsc{}, the compiler itself
generates all tools, and, therefore, developing a new tool generator
requires understanding and modifying the compiler.
Fourth, we have extended our earlier work on the Data Description Calculus
(\ddcold{})~\cite{fisher+:next700ddl} to account for \padsml{}'s
polymorphic types and we have simplified original presentation
of the parser semantics substantially, particularly for recursive types.  
We have 
proven that generated
parsers are type correct and
produce pairs of parsed data and parse descriptors in {\em canonical form}.  
Fifth and finally, we have extended our theory by giving a new interpretation of
\ddc{} descriptions as printers.  This new interpretation
helped guide and check our \padsml{} implementation.
Section~\ref{sec:ddc} presents the extended
calculus (\ddc{}), focusing on the semantics of 
polymorphic types for parsing and the key elements of the
printing semantics.

\cut{
\textbf{***Then give observations.}
\textbf{Actually, this observation should go in the conclusion.  It's
  distracting here.}
Our ``types as modules'' implementation strategy encountered the
  limits of the \ocaml{} module system in multiple ways. It therefore
  provides a natural, well-motivated challenge example for
  functional-programming researchers in type-directed programming and
  advanced module design.

Therefore, the combination of
  \padsml{} and \ocaml{} is a significant step towards a unified
  language for data description, transformation, and analysis.
}

\cut{\textbf{***Then, if still necessary give a pithy list of contributions}}

In summary, this work makes the following key contributions:
\begin{itemize}
\item We have designed and implemented \padsml{}, a functional
data-description language that can concisely express the syntactic
structure and semantic properties of data formats from numerous
application domains.
\item \padsml{} is inspired by, implemented in, and targeted to modern
functional languages, making it an outstanding example of how
functional languages can be used to solve
important data-management problems.  In particular, \ml{} modules and
functors provides an extensible framework for \padsml{} users to define their
own \padsml{} tools. 
\item We have defined the formal semantics of both \padsml{} parsers 
and printers.
\end{itemize}

\cut{
%% WHY IS PROCESSING IT HARD?
%% No control of source or target format
Once an analyst has the data, a common task is
converting the source format to a standard database loading format.
This transformation proceeds in three stages.  First, the analyst
writes a parser for the ad hoc format, using whatever (in)accurate
documentation may be available.  Second, he writes a program that 
detects and handles erroneous data records, selects records
of interest, and possibly normalizes records into a standard format,
for example, by reordering, removing, or transforming fields.
Unfortunately, the parsing, error handling, and transformation code is
often tightly interleaved.  This interleaving hides the knowledge of
the ad hoc format obtained by an analyst and severely limits the
parser's reuse in other applications.

\cut{In addition to poor documentation and error-prone data sources, other
common characteristics of ad hoc data make basic processing tasks
challenging.}

\cut{A common phenomenon is for a
field in a data source to fall into disuse.  After a while, a new
piece of information becomes interesting, but compatibility issues
prevent data suppliers from modifying the shape of their data, so
instead they hijack the unused field, often failing to update the
documentation in the process.
}

%% Sources, meaning, and handling of errors
Another challenge is the variety of errors and the
variety of application-dependent strategies for handling errors in ad
hoc data.  Some common errors, listed in \figref{figure:data-sources},
include undocumented data, corrupted data, missing data, and multiple
representations for missing values.  Some sources of errors
that we have encountered in ad hoc sources include malfunctioning
equipment, race conditions on log entry~\cite{wpp}, presence of
non-standard values to indicate ``no data available,'' human error
when entering data, and unexpected data values.  A wide range of
responses are possible when errors are detected, and they are highly
application dependent.  Possible responses range from halting processing
and alerting a human operator, to partitioning erroneous from valid
records for examination off-line, to simply discarding erroneous or
unexpected values.  One of the most challenging aspects of processing
ad hoc data is that erroneous data is often more important than
error-free data, because it may indicate, for example, that two
systems are failing to communicate.  Writing code that reliably
handles erros, however, is difficult and tedious.
}
% \subsection{\padsmlbig{} Architecture}


% In the next section, we will describe, in detail, the \padsml{} approach
% to data and meta data, the \padsml{}
% syntax for data description, and illustrative examples. Next, in
% Section~\ref{sec:} we will elaborate on
% \padsml{}'s support for data transformation, including design,
% syntax and some examples.  Section~\ref{sec:related-work} will discuss
% the related work. A discussion of conclusions and future work is
% included in section~\ref{sec:conclusion}.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
