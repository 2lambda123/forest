\section{Introduction}

\begin{figure*}
Cosmos event streams:
{\small
\begin{verbatim}
EventName=CosmosPerfCounter_1,CurrentTimeStamp=2010-04-10T02:07:54.950Z,Machine=msradb001,
name=\\Cosmos\\MessageSent,instanceName=Total,counterType=rate,currentValue=1,minValue=1,
maxValue=1,average=0,rate=1.18217E-006,runningTotalCount=0,runningTotal=0,timeIntervalMs=
845901093,CsProcessId=NONE
\end{verbatim}
}
Messages.sdb:
{\small
\begin{verbatim}
Jun  4 10:42:56 nid00004 sshd[5405]: Accepted publickey for root from 193.168.0.1 port 43484 ssh2
Jun  4 10:57:49 nid00003 syslog-ng[3504]: syslog-ng version 1.6.8 starting
Jun  4 10:57:53 nid00003 sshd[4069]: Server listening on :: port 22.
\end{verbatim}
}
\caption{Example ad hoc data sources}\label{fig:adhoc}
\vskip -2ex
\end{figure*}


Ad hoc data is any {\em non-standard}, {\em semi-structured} 
data source for which processing tools and libraries are not
readily available. HTML, XML, and data in relational databases are not ad hoc because 
many tools exist to manage such data.
Despite efforts to standardize data formats, ad hoc data persists
in many domains ranging from computer system administration to 
financial transactions
 to health care to computational biology. Figure 
\ref{fig:adhoc} shows fragments of two ad hoc data sources.
The first example comes from Microsoft's distributed computing
infrastructure Cosmos. The second example is excerpted from the file 
\texttt{/var/log/messages}
on a CRAY supercomputer.

People continue to produce and use ad hoc data because such formats 
are expedient and compact.  
Typical uses of these data sources include system fault monitoring
by tracking vital system health parameters in the system logs,
intrusion detection by matching access patterns to intrusion
models and data mining of scientific and financial data.

Despite the expediency of producing ad hoc data, these
data formats become very difficult to deal with because of missing
documentation, the lack of tools, and corruptions caused by
repeated redesign and re-engineering over time. 
In the past, ad hoc data analysis usually involved
writing a shell script or one-off wrapper program to parse each 
data format, a practice which is expensive, error-prone and brittle.

The \pads{}\anon{}{\footnote{Name substitution for blind review.}}
project \cite{padsweb} aims to solve the above 
problems. The central technology is a declarative, type-based, 
data description language that allows the user to specify the physical
layout of data sources as well as semantic properties of the data. 
\pads{} specifications can be compiled into a suite of
processing tools such as a statistical reporting tool, an 
XML converter and a query engine, and programming libraries 
including parser, printer and traversal functions. 

An impediment to using \pads{} is the need to learn
the description language and the effort required to
write \pads{} descriptions, which can be time consuming
if the data format is large and complex. Evolving data formats 
present an additional challenge because the descriptions must be updated.
For example, when analyzing Cisco router
logs\anon{ at AT\&T,}{,} we noticed that the format changes after the first 2GB, 
and then again after another few GBs. The sudden change in format is
the result of router firmware upgrades.

The large scale as well as the streaming and evolving nature of many ad hoc
sources led us to believe that a system which automatically {\em learns}
a \pads{} description of a given data source and incrementally updates that
description as the source evolves could significantly improve the productivity of ad hoc data users.
As a first step, we developed an unsupervised algorithm \learnpads{}
\cite{Fisher+:dirttoshovels,fisher+:sigmod08}
that automatically infers a \pads{} description of a data source by 
computing frequency statistics for the {\em tokens} in the data and using an information
theoretic score to guide description optimization.  
This algorithm, however,
requires all data fit into main memory and contains procedures 
that are quadratic to the size of data, 
and therefore cannot scale to very large sources. 

In this paper, we propose a new algorithm that automatically infers 
descriptions of large scale or continuous ad hoc data sources 
{\em incrementally}. The system takes as input an initial description 
and a new batch of data. It returns a
modified description that extends the initial description and covers the new 
data. The initial description may be supplied by the user or automatically
generated using the original \learnpads{} system. This iterative architecture
enables the learning of a very large data source by partitioning it 
into smaller batches and updating the description from one batch to the next. 
It also allows the user to take the description output at the end of
an iteration, make changes to it (\eg{}, renaming the 
automatically generated variable names), and insert the revised description back into the loop.

The main contributions of this paper are:
\begin{enumerate}
\item The design of a new system for generation of data descriptions
and end-to-end ad hoc data processing tools from example data.  
The system is incremental and interactive, allowing it to process
streaming data a chunk at a time, and allowing users to intercede
to correct, adapt or modify intermediate results. 
\item The engineering and optimization of algorithms that allow
the system to handle large, industrial data sources of 30GB or
more in a matter of a few hours.
\item The evaluation and analysis of the system
on 16 different examples drawn from
various industrial data sources.
\end{enumerate}

We presented an earlier, 6-page abstract of this material in an informal workshop~\cite{wasl09:zhu+}
that was reprinted unchanged and unrefereed in a SIG newsletter. 
Unlike the earlier paper, this paper presents a re-engineered algorithm
that is capable of processing data sources more than two orders of magnitude
larger, while at the same time producing outputs of higher quality. 
This paper also gives detailed empirical analysis of the 
correctness and scalability of the system.

In the rest of the paper, we give a brief overview of \pads{} and the original
\learnpads{} inference algorithm (\secref{sec:review}). We then describe
the new incremental inference algorithm (\secref{sec:algo})
and give a comprehensive experimental
evaluation of the system (\secref{sec:eval}). Finally, we compare this
system with some related work (\secref{sec:related}) and conclude
the paper (\secref{sec:conclude}).
