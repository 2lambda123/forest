\section{Evaluation}
\label{sec:eval}
The following experiments were done on a PowerBook G4 with a 1.67GHz PowerPC
CPU and 2G memory running on Mac OS X 10.4. 
Table \ref{tab:results} compares the performance results between
the original \learnpads{} system and the incremental system. 
The benchmarks used are 12 system logs with various sizes. Column 2
shows the number of lines in each log. The time columns record the total
running time in seconds, and the CT columns record the type complexity
of the final description learned. In general, lower type complexity means
more compact description. For the first 7 benchmarks which are smaller
and simpler, the initial learn size is 100 lines and the 
incremental learn size is 100 lines. For the last 5 benchmarks, 
the initial learn size is 500 lines while the incremental size remains
100 lines. Where it takes more than half an hour for the original \learnpads{}
system to learn a description, a ``-'' is put in those cells to 
indicate there is no result.

\begin{table}[th]
\begin{tabular}{|c|c|c|c|c|c|}\hline
& & \multicolumn{2}{|c|}{original} & \multicolumn{2}{|c|}{incremental} \\ \cline{3-6}
\raisebox{1.5ex}[0pt]{Formats} & \raisebox{1.5ex}[0pt]{Lines} &
	Time & CT & Time & CT \\ \hline \hline
ai.3000	&	3000	& 32	& 412.6	& 2.2	& 571.0	\\ \hline
asl.log  &	1500	& 31.9	& 960.1	& 23.9 	& 1393.3 \\ \hline
apache.txt  &	2087	& 83.9 & 1746.1 & 7.9 	& 2527.6 \\ \hline
access\_log  &	8188 	& 134.5	& 351.2	& 2.9	& 321.2	\\ \hline
error\_log  &	4510	& 101.7	& 101.9	& 0.9	& 107.9 \\ \hline
interface.loop & 1253	& 54.0	& 741.4	& 1.9	& 1053.7 \\ \hline
ai.big	&	57368	& -	& -	& 30	& 533.9 \\ \hline
pws	&	 17365	& -	& - 	& 133  & 5869 \\ \hline
coblitz.access & 9421   & -	& -	& 31.9 & 3005.2 \\ \hline
access.exlog & 260796 	& -	& -	& 610 & 3088.3 \\ \hline
debug\_getbig & 550409   & -	& -	& 668 & 9008.1 \\ \hline
dbg\_req\_redirect & 302554 & -	& -	& 1852 & 17567.7 \\ \hline
\end{tabular}
\caption{Execution times (secs) and type complexity (bits)} 
\label{tab:results}
\end{table}


% - parse metric
% - initial learn size and chunk size - these can affect results
% - update chunk by chunk 
% - optimizations/heuristics
%   due to performance concerns:
%   - memoization
%   - the clean function (to reduce the number of parses)
%   - control of aggregate size
%   - parses cut-off: kill a parse if it has more than n consecutive failures in a struct
%   due to quality of description concerns (and also performance)
%   - deterministic unions
%   - longest match in arrays
%   - merge adjacent const strings (only punctuation and white spaces)
%   - error recovery
%
%- Experiments
%	1) comparison with old LearnPADS on several large datasets (ai.3000, asl.log, apache.txt, access\_log, error\_log, interface.loop)
%	   compare exec time and type complexity. For increment, use
%	   initial learn chunk of 100 and incremental error chunks of 100
%	2) use ai format, do a table with 1000 5000 10000,...,1M miles
%	   using orig learning system and the new system with various optimization turned on and off.
%

