- need lookahead, recovered string may contain legal tokens which is further down the road
- not all const strings can be used as sync tokens - maybe long ones only, spaces?
- consider using special base types as sync tokens like url, date, time, etc - these not likely to go wrong
- problem with not merging adjacent const strings:
  1. more option nodes created
  2. too many single char or short const strings (confusing the partial and recovered mode)
  3. maybe we can't use const string as sync token any more 

- problem with merging adjacent const strings:
  1. difficulty with finding the partial tokens
  2. need lookahead also
  3. most single-char const strings can't be used as sync token, maybe only white space can.

smaller initial chunk size can produce bigger and more complex init description?

We may want to decrease the learn chunk size so that description can be updated more
frequently and more up to date. This will decrease of time to parse because parsing good
data is faster than parsing bad ones.


Example 1:

ppp31.igc.org - amnesty [16/Oct/1997:08:40:11 -0700] "GET /members/afreport.html HTTP/1.0" 200 450

Pstruct(Id = BTy_103 400, raw: 100551.077b)
        Poption(Id = BTy_116 0, raw: 0b)
                [IP] (Id = BTy_1 400, raw: 12805.044b);
        End Poption;
        Poption(Id = BTy_122 0, raw: 0b)
                [Host] (Id = BTy_118 682, raw: 0b);
        End Poption;
        [StringConst] " - " (Id = BTy_3 400, raw: 11.044b);
        Poption(Id = BTy_146 0, raw: 0b)
                Pstruct(Id = BTy_144 1, raw: 0b)
                        [StringConst] "amnesty" (Id = BTy_130 1, raw: 0b);
                        [White] (Id = BTy_132 1, raw: 0b);
                        [StringConst] "[" (Id = BTy_134 1, raw: 0b);
                        [Date] (Id = BTy_136 1, raw: 0b);
                        [StringConst] ":" (Id = BTy_138 1, raw: 0b);
                        [Time] (Id = BTy_140 1, raw: 0b);
                        [White] (Id = BTy_142 1, raw: 0b);
                End Pstruct;
        End Poption;
        [StringConst] "-" (Id = BTy_10 400, raw: 11.044b);
...

        Poption(Id = BTy_188 0, raw: 0b)
                [StringConst] "HTTP" (Id = BTy_72 400, raw: 29.044b);
        End Poption;
        Poption(Id = BTy_189 0, raw: 0b)
                [StringConst] "/" (Id = BTy_74 400, raw: 11.044b);
        End Poption;
        [Pfloat] (Id = BTy_77 400, raw: 5740.984b);
        Poption(Id = BTy_190 0, raw: 0b)
                [StringConst] """ (Id = BTy_84 400, raw: 11.044b);
        End Poption;
        Poption(Id = BTy_191 0, raw: 0b)
                [StringConst] " " (Id = BTy_88 400, raw: 11.044b);
        End Poption;
        Poption(Id = BTy_192 0, raw: 0b)
                [Int] [200...404] (Id = BTy_90 400, raw: 3093.139b);
        End Poption;
        Poption(Id = BTy_193 0, raw: 0b)
                [StringConst] " " (Id = BTy_93 400, raw: 11.044b);
        End Poption;
        [Blob] (Peor) (Id = BTy_98 400, raw: 8429.044b);

Example 2: 
polux.entelchile.net - - [15/Oct/1997:21:08:38 -0700] "GET /ainews.html HTTP/1.0" 200 13435

Using the old metric:

The top 1 parses: 
Tuple {
    ErrorB
    Fail
    Fail
    Fail
    Fail
    Fail
    Fail
    ErrorB
    Fail
    ErrorB
    Fail
    Fail
    Fail
    Part(polux)
    Fail
    Union (1) {
        Tuple {
            Fail
            Union (0) {
                Good(.entelchile.net)
            }
        }
    }
    Good( )
    Fail
    Fail
    ErrorB
    Fail
    Fail
    Fail
    Fail
    Good(- - [15/Oct/1997:21:08:38 -0700] "GET /ainews.html HTTP/1.0" 200 13435)
}
Metric = (23, 0, 91)

Using the new metric:

Tuple {
    ErrorB
    Rec(polux.entelchile.net)( )
    Good(-)
    Good( )
    Good(-)
    Good( )
    Good([)
    GoodB(15/Oct/1997)
    Good(:)
    GoodB(21:08:38 -0700)
    Good(])
    Good( )
    Good(")
    Good(GET)
    Good( )
    Union (1) {
        Tuple {
            Good(/)
            Union (0) {
                Good(ainews.html)
            }
        }
    }
    Good( )
    Good(HTTP)
    Good(/)
    GoodB(1.0)
    Good(")
    Good( )
    Good(200)
    Good( )
    Good(13435)
}
Metric = (3, 20, 71)

**** Rewriting (e stands for epsilon, S is struct and U is union): 

          S
       /     \
      U      OPT
     / \     / \
    A   B   C   e

We can't say anything about the dependency between Union(A, B) and C because Union(A, B) was there
before the OPT node was added. Suppose B always appears with e, then the above structure
can be rewritten to: 

         U
       /   \
      S     S
    /   \  /  \
   U    C  B   e
  / \
 A   B

But notice that the first structure has only 7 nodes and the second structure has 9 nodes. So
in this case the rewriting is not very helpful.

We may need to keep the OptsTable from chunk to chunk. Imagine the following scenario:

After chunk 1:

        S
     /  |  \
    A  Opt1 B
       

Because the Opt1 node is alone, no rewriting is applied to it.

After chunk 2:

          S 
     /  /   \    \ 
    A  Opt1 Opt2  B

Another opt node is added. If we don't have info about Opt1, we can't do rewriting on these 
two opt nodes.
      
Potentially

     S
   /   \
  Opt  Opt
   |    |
   A    B

Can be rewritten to

      U
    /   \
   Opt   B
    |
    A

The disadvantage of doing this kind of rewrite is that it 
can overfit the data and breaks down when we see more data 

****************************************************
Current state of incremental learning system
as on Apr 24, 2009
****************************************************

OVERVIEW:

The increment program takes an initial chunk of data
file, learns an initial description, and then parses
another data file line by line into an aggregate data
structure, while recording the decisions made at newly
created Opt nodes into a bit-array table. Opt nodes are
created when a base type failed to parse, or if some
data is recovered at refined base type, in which case
a learn node is created. A learn node is essentially
an Opt node wrapped around new description learned from
the failed data. Also if we have a refined base type 
and in the input, we see some data that doesn't comform
to the refined type but belong to the same base type,
such as "35" when we try to parse Int (0, 10), we create
a partial token. Note that in all of the refined base
types except for int ranges, a partial token carry 
an error count of 1, but int range partial token
is assumed to be error free. I'm not quite sure how
to handle the metric for these partial tokens yet.  
The above decision is made because I think int ranges
tend to over fit a lot so if we see an int that's out
of range, chances of it being a correct parse is very
high.

When all lines were parsed into the aggregate, the 
previous description is updated using the aggregate
by turning some of the base types into option nodes, 
learning new descriptions at the learn nodes, and relaxing
refined types when we have created some partial tokens at
these types. Note the previous description before learning any 
given chunk doesn't have adjacent constant strings merged. 
This has the effect of keeping all the tokens separate. 
Merging the tokens will make discovering partial tokens difficult.
A number of data independent rewriting rules are applied at
this stage to unnest structs and unions, and remove degenerated
lists. After that, using the bit-array table we create during
aggregation process, we rewrite adjacent options into a single
option, and convert adjacent options into unions. Note that
to do these rewritings, we introduce a couple of new rewriting
phases in the original reduce.sml. 

At the very end of parsing the entire data file (after multiple
chunks where the chunk size is defined by the number of bad lines
we see), we apply the merge const string rewriting rule to compact
the description. Descriptions updated after each chunk is written 
an output directory inc_output/datafile/TIMESTAMP/ with also
the Makefile and .c file to generate a PADS accumulator. 

I did some preliminary evaluation on the system. The system works
reasonably well for the web server logs such as ai.big, apache.txt.
But slow with more complex data such as irvpiv1.tail.sel.
The incrementally learned description is very sensitive to the 
initial description. So it seems that we need to trade off between
producing good initial description from learning large initial
chunk which takes longer, and using a less optimal initial
description but then incrementally improving this
description at a higher cost.


TODO:

Our current parsing algorithm is optmized from a brute force
version of the initially proposed "parse_all" function, in which
we memoize parses we have generated before for each 
(begin, end) location pair. It is also optimized to do 
deterministic branch selection (committed choice) when parsing
unions. In addition, it uses the "longest match" principle when
parsing arrays, i.e. it returns the longest parse as long as there
is no errors in the parses. The above two optimizations coincide
with the parsing semantics of PADS. Another heuristic we use
to produce better description is to prevent recovered token to
skip too many characters. If the skipped data is a certain
factor longer than the recovered data itself, we say this is a
dubious parse, and we include with it a failure parse. Exactly
which one of these two parses are better will be resolved at
the end of parsing the entire struct. This method proves to be
very successful in avoiding false recovered parsing. However,
it increases the state space of the parser program. 
Dave suggested killing a parse if it has more than three consecutive
failure within a struct. I haven't tried this yet.

Another possible data dependent rewriting rule is to merge
adjacent unions with options. We don't have data summary for
unions that were learned from the previous chunk or iteration.
But we can certainly record the branching decisions made for
unions in addition to options into the table and pass it along
from chunk to chunk. One issue is that the table will grow so
we may have to cap the size of it and throw away old entries.
However, I have the feeling that any of such data dependent 
analysis could have a down side of overfitting. So we need 
to also define rewriting rules that would "undo" the merging
if we find the dependency is broken when we see more data.

Currently no blob finding is done in the incremental learning.
Also the blob finding might need to be normalized to the
data size. Currently it seems that when data size is small
less blob finding is done.


On Performance:

1. Gold description to parse irviprv.tail.sel (21 lines) parses in 0.5 secs.
   However when I create a new file irviprv.tail.sel.2100 which contains 2100 lines
   of the same data, and use the gold description to parse it, it takes increasingly
   longer to parse a line - an apparent performance degradation is noted.
   A prelim investigation showed that the OptsTable and aggregate structure is not growing. 
   So exact reason for this degradation is unknown.
   *** this has been solved: reason is the Ty was not initialized with proper labels
   for the golden description. And when labels are not available, it is generated on
   the fly and takes up memory.

2. Learned description of irviprv.tail.sel takes a long time to parse even one line.
   The description doesn't really have very deep structures but very large structs with 
   many StringConst fields of punctuation and strings. The learned description doesn't
   capture the main array in the orig data well but instead, create unions for different
   variations. Each union branch contains one of these very large structs. 
   The main problem, as my prelim investigation shows, is the parse space for the long
   structs is getting very large. This is because I add an alternative of Failure parse
   when a refined base type is recovered after skipping too many chars. And this happens
   quite often in this data. So two choices get multiplied a number of times in a long
   struct and produce large number of alternatives to parse.

3. I also noticed a long pause (around 2-5 mins) at some point after parsing the 
   last field of a long
   struct BTy_1039. The reason for that is unknown for now.

4. Because of the way union branches are ordered, the parsing process can get stuck 
   (very slow) in one wrong branch, without trying the correct branch (I expect parsing
   the correct branch will be faster since no skipping chars).

   Possible fixes: 
   - improve the learned description - try to discover the arrays correctly
   - merge adjacent constant punctuations - this may reduce the number of failure parses 
     introduced
     *** try this but doesn't help much - it doesn't reduce the number of const strings
         a lot, so the space is still big.
   - Added "parses cut-off", a feature that cuts a parse which contains
        a number of consecutive failures when parsing a struct. Because
        introducing this feature may produce an empty parse set, our
        heuristics is to re-parse the line with no cut-off enabled if
        we see an empty set.
   - some sort of concurrent parsing of unions to kill the other attempts when one branch
     succeed (same idea as the GCC). 


TODOs after WASL paper:

1. Combine the def of path and url into one token, let's just call it
   path for now and let's phase out the URL token.
2. Implement the new rewriting rule to crack open Option nodes
   which are embeded in unions and re-sort them
3. Examine the pws to see if errors can be eliminated
4. Examine the descriptions learned from other examples to see
   if the quality of these descriptions can be improved somehow
5. Look at the way blob_finding is working to see why sometimes
   they are too aggressive and sometimes they are not.
6. Investigate why some matching tokens like ( and ) are not paired properly
7. Rewriting rule to find more general Path token (exploiting the vertical info)
8. Experimental set up to measure good N (initial chunk size) and M (incremental
   chunk size).
