%\subsection{Introduction}
%\label{ssec:intro}


Complex networked systems and applications must be {\em monitored} to
proactively find problems, record/archive system health, oversee
system operation, detect malicious processes or security violations
and perform a myriad of other tasks.  
The monitoring infrastructure necessary to perform such tasks should
be able to include embedded sensors that monitor physical processes,
machine-level monitors to manage server infrastructure, intrusion
detection systems that monitor network traffic, and application-level
monitors that can observe applications within a system. Monitoring
should be able to span multiple such systems, spread across a facility
or multiple locations.

Unfortunately, developing and maintaining monitoring
infrastructure is a tremendously difficult, costly and error-prone
enterprise. A substantial part of the problem arises because
the data produced by networked systems and applications is
varied and voluminous, poorly documented and filled
with errors.  Consequently, application developers normally lack the time,
skill or discipline to create robust monitoring tools, and, even if they do,
keeping their tools up-to-date as the underlying applications change
is tedious and time-consuming.  As a
result, complex systems are usually under-monitored. They can
fail in ways the associated monitoring system cannot observe or help
diagnose, leaving high-value systems vulnerable to a myriad of threats.

%% Complex systems must be {\em monitored} to proactively find problems,
%% record/archive system health, oversee system operation, detect
%% malicious processes or security violations and perform a myriad of
%% other tasks.  Unfortunately, application developers may lack the time, skill
%% or discipline to create monitoring tools and update them 
%% as the underlying applications change. As a result, complex systems
%% are often under-monitored, and can fail in ways that the monitoring
%% system cannot observe and help diagnose. Failures are therefore harder
%% to diagnose, and may lead to significant vulnerabilities, because the 
%% conditions that preceded the failure are often not captured.

%% To address these problems, we propose to develop a unified {\em
%% monitoring infrastructure} that can {\bf automatically} perform a
%% number of tasks that are currently performed manually, if at all.
%% This unified system should be able to include embedded sensors to
%% monitor physical processes, machine-level monitors to manage server
%% infrastructure, intrusion detection systems to monitor network
%% traffic, and application-level monitors to observe applications
%% within a system. Monitoring should span multiple such
%% systems, spread across a facility or multiple locations. It should be
%% scalable and flexible, allowing the collection of multiple types
%% and large amounts of data.

The only hope to address this immense problem, without placing impractical
burdens on application developers or system administrators, is to develop
new technologies capable of {\em automating} the monitoring process
to a much further degree than ever before thought possible.  We propose to do
so in a unique and revolutionary way --- by combining the PIs talents
in {\em programming language design} and {\em systems implementation} to develop 
a novel system for {\em automatic generation of monitoring and anomaly 
detection tools.}

\paragraph*{Language support and automation for distributed monitoring.} 
From the perspect of a naive user,
our monitoring infrastructure will look much like
existing systems:  It will have 
a component to acquire data continuously 
from complex distributed sources, a component to visualize
the current data flowing through the system and a component 
to query archival logs.  

What will make our system radically different from any other is that each
of these components will be {\em automatically generated} from
specifications written in a new high-level programming language, 
designed by the PIs, called
\pads{}~\cite{fisher+:pads,fisher+:popl06,mandelbaum+:pads-ml}.  
By generating software components automatically from
specifications, we obtain a host of important benefits.  First, the
\pads{} specification language provides a way to create 
uniform, easy-to-understand {\em documentation} of all data processed
or transmitted by any system component.  Second, generated tools are
{\em trustworthy} and {\em reliable}. Unlike manually implemented
tools, there will be no security vulnerabilities caused by buffer
overruns or other such low-level, manual coding errors.  Third,
generated tools are {\em evolvable} --- A simple change to a
specification and recompilation will generate new tools for an
evolving format.

Now, while automatic tool generation from specifications will serve as 
a new kind of robust and flexible architecture for the develop of
monitoring tools, it raises another question: where do we obtain the
data specifications themselves?  Such specifications can be written completely
by hand, but developing and maintaining them,
while superior to writing ad hoc data processing tools directly, 
can still be time consuming and difficult, particularly when facing many
new, unknown data sources.  Hence, a second key prong of our proposal
is to push automation still further into the tool development chain,
by {\em automatically inferring specifications} directly from 
unannotated, available data we have collected.  If one can automatically
infer a data format from available data and then push that
inferred format through our tool-generating compiler, one has a
complete, {\em end-to-end} mechanism for automatic generation of
monitoring infrastructure. 

Finally, in addition to developing new {\em off-line} techniques for
generating monitoring tools, we intend to develop new {\em on-line}
techniques for automatically classifying live wire data and
detecting anomalous network behavior.

While our proposal may sound highly ambitious, and it is, we have
assembled a team with the combined experience and capabilities
necessary to be able to succeed.  Kathleen Fisher (AT\&T research) 
is an expert in the design and implementation of domain-specific
programming languages and her work at AT\&T over the past 9 years
has involved developing innovative tools for managing AT\&T's
massive data streams.  Vivek Pai (Princeton) is an expert in distributed
systems and networked applications
and has been the primary architect of the CoMon monitoring system
for PlanetLab~\cite{planetlab}.  David Walker is an expert in language-based security
and has worked closely with Fisher on the design of programming languages
for data processing. 

\paragraph*{Structure of Technical Subsections.}
The following technical subsections explain the key components of
our proposed research in greater detail.  Section~\ref{sec:sniffer}
describes the construction of tools and algorithms for 
performing automated data characterization and analysis.  
Section~\ref{sec:pads} describes our proposal for a high-level, 
domain-specific programming language, called
\pads{}~\cite{fisher+:pads,fisher+:popl06,mandelbaum+:pads-ml}, that
allows users to specify the data that monitoring systems 
accumulate and present to users.  A compiler for \pads{} will
automatically generate data processing libraries, interfaces and tools
from such specifications.  Such tools and libraries will subsequently 
be able to be linked in to the
rest of the monitoring framework.   
Section~\ref{sec:inference} describes our proposal for a 
data format inference engine capable
of automatically generating \pads{} descriptions from
large, multi-level data archives.   Section~\ref{sec:comon}
describes CoMon -- the overaching monitoring system into which our other 
technologies fit.  Overall, this adaptive, scalable monitoring system will 
be able to present new and evolving data sources to operators, allowing them to
analyze and visualize data across multiple domains.  Finally, 
section~\ref{sec:naval} describes the relevance of our research to future
naval operations.


% Our approach consists of three components: (1) a non-intrusive
% application traffic sniffer that performs automated characterization
% and analysis, (2) a high-level language, called PADS, capable of
% specifying the data that monitoring systems accumulate, archive, and
% present to users, and (3) a scalable monitoring system, CoMon, that
% can adaptively present new data sources and allow operators to analyze
% and visualize data across multiple domains.

% Focusing on automation and high-level processing addresses a number of
% shortcomings of modern monitoring systems. Automated format inference
% and parsing of ad-hoc data allows application data to be sniffed on
% the wire, without having to modify applications. The monitoring system
% can react as applications change or as new applications are
% introduced. By working from the live data, rather than a separate
% representation of it, the monitoring system always has access to the
% ``ground truth'' rather than some abstraction that may fail to stay
% synchronized with its associated application.  While this approach may
% sound ambitious, we believe that automated monitoring is feasible and
% practical, and we discuss our approach later in the proposal.

%% This approach addresses a number of shortcomings of modern monitoring
%% systems.  First, the \pads{} specification language provides a 
%% way to create high-level, uniform, easy-to-understand {\em documentation}
%% of all data processed or transmitted by any system component, 
%% including legacy components, without having to modify or alter existing
%% systems or
%% application in any way.  Second, this documentation is 
%% {\em executable} --- the
%% \pads{} compiler will automatically generate a collection of
%% reliable, secure, and high-performance libraries and interfaces
%% to perform all low-level data processing tasks such as ingesting
%% data from multi-source archives, detecting and reporting
%% errors, gathering statistics, 
%% transforming data into standard formats and providing
%% general-purpose programmatic interfaces.  Third, generated
%% tools are {\em trustworthy} and {\em reliable}. Unlike manually 
%% implemented tools, there will be no security vulnerabilities caused by
%% buffer overruns or other such low-level, manual coding errors.
%% Fourth, generated tools are {\em evolvable}. A simple change to
%% a specification and recompilation will generate new tools for
%% an evolving format.  Fifth, new data with unknown formats can be analyzed
%% using novel format inference algorithms that automatically generate
%% candidate specifications.  Users can inspect and if necessary modify
%% such specifications. Once satisfied, they can invoke the \pads{}
%% compiler to generate new tools {\em with just the press of a button}.
%% Sixth, such automatically generated data-processing tools and libraries 
%% can be linked to the CoMon monitoring
%% system.  This linkage allows users to query
%% data, view it graphically, and analyze trends, all in real
%% time and without having to load a database or configure the system manually.



%% In the following subsections, we explain the key components of
%% our technical approach (1-5 listed above) and our research agenda in 
%% more detail.
