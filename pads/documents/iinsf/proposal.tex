\documentclass[11pt]{article}
\usepackage{times,url}
\usepackage{code} 
%\usepackage{proof}
%\usepackage{newcode}
%\usepackage{epsfig}

\input{defs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\voffset             0in    %  top vertical offset
\hoffset             0in    %  left horizontal offset
\oddsidemargin       0pt    %  Left margin on odd-numbered pages.
\evensidemargin      0pt    %  Left margin on even-numbered pages.
\topmargin           0pt    %  Nominal distance from top of page to top of
\headheight          0pt    %  Height of box containing running head.
\headsep             0pt    %  Space between running head and text.
\textwidth         6.5in    %  Width of text on page
\textheight          9in    %  Height of text on page
\setlength{\parskip}{.05in}
\renewcommand{\floatpagefraction}{.9}
\renewcommand{\textfraction}{0.1}
\renewcommand{\baselinestretch}{1.01}
%\input{macro}  %%%  other macro definitions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\setcounter{page}{1}
\pagenumbering{roman}
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{NSF 04-528 Project Summary:  \\
Automatic Tool Generation for Ad Hoc Scientific Data}
%%%%%%%
\input{summary}
\newpage
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Table of Contents}
% \tableofcontents\
% \vspace*{.1in} 
% \begin{center}
% {\Large\bf\em{}This is a place-holder. Fastlane generates this page automatically.}
% \end{center} 
% \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{arabic}
\setcounter{page}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Project Description}

\subsection{Introduction}
\label{ssec:intro}

An {\em ad hoc data format} is any nonstandard data format for which
parsing, querying, analysis or transformation tools are not readily
available.  \xml{} is not an ad hoc data format --- there are hundreds
of tools for reading, querying, and transforming \xml{}.  However,
network administrators,
data analysts, computer scientists,
biologists, chemists, and physicists deal with ad hoc
data in a myriad of complex formats on a daily basis.
This data, which is often unpredictable, poorly documented,
filled with errors (malicious and benign) and unwieldy,
poses tremendous challenges to its users and the software
that manipulates it.  
Our goal is to alleviate the burden, risk and confusion associated
with ad hoc data by developing a universal data processing system
capable of 

\begin{enumerate}
\item concisely and accurately describing any ad hoc data source at an 
easy-to-understand, high-level of abstraction, and
\item automatically generating a suite of highly reliable and
efficient data processing tools that make understanding, manipulating 
and transforming ad hoc data an effortless task.
\end{enumerate}

We have already begun to develop a system called \pads{} (Processing
Ad hoc Data Sources) that helps to address some of these concerns.
It includes a high-level, declarative language for describing
ad hoc data formats and it is possible to generate tools for
parsing data, printing statistical summaries of data and transforming 
data into \xml.  We propose to improve and generalize our work 
in several important ways.

\begin{enumerate}
\item The current \pads{} system is far from a universal
data processing platform.  There are many commonly-used
data formats \pads{} cannot currently handle.  We propose
a number of innovative ways of extending the \pads{} description language
and compiler to alleviate these deficiencies.
\item The current \pads{} tool-generation infrastructure is brittle and
inflexible, and the generated tools have sub-par performance.  We propose
to improve the tool-generation infrastructure by augmenting \pads{}
with a system of user-defined attributes that can communicate
semantic information to the tool generator and implementing a mechanism for 
application-specific customization.
\item The \pads{} description language has no specification of its 
meaning or properties.  We propose to define a formal semantics for 
\pads{}, prove properties of its data processing algorithms and
use our formal model to re-evaluate, debug and generalize our 
implementation.  
\item In addition to our work on using \pads{} to implement applications 
in the networking and telecommunications domains, part of our mission
is to have a broad impact on researchers who do data processing
in the natural sciences, including physics, biology and chemistry.
We have already developed a partnership with
Olga Troyanskaya in Princeton's Lewis-Sigler Institute for 
Integrative Genomics
and are studying her data processing problems.  We believe that 
we have a tremendous
opportunity to make an impact on the productivity of 
genomics researchers at Princeton
and elsewhere by supplying them with easy-to-use tools produced by our system.
\item To extend and support our relationship with the Genomics Institute,
and to have a broad impact on interdisciplinary education,
we plan to develop undergraduate research projects in which
computer science majors use \pads{} to help biologists 
with their data processing problems.  
\end{enumerate}

Overall, our research combines novel language design, high-performance
systems engineering and theoretical analysis.  It will substantially
increase the overall productivity of data analysts, researchers and
software architects who deal with ad hoc data regularly, and it will
improve the security and reliability of the software they produce.
Finally, our research will also have a broad impact on research in the
natural sciences, where ad hoc data is pervasive, and on
interdisciplinary computer science.

In the rest of this introduction, we will explain the immense
challenges and risks posed by ad hoc data in more detail.  We will
explain how our proposed system as a whole can deal with them.  We
will then describe some of the features of \pads{} we have already
implemented.  After this introductory section, we will describe the
key components of our new research agenda and explain how our research
will make a broad impact on disciplines well outside the bounds of
traditional computer science and on interdisciplinary education.

\subsubsection{The Challenges of Ad Hoc Data}

There are vast amounts of useful data stored in
traditional databases and \xml{} formats, but there is just as much in
ad hoc formats.  \figref{figure:data-sources} provides some information
on ad hoc data formats from several different domains ranging from genomics
to cosmology to networking to finance to internal corporate billing information.  
They include ASCII, binary, and Cobol data formats, with
both fixed and variable-width records, ranging in size from
relatively small files through network applications which process over
a gigabyte per second.  Common errors include undocumented data,
corrupted data, missing data, and multiple missing-value
representations.


\begin{figure*}
\begin{center}
\begin{tabular}{@{}|l|l|l|l|l|}
\hline
Name: Use                           & Representation    & Processing Problems \\ \hline\hline
Gene Ontology (GO)~\cite{??}:                  & Variable-width    & White-space ambiguities \\
Gene Product Information 	      & ASCII records & Unused field options \\ \hline
Reglens Data~\cite{??}:                & Floating point numbers, & Repeated multiplicative error \\
Weak gravitational lensing analysis   & among others & \\ \hline
Web server logs (CLF):                & Fixed-column      & Race conditions on log entry\\ 
Measuring web workloads               & ASCII records     & Unexpected values\\ \hline
Call detail: Fraud detection          & Fixed-width       & Undocumented data\\
                                      & binary records  & \\ \hline 
AT\&T billing data:      & Cobol  &  Unexpected values\\ 
Monitoring billing process   & & Corrupted data feeds \\ \hline
Newick:   Immune  & Fixed-width ASCII records & None \\ 
system response simulation & in tree-shaped hierarchy &\\ \hline                                
OPRA:              & Mixed binary \& ASCII records 
      & 100-page informal \\
Options-market transactions & with data-dependent unions & documentation \\ \hline
Palm PDA:          & Mixed binary \& character & No high-level  \\
Device synchronization  & with data-dependent constraints & documentation available\\ \hline
\end{tabular}
\caption{Selected ad hoc data sources.}
\label{figure:data-sources}
\end{center}
\end{figure*}


Processing ad hoc data is challenging for a variety of further reasons. 
First, ad hoc data typically arrives ``as is'': the analysts
who receive it can only say ``thank you,'' not request a more
convenient format.  Second, documentation for the format may not exist
at all, or it may be out of date.  A common phenomenon is for a field
in a data source to fall into disuse.  After a while, a new piece of
information becomes interesting, but compatibility issues prevent data
suppliers from modifying the shape of their data, so instead they
hijack the unused field, often failing to update the documentation in
the process.

Third, such data frequently contain errors, for a variety of reasons:
malfunctioning equipment, race conditions on log entry~\cite{wpp},
non-standard values to indicate ``no data available,'' human error in
entering data, unexpected data values, \etc{} The appropriate response
to such errors depends on the application.  Some applications require
the data to be error free: if an error is detected, processing needs
to stop immediately and a human must be alerted.  Other applications
can repair the data, while still others can simply discard erroneous
or unexpected values.  For some applications, errors in the data can
be the most interesting part because they can signal where two systems
are failing to communicate.

A fourth challenge is that ad hoc data sources can be high volume:
AT\&T's call-detail stream contains roughly 300~million calls per day
requiring approximately 7GBs of storage space. Although this data is
eventually archived in a database, analysts mine it profitably before
such archiving~\cite{kdd98,kdd99}. More challenging, the \ningaui{}
project at AT\&T accumulates billing data at a rate of 250-300GB/day,
with occasional spurts of 750GBs/day. Netflow data arrives from Cisco
routers at rates over a gigabyte per second~\cite{gigascope}! Such
volumes mean it must be possible to process the data without loading
it all into memory at once.

Finally, before anything can be done with an ad hoc data source,
someone has to produce a suitable parser for it.  Today, people tend
to use \C{} or \perl{} for this task.  Unfortunately, writing parsers
this way is tedious and error-prone, complicated by the lack of
documentation, convoluted encodings designed to save space, the need
to produce efficient code, and the need to handle errors robustly to
avoid corrupting down-stream data.  Moreover, the parser writers'
hard-won understanding of the data ends up embedded in parsing code,
making long-term maintenance difficult for the original writers and
sharing the knowledge with others nearly impossible.

\subsubsection{\pads{}:  Taking on the Challenge of Ad Hoc Data}

The \pads{} system makes life easier for data analysts by addressing
each of the concerns outlined in the previous section.
It provides a declarative data description
language that permits analysts to describe the physical layout of
their data, \textit{as it is}.  The language also permits analysts to
describe expected semantic properties of their data so that deviations can
be flagged as errors. The intent is to allow analysts to capture in a
\pads{} description all that they know about a given data source
and to provide the analysts with a library of useful routines in exchange. 


\pads{} descriptions are intended to be concise enough to serve as
documentation and flexible enough to describe many ASCII, binary,
Cobol, and mixed data formats.  In addition, useful software tools
can be generated from the descriptions and this feature provides
strong incentive for keeping the descriptions current, allowing them
to serve as living documentation.  

Given a \pads{} description, the \pads{} compiler produces
customizable \C{} libraries and tools for parsing, manipulating, and
summarizing the data.  The core \C{} library includes functions for
reading the data, writing it back out in its original form, writing it
into a canonical \xml{} form, pretty printing it in forms suitable for
loading into a relational database, and accumulating statistical
properties.  An auxiliary library provides an instance of the data API
for Galax~\cite{galax,galaxmanual}, an implementation of XQuery.  This
library allows users to query data with a \pads{} description as if
the data were in \xml{} without having to convert to \xml{}.  In
addition to these libraries, the \pads{} system provides wrappers that
build tools to summarize the data, format it, or convert it to \xml{}.

The declarative nature of \pads{} descriptions facilitates the
insertion of error handling code.  The generated parsing code checks
all possible error cases: system errors related to the input file,
buffer, or socket; syntax errors related to deviations in the physical
format; and semantic errors in which the data violates user
constraints.  Because these checks appear only in generated code, they
do not clutter the high-level declarative description of the data
source.  Moreover, since tools are generated
automatically by a compiler rather than written by hand, 
they are far more likely to be robust
and far less likely to have dangerous vulnerabilities such as
buffer overflows.  Moreover, as formats change and are updated,
one can make a small change in the high-level description
and the compiler will takes care of the rest.  It is extremely unlikely
new vulnerabilities or buffer overflows will be added to the code
base over time as formats change.

The result of a parse is a pair consisting of a canonical
in-memory representation of the data and a parse descriptor. The parse
descriptor precisely characterizes both the syntactic and the semantic
errors that occurred during parsing.  This structure allows analysts
to choose how to respond to errors in application-specific ways.

With such huge datasets, performance is critical. The \pads{} system
addresses performance in a number of ways.  First, we compile the
\pads{} description rather than simply interpret it to reduce run-time
overhead.  Second, the generated parser provides multiple entry
points, so the data consumer can choose the appropriate level of
granularity for reading the data into memory to accommodate very large
data sources.  Finally, we parameterize library functions by
\textit{masks}, which allow data analysts to choose which semantic
conditions to check at run-time, permitting them to specify all known
properties in the source description without forcing all users of that
description to pay the run-time cost of checking them.

Given the importance of the problem, it is perhaps surprising that
more tools do not exist to solve it.  \xml{} and relational databases
only help with data already in well-behaved formats.  Lex and Yacc are
both over- and under- kill.  Overkill because the division into a
lexer and a context free grammar is not necessary for many ad hoc data
sources, and under-kill in that such systems require the user to build
in-memory representations manually, support only ASCII sources, and
don't provide extra tools.  ASN.1~\cite{asn} and related
systems~\cite{asdl} allow the user to specify an in-memory
representation and generate an on-disk format, but this doesn't help
when given a particular on-disk format.  Existing ad hoc description
languages~\cite{gpce02,sigcomm00,erlang} are steps in the right
direction, but they focus on binary, error-free data and they do not
provide auxiliary tools.

% We propose to extend and improve the current \pads{}\ prototype by engaging
% in three intertwined research efforts:

% \begin{enumerate}
% \item Our experience with real-world data has revealed that
% the current \pads{}\ system is unable to process many common
% data formats.  We propose a number of extensions to \pads{} 
% that will greatly improve its expressive power and enable
% us to handle many more important ad hoc formats.
% \item While \pads{}\ can automatically generate tools for providing
% statistical data summaries, querying ad hoc data and generating
% \xml, the tool generation process is extremely brittle, inflexible
% and results in low-performance software.  We propose to redesign
% the automatic tool generation process using a principled methodology
% based upon a novel \pads{}-attribute system that can communicate
% semantic information across tools.  Our new methodology will
% improve the flexibility and reliability of the \pads{} system.
% \item There is no formal description of the \pads{} language
% and no semantics for the compiler or tools.  We propose to
% formalize the language and analyze the semantics of the
% compiler.  Our formal analysis may find bugs or flaws in the
% \pads{} system and will improve our confidence in the implementation.
% \end{enumerate}

% \noindent
% In addition, part of our mission will be to use \pads{} to provide
% data processing tools for researchers in the natural sciences including
% biology, chemistry and physics.  

% In building the \pads{} system, we faced challenges on two fronts:
% designing the data description language and
% crafting the generated libraries and tools.
% In the rest of the paper, 
% we describe each of these designs.
% We also give examples to show how useful they have been at AT\&T.


\subsubsection{The Current \pads{} Language Infrastructure}

% \figref{figure:data-sources} summarizes some of the sources we have
% worked with.  They include ASCII, binary, and Cobol data formats, with
% both fixed and variable-width records, ranging in size from
% relatively small files through network applications which process over
% a gigabyte per second.  Common errors include undocumented data,
% corrupted data, missing data, and multiple missing-value
% representations.


% \subsection{Common Log Format}
% Web servers use the Common Log Format (CLF) to log client
% requests~\cite{wpp}.  Researchers use such logs to measure
% properties of web workloads and to evaluate protocol changes
% by "replaying" the user activity recorded in the log.
% This ASCII format consists of a sequence of
% records, each of which has seven fields: the host name or IP address
% of the client making the request, the account associated with the
% request on the client side, the name the user provided for
% authentication, the time of the request, the actual request, the
% \textsc{http} response code, and the number of bytes returned as a
% result of the request.  The actual request has three parts: the
% request method (\eg, \texttt{GET}, \texttt{PUT}), the requested
% \textsc{uri}, and the protocol version.  In addition, the second and
% third fields are often recorded only as a '-' character to indicate
% the server did not record the actual data.  \figref{figure:clf-records}
% shows a couple of typical records.


In order to understand further how \pads{} can be used,
will take a look at an example of ad hoc data:
a tiny fragment of data from the Gene Ontology Project~\cite{go}.  
Data in this format is widely used by molecular biologists to
analyze gene products produced by various organisms.
\figref{figure:dibbler-records} shows a tiny sample of
the format.  Cursory examination of the example data reveals
first of all that this is an ASCII format with two 
logical parts, a header portion (that portion beginning with
\texttt{format-version} and ending with the first blank line)
and a main portion, which includes a sequence of 
information blocks separated by blank lines.  The format
designers call these information blocks {\em stanzas}.
Each stanza begins with a stanza tag, which may be either
\texttt{[Term]}, as shown in the example data, or
\texttt{[Typeref]}.  On the following line,
the first element of each stanza, is
the string \texttt{id}, followed by a colon, followed by a
GO identifier.  A GO identifier is the word GO, followed
by another colon, followed by a string.  
After the first mandatory id field, the rest of the stanza includes
any number of identifier, colon, field entries.
While this format is actually remarkably simple, and
we have left the several details in this coarse description,
it should be apparent that English is a poor
language for describing data formats!

\begin{figure*}
\begin{small}
%\begin{center}
\begin{verbatim}
format-version: 1.0
date: 11:11:2005 14:24
saved-by: midori
auto-generated-by: DAG-Edit 1.419 rev 3
default-namespace: gene_ontology
remark: cvs version: $Revision: 1.1 $
subsetdef: goslim_goa "GOA and proteome slim"
subsetdef: goslim_yeast "Yeast GO slim"
subsetdef: goslim_plant "Plant GO slim"
subsetdef: goslim_generic "Generic GO slim"
subsetdef: gosubset_prok "Prokaryotic GO subset"

[Term]
id: GO:0000001
name: mitochondrion inheritance
namespace: biological_process
def: "The distribution of mitochondria\, including the mitochondrial 
genome\, into daughter cells after mitosis or meiosis\, mediated by 
interactions between mitochondria and the cytoskeleton." [PMID:10873824, 
PMID:11389764, SGD:mcc]
is_a: GO:0048308 ! organelle inheritance
is_a: GO:0048311 ! mitochondrion distribution

[Term]
id: GO:0000002
name: mitochondrial genome maintenance
namespace: biological_process
def: "The maintenance of the structure and integrity of the mitochondrial 
genome." [GO:ai]
is_a: GO:0007005 ! mitochondrion organization and biogenesis
\end{verbatim}
\caption{Tiny example of Gene Ontology data. (Some lines broken to
present data on this page.)}
\label{figure:dibbler-records}
%\end{center}
\end{small}
\end{figure*}

Now, we can examine how to use \pads{} to describe 
the {\em physical layout} and 
{\em semantic properties} of our ad hoc data source. 
The language provides a type-based model:
basic types describe atomic data such as integers, characters, 
strings, dates, urls, \etc, while
structured types describe compound data built from simpler pieces.
\suppressfloats

In a bit more detail,
the \pads{} library provides a collection of broadly useful base
types.  Examples include 8-bit unsigned integers (\cd{Puint8}), 32-bit
integers (\cd{Pint32}), dates (\cd{Pdate}), strings (\cd{Pstring}),
and IP addresses (\cd{Pip}).  Semantic conditions for such base types
include checking that the resulting number fits in the indicated
space, \ie, 16-bits for \cd{Pint16}.  By themselves, these base types
do not provide sufficient information to allow parsing because they do
not specify how the data is coded, \ie{}, in ASCII, EBCDIC, or binary.
To resolve this ambiguity, \pads{} uses the \textit{ambient} coding,
which the programmer can set.  By default, \pads{} uses ASCII.  To
specify a particular coding, the description writer can select base
types which indicate the coding to use.  Examples of such types
include ASCII 32-bit integers (\cd{Pa_int32}), binary bytes
(\cd{Pb_int8}), and EBCDIC characters (\cd{Pe_char}).  In addition to
these types, users can define their own base types to specify more
specialized forms of atomic data.

To describe more complex data, \pads{} provides a collection of
structured types loosely based on \C{}'s type structure.  In
particular, \pads{} has \kw{Pstruct}s, \kw{Punion}s, and \kw{Parray}s
to describe record-like structures, alternatives, and sequences,
respectively.  \kw{Penum}s describe a fixed collection of literals,
while \kw{Popt}s provide convenient syntax for optional data.  Each of
these types can have an associated predicate that indicates whether a
value calculated from the physical specification is indeed a legal
value for the type.  For example, a predicate might require that two
fields of a \kw{Pstruct} are related or that the elements of a
sequence are in increasing order.  Programmers can specify such
predicates using \pads{} expressions and functions, written using a
\C{}-like syntax.  Finally, \pads{} \kw{Ptypedef}s can be used to
define new types that add further constraints to existing types.

\pads{} types can be parameterized by values.  This mechanism serves
both to reduce the number of base types and to permit the format and
properties of later portions of the data to depend upon earlier
portions.  For example, the base type \cd{Puint16_FW(:3:)} specifies
an unsigned two byte integer physically represented by exactly three
characters, while the type \cd{Pstring(:COLON:)} describes a string
terminated by a colon (when \texttt{COLON} is defined to be \texttt{':'}).  
Parameters can be used with compound types to
specify the size of an array or which branch of a union should be
taken.


\figref{figure:dibbler} gives an abbreviated \pads{} description 
for the Gene Ontology
data.  
We will use this example to illustrate some of the basic
features of the current \pads{} language.  
In \pads{} descriptions, types are declared before they are used, 
so the type that describes the totality of the data source appears 
at the bottom of the description.  In this case,
the type \texttt{OBO\_file} describes the the entirety of the
GO data source (the \texttt{Psource} type qualifier indicates
this fact explicitly).  \texttt{OBO\_file} is a \kw{Pstruct} type with
two fields, a \texttt{hdr} field, with type  \texttt{OBO\_header}
and a \texttt{stanzas} field with type \texttt{OBO\_stanza[]},
an array of \texttt{OBO\_stanza}s of arbitrary length.
In general, \kw{Pstruct}s describe fixed sequences of data with 
unrelated types.  A little further up the description, there is
a slightly more complex \kw{Pstruct}:  \texttt{OBO\_stanza\_tag}.
This struct contains two kinds of fields, named fields, like
before, and unnamed fields \texttt{'['} and \texttt{']'}.
These fields match literal characters in the data, in this
case square brackets.  In 
\texttt{OBO\_tag\_value\_pair}, another unnamed field involving 
a regular expression (introduced by \texttt{Pre}) matches a colon
followed by any number of spaces.  Named fields are included in
the internal representation of the data, while unnamed fields are
not.  

Both \texttt{OBO\_tag\_value\_pair} and
\texttt{OBO\_stanza\_tag} mentioned above are preceded by the
\texttt{Precord} type qualifier.  
The notion of a record varies depending upon the data encoding.  
ASCII data typically uses new-line characters to delimit 
records, binary sources tend to have fixed-width records, while 
COBOL sources usually store the length of each record before the actual data.
\pads{} supports each of these encodings of records and allows users to define
their own encodings.  By default, \pads{} assumes records are new-line terminated.  The constant \texttt{Peor} is also set to this
\texttt{e}nd-\texttt{o}f-\texttt{r}ecord marker, and may be used anywhere in a
description, as we did in the
type \texttt{OBO\_tag\_value\_pair}.
\texttt{Precords} have error-recovery semantics -- if errors 
in the data cause the parser to become seriously confused,
it will attempt to recover to a record boundary.
In practice, we have found this to be a very robust recovery mechanism
for ad hoc data.  

The \texttt{OBO\_stanza} structure 
illustrates the use of some simple semantic constraints.
For instance, following the \texttt{id} field is the constraint
\texttt{hastag(id,\"id\")}, which is defined earlier in the file
as a function in defined in C.  In general, such constraints should
be pure (have no externally visible effects), but otherwise may be
arbitrary C expressions.  Notice also that the first argument to the constraint
is \texttt{id}, the name of the current field.  More generally,
constraints may depend upon any previously parsed data as constraints
may reference any previous field in the current struct and type 
parameters, not shown here (except in the case of base types), 
may be used to pass information from
one portion of the description to another.   
The second semantic constraint in \texttt{OBO\_stanza} is
\texttt{Pterm(Peor)}, which is a built-in constraint for 
controlling array termination.  In this case, it states that
the array terminates when it sees an end-of-record marker 
\texttt{Peor} following any array element.  
In general, \pads{} provides a rich collection
of array-termination conditions: reaching a maximum size, finding a terminating
literal 
or satisfying a user-supplied predicate over the already-parsed portion of 
the \kw{Parray}.  \pads{} also has convenient syntax for 
specifying separator symbols that
appear between elements of an array and declaring inter-element
constraints including sorting.

One last feature of note in this description is the \texttt{Penum}
\texttt{OBO\_stanza\_type}.  \texttt{Penum}s specify that
the data must be one of several fixed strings, in this case, 
either \texttt{Term} or \texttt{Typeref}.  \pads{} also admits
\texttt{Punion}s of several forms for more general sorts of alternatives
in data formats.

Other examples of \pads{} descriptions and an online demo may be
found at \url{www.padsproj.org}.

\begin{figure}[t]

\input{go3}
%\input{dibbler_new}

\caption{Abbreviated \pads{} description for Gene Ontology data.}
\label{figure:dibbler}
\end{figure}


% Returning to the CLF description in \figref{figure:wsl}, the \kw{Penum} type \cd{method_t} describes
% a collection of data literals.  During parsing, \pads{} interprets these
% constants using the ambient character encoding.  The \kw{Ptypedef} 
% \cd{response_t} describes possible server response codes in CLF data by adding
% the constraint that the three-digit integer must be between 100 and 600.

% \setlength{\floatsep}{0pt}
% \setlength{\dblfloatsep}{0pt}
% \setcounter{totalnumber}{4}
% \setcounter{dbltopnumber}{2}
% \begin{figure*}[t!]
% \begin{small}
% \begin{center}
% \begin{code}
% \input{ai.paper}
% \end{code}
% \vskip -2ex
% \caption{Tiny example of web server log data.}
% \label{figure:clf-records}
% \end{center}
% \end{small}
% \end{figure*}


% \begin{figure}
% \input{wsl}
% \caption{\pads{} description for web server log data.}
% \label{figure:wsl}
% \end{figure}

\subsection{Overview of Planned Research}
\label{ssec:sow}

Our central research agenda is divided into three broad subsections,
which we will discuss here; our broader impacts will be discussed in the next 
section.  First, our experience with real-world data has revealed that
the current \pads{}\ system is unable to process many common
data formats.  We propose a number of extensions to \pads{} 
that will greatly improve its expressive power and enable
us to handle many more important ad hoc formats.  Second,
while \pads{}\ can automatically generate tools for providing
statistical data summaries, querying ad hoc data and generating
\xml, the tool generation process is extremely brittle, inflexible
and results in low-performance software.  We propose to redesign
the automatic tool generation process using a principled methodology
based upon a novel \pads{}-attribute system that can communicate
semantic information across tools.  Third, there is no formal
basis for the \pads{} description language and compiler.  We propose to
formalize the language and prove critical \pads{} correctness
properties.  Overall, our research combines novel language
design, high-performance systems engineering and theoretical analysis,
all aimed at solving crucial data processing problems.

\subsubsection{Towards a Universal Data Description System}

An important part of our research involves investigating as many
different ad hoc data formats that are used in practice as possible.
These practical examples highlight deficiencies in our current tools
and suggest many ways in which they need to be improved.  In this
section, we discuss a number of the limitations of the current \pads{}\ 
system and suggest extensions that we believe will be able to rectify
these deficiencies.  In all cases, our ideas for extensions
are driven directly by our experience dealing with real-world data.

\paragraph*{Data Transformation, Abstraction and Error-correction}
Many data sources require simple transformations of various kinds
immediately before or after parsing.  On output from a system,
inverse transformations are applied after or before printing.  
Examples of pre-parsing routines include decryption for
security-sensitive data and decompression for high-volume scientific data.  
The natural inverses for printing routines are encryption and compression.
After parsing, many data sources require, or at least benefit from,
a variety simple data transformations.  For example,
sometimes poorly-designed ad hoc data, a remarkable amount of the data
we have seen in practice, will have multiple different representations
of the same concept -- dates in different formats, several different strings
to represent ``no value'' etc.  Simple transform can be used to convert
these representations into a canonical form that facilitates down-stream
processing.  As another example, many data sources have privacy-sensitive parts
that should be ``sanitized'' in some way or another, possibly by scrubbing or
filtering data fields before passing them to down-stream applications.
Ironically, in October 2005, we asked the Princeton Computer Science
Department Technical Staff for access to Web Logs for some experiments
with \pads, but they refused until they had written scripts to
sanitize the data for us.  Medical data is another example of highly
privacy-sensitive data.
Lastly, almost all ad hoc data may contain errors, but sometimes there will be simple data-specific heuristics such as substitution of default values
that can be used to fix the most common problems.

It is currently impossible to code up multi-stage processing and transformation
directly in PADS.  One solution
to this problem might be to let auxiliary passes through the data
remain outside the PADS system as prepasses or postpasses.  
Unfortunately, this solution is completely unsatisfactory for a
number of reasons.  First, doing so
will often leave us in a situation in which the \pads{} description 
is not a self-contained
definition of the data format in question.  Consequently some of the value
of \pads{} as documentation is lost.  
Second, programmers must do
more work themselves to produce pads applications.  They cannot
simply run our automatic generators and receive a well-packaged 
query engine or statistical analyzer for the raw data.
Moreover, when coding transformation directly in C, the host language
for \pads{}, they must program at a much lower level of abstraction
than we might provide by supplying specialized domain-specific
transformers directly in \pads{}
Since our goal is to maximize the productivity of scientists 
who use ad hoc data, ease-of-use is a key constraint.
Finally, some data formats and
tasks are not well-suited for implementation as a separate pre- or post-pass.
For example, some format use non-uniform compression or encryption 
schemes~\cite{korn+:delta,korn+:data-format}.
Moreover, tasks such as data sanitization and error correction may be
data-dependent and directed by the structure of the data.  In other words,
they may involve just the sort of data analysis that PADS was built for
and should be integrated directly into the specification mechanism.

To address these difficulties,
we plan to research mechanisms that facilitate multi-stage data processing 
directly in PADS.  Since any PADS description must be able to generate 
both data
input tools {\em and} data output tools, we currently believe that each
data processing stage, or layer, should be specified as a pair of
transformations.  For instance, if data is decompressed on the way in,
it must be compressed on the way out.  If a field of the data is ``sanitized''
or filtered on the way in, perhaps a default value of the correct form must
be written back out to preserve the syntactic structure of the data format.
To achieve this functionality, we will begin by considering a
transformation specification with the following general form: 

\begin{code}
Ptransform \{
  i,o :  Tphys <-> Tlog
\}
\end{code}

Here, {\tt Tphys} is the PADS type of the external or {\tt phys}ical
representation and {\tt Tlog} is the PADS type of the internal or 
{\tt log}ical representation.  The internal representation of the current
phase may in turn serve as the external representation for the next
phase of the transformation.  The functions {\tt i} and {\tt o} are
user-defined functions that transform data from {\tt Tphys} to 
{\tt Tlog} and vice versa.  For example, {\tt i} may implement
decompression and {\tt o} may implement compression.

We intend to add these transformations as first-class
descriptions/types to the system.  In other words, these
transformation may be nested inside or otherwise composed with any
other form of PADS description.  When so nested, the transformation
will only apply to the appropriate specific subcomponent of the
format.  Therefore, transformations will be useful for simple
subcomponent error correction or representation casting as well as
full data source transformations such as decompression.  In addition,
transformations with compatible types will be composable.  For
example, if a data format is compressed and potentially contains
errors, a decompression transform may be composed with an
error-correction transform.

The meat of certain transformations such as compression and encryption
are probably best written as ordinary program code that is
subsequently included in the PADS description.  However, for smaller
scale, local transformations, we will investigate adding domain-specific
programming support.  This support would allow programmers to write
transforms quickly at an easy-to-understand and high level
abstraction.  It would also ensure that the output transformations are
proper inverses of the inputs.  In particular, inspired by recent work
by Foster et al.~\cite{foster+:lens}, we will explore how to develop a
library of {\em combinators}, simple composable functions, that can be
combined in a myriad of ways to produce the transforms {\em and their
inverses} at the same time.  Foster et al. used such transforms to
solve the view-update problem on error-free tree-shaped data.  The
view-update problem arises when one attempts to synchronize two
different data sources.  While we will exploit some of the
ideas behind their bi-directional transformations, our application and
context are different: we are parsing, transforming and printing ad
hoc data.  As we do so, we are specifically interested in uncovering,
representing and handling error-filled data.  One of the critical
challenges for us will be to design the combinators so that they deal
with \pads{} internal error representations effectively.


\paragraph*{Integrating Multiple Data Repositories}
Sometimes, a single logical data source is represented
as several distinct, concrete repositories.   This is the case
in the GO data source, where data is split into four disjoint
files: a molecular function file, a biological process file,
a cellular component file and a term definitions file.
The current \pads{} implementation is unable to process
these distinct repositories as a single, coherent data source.

% begin BS

We believe that the right way to describe such data sources
is to introduce a notion of {\em data module} into \pads{}.
In this case, a data source as a whole may link
a series of data modules together.  One of the goals
of this design would be to allow enough flexibility that
the whole data source could be processed together or a single module
(such as for molecular function module) could be processed on its own.
However, it remains unclear if this is indeed the right processing model;
more research and experimentation is necessary to determine the
pros and cons of such a set up.

% end BS

\paragraph*{Recursive Data}
At its core, PADS currently defines three main different sorts of type 
constructors: 
records, unions and arrays.  However, many data sources have a recursive
structure that cannot be captured with these basic types.  We are
particularly familiar with Gene Ontology (GO) Project~\cite{geneontology},
which is a data repository used by Olga Troyanskaya
(Princeton Lewis-Sigler Institute for Integrative Genomics) 
and other biologists who study protein-protein
interactions.  One component of the GO data repository classifies reactions by placing them
in a tree-structured hierarchy.   We believe that part of the solution
to describing GO's ad hoc data format is to describe the hierarchy
using recursive types.

There are several challenges involved with adding recursive types to the 
PADS infrastructure.  First, we need to analyze recursive type definitions
to ensure they
have the appropriate {\em contractiveness} property so that we can
generate well-formed internal data structures to represent the result of
a parse.  Second,
we need to transform the current ``descent'' parser into
a ``recursive descent'' parser.  Third, the PADS parser is 
parameterized by user-supplied {\em masks} that tell the parser
to turn off or on certain semantic checks.  We believe that
masks for recursive data must themselves be recursive or cyclic
data structures, though we must do more research to determine the
optimal solution.  Finally, PADS parsers output parse descriptors
that track data errors and statistical summaries of the data values
parsed.  We need to investigate the form these auxiliary
data structures should take when the data parsed is recursive. 
It is necessary balance system performance with accuracy
and consequently, the choice of parse descriptor and summary 
format is far from obvious.

\paragraph*{Data with Pointers}
Some data formats contain explicit embedded pointers 
from one part of the data source to another.
For example, part of the DNS packet format compresses
host names by using pointers to point back to common
hostname suffixes.  The GO data source
mentioned above also contains pointers as the
hierarchy that describes gene product interactions
is not a tree, but rather a DAG.  

PADS currently provides no support for processing these pointers.
Ideally, PADS should generate internal
data structures with pointers that point from one part
of the internal structure to another.  This way the backend application
will be able to manipulate the data in a much more convenient fashion.
We plan to explore the language design space here and to determine
the most effective way to specify and automatically process data
with pointers.  

\paragraph*{Specification Reuse}
Many specifications contain numerous repeated subcomponents.  For
example, in the common format for web server logs~\cite{wpp}, optional
values are represented as either a single dash (if the item does not
appear) or the item itself (if it does appear).  Currently, for every
different sort of optional value (\ie ``dash or integer'' vs. ``dash
or string''), one must write down a separate specification.  It should
be possible to eliminate this sort of redundancy and reduce the size
of format specifications using polymorphic types (types parameterized
by other types).  We plan to investigate how to integrate
polymorphism into PADS specifications and our implementation.
As is the case with recursive types, there are bound to be
a number of language design and implementation issues to deal with.


\subsubsection{Automatic Tool Generation}

The current \pads{}\ system can automatically generate tools for
creating statistical summaries of data sources, querying
data sources and outputting data in ``hoc'' formats such as
\xml.  Unfortunately, the tool generation process is quite brittle and the
performance of the generated artifacts can be substantially improved.  
In this section, we propose several innovative ways of improving our
automatic tool generation infrastructure.

\paragraph*{Attribute-based Tool Generation}
The primary reason that automatic tool generation is brittle
is that the tools assume that the data passed to them fits a generic
shape: it must be an optional {\em header} followed by
a {\em body} that consists of a sequence of records 
terminated by the end of source.  
When the data does not fit the assumed shape, automatic
tool generation cannot be used.  Moreover, in order to specify 
the header and body sections, a programmer must write a complicated series of
C macros that specialize the tool.  This process is obviously highly 
error-prone and extremely inflexible.  We need a more flexible 
tool generation paradigm that allows data analysts to manipulate
any data format they might come across, and also to
analyze any subparts of a data set they deem important.

We propose to develop a more principled and substantially more robust
approach to automatic tool generation by extending \pads{}\ with a
high-level attribute system.  {\em Attributes} are tokens that may be
attached to \pads{}\ specifications and that communicate semantic
information to tools such as the statistical analyzer and the query
engine.  For example, if a data analyst wanted to use the statistical
summary tool, he or she might attach the {\cd{summary}} attribute to
the specific parts of a pads description that describe data that must
be summarized.  Likewise, to generate \xml{}\ from a portion of the
data, the analyst might attach the {\cd{xml}} attribute to the
appropriate component of the \pads{} description.  In addition to
enabling more robust tool generation, attributes will make our
generated tools much more flexible.  Analysts will be able to use
attributes to select portions of the data that they wish to analyze or
manipulate.

In order to make this new attribute-based tool generation system work,
we must do considerable research in language design and we must
re-architect some of the current tool base.  The language design is
nontrivial because attribute specifications must be separated from the
basic data format description.  The attributes need to the separated
because they change frequently and depend upon the current needs of
the data analyst, whereas the basic data format description is
independent of any given tool or any given analyst, and consequently
persists indefinitely.

\paragraph*{Exploiting Semantic Constraints}
In addition to being brittle, the current \pads{}\ tools suffer from
performance penalties because there is no automatic way to communicate
semantic information from the data source to the tool.  The query
engine tool, in particular, could benefit tremendously from semantic
information such as the property that a given field acts as a {\em
key} for a certain record (\ie\  each record of this type in the data
source has a unique key field) or the property that an array of
records is {\em sorted}.

We plan to extend the basic attribute system described above with
user-defined attributes that are associated with semantic constraints
such as the \cd{key} attribute or the \cd{sorted} attribute.  When we
generate tools from a \pads{} description and associated attributes,
the generated parser will check the semantic constraints as it reads
new data.  On the other hand, a downstream tool such as the query
engine will be able to assume that the semantic constraints hold
(provided the parse descriptor indicates the data is error-free) and
it will be able to exploit this knowledge to optimize its query plan.
Overall, we believe attributes with semantic constraints will provide
an efficient and robust means of communicating information between
tools.  However, once again, we must do much more research to develop
the right language design and effective tool interfaces.

% mention FAX here?

\paragraph*{Application-specific, Compile-time Specialization}
For any ad hoc data format, there should be a single \pads{} description
to describe the data format including all appropriate
semantic constraints.  However, different applications
may use the data in entirely different ways.  Consequently,
it can be extremely advantageous to specialize the basic
parser to an individual application.  We have discovered
four important ways to specialize generated \pads{} parsers
and hence the generated tools as well.

\begin{enumerate}
\item Some data sources are quite irregular and have much redundancy.
It is common to want to ``clean up'' a data source by
normalizing it in some way or another.  For instance,
IP backbone data from the Regulus project at AT\&T has
multiple different formats that represent  ``missing values.''
For statistical summaries and querying,
it is highly desirable to take the multiple missing value representations
and convert them all to a single representation.
\item For performance reasons, when processing high-volume data,
it is sometimes necessary to turn off semantic checks.  
Currently, this is possible with \pads{} masks, but the masks
are checked over and over again at run time.  We believe it is
possible to partially evaluate the masks at compile time
and achieve performance improvements.
\item Different applications deal with errors in different ways. 
Sometimes a single error in a data fragment is sufficient
reason to skip to the end of the fragment; other times,
the parser can attempt to recover.  Sometimes, there are
application-specific ways to fix errors as they are recognized.
Allowing the user to control when and how to perform
error recovery can result in both
higher performance and better parsing.
\item Some applications only need a small fragment of the information
available in the totality of a data source.  These applications can
skip large portions of the data source and we conjecture specializing
our tools for these applications will allow us to achieve substantial
performance improvements.
\end{enumerate}

We propose to investigate how we might develop an effective mechanism
that will allow us to specialize the \pads-generated parsers 
with this sort of application-specific behavior.  In some cases,
we believe that it may be possible to use some ideas from the 
literature on partial 
evaluation to accomplish our goal, but we must do much more
research to determine the best way to solve these problems.
We also believe that there is a further role for research
in language design here as we would like to develop
a clear, concise and general notation for specifying
how and where to specialize a \pads-generated parser
and tool suite. 

%\label{subsec:general}

% We start by showing in \figref{figure:dibbler-filter} a simple use of 
% the core library to clean and normalize \dibbler{} data. After initializing
% the \pads{} library handle and opening the data source, the code sets
% the mask to check all conditions in the \dibbler{} description except the
% sorting of the timestamps.  We have omitted from the figure the code to read and write the header. 
% The code then echoes error records to one file and cleaned ones to another.
% The raw data has two different representations of unavailable phone numbers:
% simply omitting the number altogether, which corresponds to the \cd{NONE}
% branch of the \kw{Popt}, or having the value \cd{0} in the data.  
% The function \cd{cnvPhoneNumbers} unifies these two representations 
% by converting the zeroes into \cd{NONE}s.  The function \cd{entry_t_verify}
% ensures that our computation hasn't broken any of the semantic properties
% of the in-memory representation of the data.
% \begin{figure}[t]
% \begin{small}
% \begin{center}
% \input{dibbler_filter}
% \caption{Code fragment to filter and normalize \dibbler{} data.}
% \label{figure:dibbler-filter}
% \end{center}
% \end{small}
% \end{figure}

%\paragraph*{Transforming Data from One Ad Hoc Format to Another}

\subsubsection{Formalization and Semantic Analysis}

Currently, \pads{} has no formal description or semantics. 
In other words, there is no implementation-independent specification
of what a \pads{} description means and there is no way to
formally guarantee that a parse descriptor accurately describes the
error structure of the result.
We propose to ameliorate this situation by
formalizing the core features of the current language
and analyzing the semantics of our extensions as we develop them.
%We believe the formalization will provide another dimension along which we can
%evaluate our tools, will help us find bugs in the implementation and 
%may suggest ways to generalize or streamline the \pads{} system.

We believe that the best way to model \pads{} is as a dependent
type theory with dependent record types ($\Sigma x{:}\tau.\tau'$) to model
\pads{} structures, set types ($\{x{:}\tau \; | \; P(x) \}$) to model
semantic constraints, sum types to model unions and type functions
($\lambda x{:}\tau.\tau'$) to model parameterized \pads{}\ types.  Further
type constructors will also be necessary to model arrays, switched unions,
and literals.  As we add features such as recursive types and 
polymorphic types, we will need to extend the type theory to include
type variables as well.

In order to give a semantics to the \pads{} compiler, we will explore using
a denotational semantics in which {\em types} are interpreted 
{\em as data processors}:

\[
\lsem \tau \rsem = f
\]

\noindent
where the data processor $f$ is a function that maps bitstreams into 
data structures in a host language.  The host language for the
implementation is C, but to simplify the semantics, we will 
develop an abstract host language based on the lambda calculus.

We have several goals for the semantics:  

\begin{enumerate}
\item By formalizing the language and then comparing the implementation to our
formal model we will be able to uncover bugs in the implementation.
Indeed, we have already made preliminary progress on the semantics and we have
found bugs in the array-processing code of the compiler.  
\item There are several corner cases in the current system 
that we are unsure how to implement.  We hope our semantics
will allow us to reevaluate our overall design
and will also suggest implementation strategies in these 
corner cases.
\item We hope to use our semantics to understand other data
processing languages such as
\packettypes{}~\cite{sigcomm00} and
\datascript{}~\cite{gpce02} and their relationship to
\pads{}.  This understanding may help us augment \pads{}\ with new
and useful features from these other systems.  For instance,
our preliminary analysis of \packettypes{}\ suggests that their
overlays can be modeled by intersection types.  By taking advantage of this 
insight, we believe it should be possible to add overlays to \pads{}\ 
in a relatively straightforward manner.
\item Perhaps most importantly, we wish to prove an important
correctness property of the \pads{}\ compiler:  When the generated parser
produces a data representation and parse descriptor, all semantic 
constraints on the data representation hold except when the parse 
descriptor signals an error.  This crucial property implies that
if a tool checks the parse descriptor and the parse descriptor
signals no error then the data representation is valid.  Consequently,
downstream tools such as the query engine can assume semantic constraints
are valid merely by checking the parse descriptor.
\end{enumerate}

\subsection{Broader Impacts}
\label{ssec:impact}

If funded, this project will have two major broad impacts.

\paragraph*{Supporting Research in the Natural Sciences}
While PADS can be used to implement applications involving
networking and telecommunications data,  we aim to make it
as general as possible so it can be used to process all
kinds of ad hoc data.  More specifically, part of our mission will be
to provide support for the data processing needs of biologists,
chemists and physicists at Princeton University first and 
the broader natural sciences community later.  
 
To this end, we have already begun to work with Olga Troyanskaya, a
computational biologist, who works in Princeton's Lewis-Sigler Institute 
for Integrative Genomics.  
As is the case with many computational biologists,
Troyanskaya analyzes genomics data that is provided to her in ad hoc
formats.  In the past, Troyanskaya and her students have spent
substantial blocks of time building parsers to collect and integrate
this data.  This wastes her valuable time, which could otherwise be
used on the challenging and important algorithmic problems for which
she is a world expert.

Troyanskaya has pointed us to several data
repositories~\cite{grid,bind,geneontology} that she and her colleagues
commonly use in their scientific investigations.  
In fact, many of our proposed extensions to \pads{}\ are motivated directly by
our investigation of these data sources.
If this research is
funded, we will be able to provide a suite of tools that Troyanskaya
and her colleagues at Princeton's Genomics Institute and the wider
genomics community will be able to use to boost their productivity
substantially.  Part of our contribution will be a series of PADS
descriptions for these formats and the analysis and querying tools we
can generate automatically from PADS.  A second important contribution
will be a visual interface built on top of PADS that allows scientists
to browse data represented in ad hoc data formats without having to
know anything about programming or parsing.  All of our software will
be freely available to academics via the Web.

\paragraph*{Undergraduate Research}
Princeton University has a very active undergraduate research program.
Princeton students usually do at least one semester of
Junior Independent Work and one semester of Senior Independent Work. 
Many students also do a year-long
Junior and Senior Theses.  We believe that PADS is an excellent
platform for undergraduate independent work and we plan to link
undergraduate independent work with our interdisciplinary effort to
produce tools for biologists.
More specifically, we will recruit undergraduates to help us
build our PADS visualization tool and to build
specific data processing tools for the biological data formats
used in Princeton's Genomics Institute.
This will introduce students both to interesting and novel
computer science research and also allow them to build useful
tools for biological applications. 

\subsection{Comparison with Other Research}
\label{ssec:related}

There are many tools for describing data formats. For example,
\textsc{ASN.1}~\cite{asn} and \textsc{ASDL}~\cite{asdl} are both
systems for declaratively describing data and then generating
libraries for manipulating that data.  In contrast to \pads{},
however, both of these systems specify the {\em logical\/} representation
and automatically generate a {\em physical\/} representation.
Although useful for many purposes, this technology does not help
process data that arrives in predetermined, ad hoc formats.

Lex and yacc-based tools generate parsers from declarative
descriptions, but they require users to write both a lexer and a
grammar and to construct the in-memory representations by hand.  In
addition, they only work for ASCII data, they do not easily
accommodate data-dependent parsing, and they do not provide auxiliary
services.

More closely related work includes \erlang{}'s bit syntax~\cite{erlang} and
the \packettypes{}~\cite{sigcomm00} and
\datascript{} languages~\cite{gpce02}, 
all of which allow declarative descriptions of physical data.  These projects were motivated by parsing protocols,
\textsc{TCP/IP} packets, and \java{} jar-files, respectively.  Like
\pads{}, these languages have a type-directed approach to
describing ad hoc data and permit the user to define semantic constraints.
In contrast to our
work, these systems handle only binary data and assume the data is
error-free or halt parsing if an error is detected. 
Parsing non-binary data poses additional challenges because of the need
to handle delimiter values and to express richer termination conditions
on sequences of data. These systems also
focus exclusively on the parsing/printing problem, whereas we have 
leveraged the declarative nature of
our data descriptions to build additional useful tools.


Recently, a standardization effort has been started whose stated goals are quite similar to those of the \pads{} project~\cite{dfdl}. The description
language seems to be \xml{} based, but at the moment, more details are 
not available.

\subsection{Results from Prior NSF Support}
\label{ssec:results}

\paragraph*{David Walker, PI} (NSF CCR-0238328 CAREER: Programming Languages for Secure and Reliable Component Software
Systems, PI)
The goal of Walker's career award is to develop new programming language
technology that will improve the security and reliability of component software systems.
More specifically, Walker and his students have begun to develop new techniques for defining,
implementing and reasoning about run-time enforcement of program properties.
For example, he developed a rich new theory of security monitors as formal
automata that transform untrusted program behavior as they 
execute~\cite{ligatti+:edit-automata}.
This theory allows security
architects to model a variety of different sorts of run-time
enforcement mechanisms, to prove that certain mechanisms can or cannot
enforce various security properties, and to compare the power of
different classes of security monitors.    Recently, Walker 
has used the theory to prove the surprising new result that powerful run-time
program monitors can enforce certain kinds of liveness properties~\cite{ligatti+:renewal}.  
In addition, he has implemented the theory as an extension to Java and demonstrated
how to build compositional security monitors that can be applied to oblivious third-party 
software~\cite{bauer+:polymer}.

Walker's security monitoring language is a form of aspect-oriented programming language.
In order to better understand aspect-oriented technologies and their
potential impact on security, Walker formalized and proved
safe the {\em first} higher-order, strongly-typed calculus of 
aspects~\cite{walker+:aspects}.  This calculus defines
both static typing rules and the execution behavior of aspect-oriented
programs.  Consequently, it may
serve as a starting point for analysis of deeper properties of programs.
Recently, he has used the calculus to study the design of a
program analysis that determines the effect of security monitors on
the code they monitor~\cite{dantas+:harmless-advice}.   The analysis
can guarantee that a security monitor does not interfere with the 
original application, which greatly increases a user's incentive
to apply security patches.

To complement his work on run-time monitoring programs, Walker has also
developed several type systems to ensure basic type and memory safety conditions
for low-level programs.  Basic type- and memory-safety guarantees provide a foundation on which
richer security mechanisms can be implemented.  More specifically, he
has extended his earlier work on typed assembly language (TAL)~\cite{morrisett+:tal,morrisett+:journal-stal} with
logic-based type systems that can detect memory errors involving
stack-allocated data~\cite{ahmed+:stack} and heap or region-allocated
data~\cite{ahmed+:hierarchical-storage}.  In addition to studying memory safety
properties, Walker has shown how to use related type-theoretic techniques
to enforce adherence to very general software
protocols~\cite{mandelbaum+:refinements}.  

Finally, Walker's career grant allowed him to be a leader in
programming languages and security education. In 2004, he organized
a 10-day summer school on software security 
attended by over 70 participants~\cite{summerschool04}.  In 2005, he is in the process of
organizing a second summer school on high-assurance software and reliable computing.  He has also written a
chapter of a new textbook on type systems~\cite{walker:attapl}.



\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{References Cited}
{\bibliographystyle{abbrv}
 \small\bibliography{pads}
} \newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Biographical Sketch}
%\input{dpw-bio}

They are in separate files now (see cv/lastname-cv.tex).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Budget}

The budget pages (no longer necessary because of fastlane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Current and Pending Support (NSF Form 1239)}

The current-and-pending pages (no longer necessary because of fastlane). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Facilities, Equipment and Other Resources (NSF Form 1363)}

The facility page (no longer necessary because of fastlane).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Special Information and Supplementary Documentation}

We may consider asking AT\&{}T to provide a support letter.

\end{document}


