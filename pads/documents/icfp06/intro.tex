\section{Introduction}
\label{sec:intro}

To do:
\begin{itemize}
\item Update chart of data sources.
\item Convert outlines to text.
\end{itemize}

%% WHAT IS AD HOC DATA?

An {\em ad hoc data format} is any non-standard data format for which
parsing, querying, analysis, or transformation tools are not readily
available.  Despite the increasing use of standard data formats such
as \xml{}, ad hoc data sources continue to arise in numerous
industries such as finance, health care, transportation, and
telecommunications as well as in scientific domains, such as
computational biology and physics.  The absence of tools for
processing ad hoc data formats complicates the daily data-management
tasks of data analysts, who may have to cope with numerous ad
hoc formats even within a single application.  

Common characteristics of ad hoc data complicate the building of tools
to perform even basic data-processing tasks.  Documentation of ad hoc
formats, for example, is often incomplete or inaccurate, making it
difficult to define a database schema for the data or to build a
reliable data parser.  The data itself often contains numerous kinds
of errors, which can thwart standard database loaders.  Surprisingly,
few meta-language tools, such as data-description languages or parser
generators, exist to assist in management of ad hoc data.  And
although ad hoc data sources are among the richest for database and
data mining researchers, they often ignore such sources as the work
necessary to clean and vet the data is prohibitively expensive.

%% EXAMPLE SOURCES AND CHARACTERISTICS

The variety of application domains, format characteristics, sources of
errors, and volume of data makes processing ad hoc data both
interesting and challenging.  \figref{figure:data-sources} summarizes
several ad hoc sources from the networking and telecommunication
domains at AT\&T and from computational biology applications at
Princeton.  Formats include ASCII, binary, and Cobol, with both fixed
and variable-width records arranged in linear sequences and in
tree-shaped or DAG-shaped hierarchies.  Even within one format, there
can be a great deal of syntactic variability.  For example,
\figref{fig:darkstar-records1} contains two records from the
network-monitoring application.  Note that each record has different
number of fields (delimited by '$|$') and that individual fields contain
structured values (e.g., attribute-value pairs separated by '=' and
delimited by ';').

%% WHY IS PROCESSING IT HARD?
%% No control of source or target format
Unfortunately, data analysts have little control of the format of
ad hoc data at its source nor at its final destination, for
example, in a database.  The data arrives ``as is'', and the analyst
who receives it can only thank the supplier, not request a more
convenient format.  Once an analyst has the data, a common task is
converting the source format to a standard database loading format.
This transformation proceeds in three stages.  First, the analyst
writes a parser for the ad hoc format, using whatever (in)accurate
documentation may be available.  Second, he writes a program that 
detects and handles erroneous data records, selects records
of interest, and possibly normalizes records into a standard format,
for example, by reordering, removing, or transforming fields.
Unfortunately, the parsing, error handling, and transformation code is
often tightly interleaved.  This interleaving hides the knowledge of
the ad hoc format obtained by an analyst and severely limits the
parser's reuse in other applications.

\begin{figure*}
\begin{center}
\scriptsize
\begin{tabular}{@{}|l|l|l|}
\hline
\textbf{Name:} Use & Record Format (Size) 
%& Size
           & Common Errors \\ \hline\hline
\textbf{Web server logs (CLF):}           & Fixed-column ASCII & Race conditions on log entry\\ 
Measuring Web workloads  & ($\leq$12GB/week)  & Unexpected values\\ \hline
\textbf{AT\&T provisioning data (\dibbler{}):} & Variable-width ASCII & Unexpected values \\ 
Monitoring service activation  & (2.2GB/week) & Corrupted data feeds \\ \hline
\textbf{Call detail:}                   & Fixed-width binary &  Undocumented data\\
Fraud detection                         &   (\appr{}7GB/day) & \\ \hline 
\textbf{AT\&T billing data (\ningaui{}):}      & Cobol      & Unexpected values\\ 
Monitoring billing process  &  ($>$250GB/day) & Corrupted data feeds \\ \hline
\textbf{IP backbone data (\darkstar{}):}  & ASCII & Multiple representations \\
{Network Monitoring}       &  ($\ge$ 15 sources,\appr{}15 GB/day)  & of missing values \\
          & & Undocumented data \\ \hline
\textbf{Netflow:}               & Data-dependent number of & Missed packets\\ 
{Network Monitoring}  & fixed-width binary records & \\ 
                      & ($\ge$1Gigabit/second) & \\ \hline
\textbf{Gene Ontology data:}    & Variable-width ASCII & \\
Gene-gene correlations in Magic & in DAG-shaped hiearchy & \\\hline
\textbf{Newick data}              & Fixed-width ASCII & Manual entry errors \\
Immune system response simulation & in tree-shaped hierarchy 
& \\
\hline
\end{tabular}
\normalsize
\caption{Selected ad hoc data sources.}
\label{figure:data-sources}
\end{center}
\end{figure*}

\begin{figure}
  \centering
  \small
\begin{verbatim}
 2:3004092508||5001|dns1=abc.com;dns2=xyz.com|c=slow link;w=lost packets|INTERNATIONAL
 3:|3004097201|5074|dns1=bob.com;dns2=alice.com|src_addr=192.168.0.10;
 dst_addr=192.168.23.10;start_time=1234567890;end_time=1234568000;cycle_time=17412|SPECIAL
\end{verbatim}  
  \caption{Simplified network-monitoring data. Newlines 
inserted for legibility.}
  \label{fig:darkstar-records1}
\end{figure}

\cut{In addition to poor documentation and error-prone data sources, other
common characteristics of ad hoc data make basic processing tasks
challenging.}

\cut{A common phenomenon is for a
field in a data source to fall into disuse.  After a while, a new
piece of information becomes interesting, but compatibility issues
prevent data suppliers from modifying the shape of their data, so
instead they hijack the unused field, often failing to update the
documentation in the process.}

%% Sources, meaning, and handling of errors
Another challenge is the variety of errors and the
variety of application-dependent strategies for handling errors in ad
hoc data.  Some common errors, listed in \figref{figure:data-sources},
include undocumented data, corrupted data, missing data, and multiple
representations for missing values.  Some sources of errors
that we have encountered in ad hoc sources include malfunctioning
equipment, race conditions on log entry~\cite{wpp}, presence of
non-standard values to indicate ``no data available,'' human error
when entering data, and unexpected data values.  A wide range of
responses are possible when errors are detected, and they are highly
application dependent.  Possible responses range from halting processing
and alerting a human operator, to partitioning erroneous from valid
records for examination off-line, to simply discarding erroneous or
unexpected values.  One of the most challenging aspects of processing
ad hoc data is that erroneous data is often more important than
error-free data, because it may indicate, for example, that two
systems are failing to communicate.  Writing code that is reliably
\emph{error aware}, however, is difficult and tedious.

%% High-volume
The high volume of ad hoc data sources is another challenge.
~\figref{figure:data-sources} gives the volume of several sources.
AT\&T's call-detail stream, for example, contains roughly 300~million
calls per day requiring approximately 7GBs of storage space.  Although
this data is eventually archived in a database, data analysts mine it
profitably before such archiving~\cite{kdd98,kdd99}.  More
challenging, the \ningaui{} project at AT\&T accumulates billing data
at a rate of 250-300GB/day, with occasional spurts of 750GBs/day, and
netflow data arrives from Cisco routers at rates over a Gigabit per
second~\cite{gigascope}!  Such volumes require that the data be
processed without loading it into memory all at once.  Not
surprisingly, flexible error-response strategies are especially
critical with high-volume sources, so that error detection does not
halt or delay normal processing.

%% EXISTING SOLUTIONS

% Contivo, Pervasive, SAS ... We need to say something for real in the
% final paper. 
Commercial data-management products for ad hoc data address 
some of these problems, but to our knowledge, none can handle all the variability
in formats that we have encountered, nor do they support error-aware
processing of high volume sources.  Without tools adequate to the
task, analysts often write custom programs in \C{} or \perl{}.
Unfortunately, writing parsers, transformers, and printers by hand is
tedious and error-prone.  These tasks are complicated by lack of
documentation, convoluted encodings designed to save space, the need
to produce efficient code, and the need to handle errors robustly to
avoid corrupting down-stream data.  Moreover, the parser writers'
hard-won understanding of the data ends up embedded in parsing code,
making long-term maintenance difficult for the original writers and
sharing the knowledge with others nearly impossible.

%% OUR SOLUTION & CONTRIBUTIONS
\subsection{\padsmlbig{}}

% What is PADS/ML?
% \begin{itemize}
% \item \padsml{} is a declarative data description language.
% \item user describes physical format and semantic constraints.
% \item based on ML type structure. closely corresponds to rep and pd
%   type. Expression are from ML.
% \item descriptions serve as formal, precise, concise documentation
%   (compare to English).
% \item Descriptions compiled to robust parsers. talk about rep and pd
%   types here.
% \item Can leverage descriptions to generate arbitrary data analysis tools.
% \end{itemize}
Our solution to the challenges of ad hoc data processing is \padsml{},
a declarative language for describing data sources.  With \padsml{}, a
data analyst can describe both the physical format of the data and any
of its relevant semantics constraints. Such descriptions provide two
benefits to the data analyst: first, a description in \padsml{} serves
as a precise, formal, and concise documentation for the data source.
Any reader who has encountered data format descriptions written in
English (or other spoken langauges) will quickly appreciate this
benefit. The second, perhaps more widely appreciable, benefit is that,
from a description, the \padsml{} compiler can generates a large
selection of robust data processing tools.

One particularly essential tool generated from a description is robust
parser for the data source, written in the functional programming
langauage \ocaml{}.  The parser maps raw data from the data source
into two in-memory data structures: a canonical representation of the
parsed data, and a \textit{parse descriptor} (PD), a meta-data object
detailing properties of the parsing process for the paired
representation. The structure of the parse descsriptor mimics that of
the representation, with a given portion of the parse descriptor
reporting on the corresponding portion of the data representation. A
particular role of the PD is to report errors encountered in parsing a
peice of data.  This approach allows the parsers to provide the user
with a detailed profile of the errors perceived in the data source as
a first-class value, rather than as error messages printed on
\textit{stderr}.

Like the parser, the types of the representation and parse descriptor
produced by the parser are derived automatically from the data description. The
constructs of \padsml{} are based on the type structure of the ML
family of languages. Therefore, the compiler can map the description
into type declarations in ML, with little change.

% \begin{itemize}
% \item PADS/ML is based on experience in designing and implemented
%   PADS/C as well as semantics.
% \item Differs in two key respects.
% \item Supports concise and elegant descriptions via parameterized,
%   recursive datatypes.
% \item PADS/C generates tools directly (from within compiler).
% \item PADS/ML has infrastructure for tool development.
% \item Developer writes generic tool. Compiler generates description
%   specific ``specializer'' which takes generic tool and specializes it
%   to the description.
% \item 
% \end{itemize}

\padsml{} is closely related to previous work on
\pads{}~\cite{pads-pldi}, and can be seen as a natural evolution of
that work. \padsml{} is different than \pads{} in three key respects.
First, it is targeted at the ML family of languages. Second, \pads{}
types can be parameterized by expressions. \padsml{} adds the ability
to parameterize types by other types, thereby support more concise and
elegant descriptions along with code reuse.

Third, with \padsml{}, we have taken a substantially different
approach to generating tools beyond the parser. An essential benefit
of the \pads{} approach to processing is the high return-on-investment
that a user derives from writing a description of their data. To this
end, we aim to generate a broad suite of data analysis and processing
tools from a single specification. To achieve this goal, it is
critical that outside developers be able to easily add their own tools
- based on their unique expertise - to our tool suite. However, in
\pads{}, all tools are generated directly from within the compiler.
Therefore, developing a new tool generator for \pads{} is a complex
and difficult process requiring a deep understanding of the core
compiler code base.

With \padsml{}, we have created a new framework for tool generation
whereby a large class of tool generators can be developed external to
the compiler.  New tools need only match a well-specified, generic
tool interface, specified as an ML signature. Correspondingly, the
compiler generates, for each description, a tool that can take any
such generic tool and specialize it for use with the particular
description.

Contributions
\begin{itemize}
\item design of a data description language for functional programmers.
\item as well as pleasing the ICFP crowd, functional programming
  supports data processing and transformation much better than C -- it
  lifts the level of abstraction substantially.
\item better tool support than PADS/C: generic interfaces for external
  tool development and example tools
\item new features: datatypes, polymorphism
\item theory to support polymorphism. Substantially simplified theory and
meta-theory of recursive types.
\item implementation:
\item "types as modules" implementation stratgy -- exploits the strengths of FP
\item a natural, well-motivated challenge example for FP researchers
  in type-directed programming and advanced module design
\end{itemize}

% \subsection{\padsmlbig{} Architecture}


% In the next section, we will describe, in detail, the \padsml{} approach
% to data and meta data, the \padsml{}
% syntax for data description, and illustrative examples. Next, in
% Section~\ref{sec:} we will elaborate on
% \padsml{}'s support for data transformation, including design,
% syntax and some examples.  Section~\ref{sec:related-work} will discuss
% the related work. A discussion of conclusions and future work is
% included in section~\ref{sec:conclusion}.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
