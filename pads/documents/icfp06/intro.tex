\section{Introduction}
\label{sec:intro}

{\em
To do:
\begin{itemize}
\item Update chart of data sources.
\end{itemize}
}
%% WHAT IS AD HOC DATA?

An {\em ad hoc data format} is any non-standard data format for which
parsing, querying, analysis, or transformation tools are not readily
available.  Despite the increasing use of standard data formats such
as \xml{}, ad hoc data sources continue to arise in numerous
industries such as finance, health care, transportation, and
telecommunications as well as in scientific domains, such as
computational biology and physics.  The absence of tools for
processing ad hoc data formats complicates the daily data-management
tasks of data analysts, who may have to cope with numerous ad
hoc formats even within a single application.  

Common characteristics of ad hoc data complicate the building of tools
to perform even basic data-processing tasks.  Documentation of ad hoc
formats, for example, is often incomplete or inaccurate, making it
difficult to define a database schema for the data or to build a
reliable data parser.  The data itself often contains numerous kinds
of errors, which can thwart standard database loaders.  Surprisingly,
few meta-language tools, such as data-description languages or parser
generators, exist to assist in management of ad hoc data.  And
although ad hoc data sources are among the richest for database and
data mining researchers, they often ignore such sources as the work
necessary to clean and vet the data is prohibitively expensive.

%% EXAMPLE SOURCES AND CHARACTERISTICS

The variety of application domains, format characteristics, sources of
errors, and volume of data makes processing ad hoc data both
interesting and challenging.  \figref{figure:data-sources} summarizes
several ad hoc sources from the networking and telecommunication
domains at AT\&T and from computational biology applications at
Princeton.  Formats include ASCII, binary, and Cobol, with both fixed
and variable-width records arranged in linear sequences and in
tree-shaped hierarchies.  Even within one format, there
can be a great deal of syntactic variability.  For example,
\figref{fig:darkstar-records1} contains two records from the
network-monitoring application.  Note that each record has different
number of fields (delimited by '$|$') and that individual fields contain
structured values (e.g., attribute-value pairs separated by '=' and
delimited by ';').

%% WHY IS PROCESSING IT HARD?
%% No control of source or target format
Unfortunately, data analysts have little control of the format of
ad hoc data at its source nor at its final destination, for
example, in a database.  The data arrives ``as is'', and the analyst
who receives it can only thank the supplier, not request a more
convenient format.  Once an analyst has the data, a common task is
converting the source format to a standard database loading format.
This transformation proceeds in three stages.  First, the analyst
writes a parser for the ad hoc format, using whatever (in)accurate
documentation may be available.  Second, he writes a program that 
detects and handles erroneous data records, selects records
of interest, and possibly normalizes records into a standard format,
for example, by reordering, removing, or transforming fields.
Unfortunately, the parsing, error handling, and transformation code is
often tightly interleaved.  This interleaving hides the knowledge of
the ad hoc format obtained by an analyst and severely limits the
parser's reuse in other applications.

\begin{figure*}
\begin{center}
\scriptsize
\begin{tabular}{@{}|l|l|l|l|l|}
\hline
Name: Use                           & Representation    & Processing Problems \\ \hline\hline
Gene Ontology (GO)~\cite{geneontology}:                  & Variable-width    & White-space ambiguities \\
Gene Product Information 	      & ASCII records &  \\ \hline
SDSS/Reglens Data~\cite{mandelbaum+:reglens}:                & Floating point numbers, & Repeated multiplicative error \\
Weak gravitational lensing analysis   & among others & \\ \hline
Web server logs (CLF):                & Fixed-column      & Race conditions on log entry\\ 
Measuring web workloads               & ASCII records     & Unexpected values\\ \hline
AT\&T Call detail data:                          & Fixed-width       & Undocumented data\\
Phone call fraud detection            & binary records  & \\ \hline 
AT\&T billing data:                 & Cobol             &  Unexpected values\\ 
Monitoring billing process          &                   & Corrupted data feeds \\ \hline
Newick:   Immune                    & Fixed-width ASCII records & None \\ 
system response simulation          & in tree-shaped hierarchy &\\ \hline                                
OPRA:                               & Mixed binary \& ASCII records 
                                                       & 100-page informal \\
Options-market transactions         & with data-dependent unions & documentation \\ \hline
Palm PDA:                           & Mixed binary \& character & No high-level  \\
Device synchronization              & with data-dependent constraints & documentation available\\ \hline
\end{tabular}
\caption{Selected ad hoc data sources.}
\label{figure:data-sources}
\end{center}
\end{figure*}

% \begin{figure*}
% \begin{center}
% \begin{tabular}{@{}|l|l|l|}
% \hline
% \textbf{Name:} Use & Record Format (Size) 
% %& Size
%            & Common Errors \\ \hline\hline
% \textbf{Web server logs (CLF):}           & Fixed-column ASCII & Race conditions on log entry\\ 
% Measuring Web workloads  & ($\leq$12GB/week)  & Unexpected values\\ \hline
% \textbf{AT\&T provisioning data (\dibbler{}):} & Variable-width ASCII & Unexpected values \\ 
% Monitoring service activation  & (2.2GB/week) & Corrupted data feeds \\ \hline
% \textbf{Call detail:}                   & Fixed-width binary &  Undocumented data\\
% Fraud detection                         &   (\appr{}7GB/day) & \\ \hline 
% \textbf{AT\&T billing data (\ningaui{}):}      & Cobol      & Unexpected values\\ 
% Monitoring billing process  &  ($>$250GB/day) & Corrupted data feeds \\ \hline
% \textbf{IP backbone data (\darkstar{}):}  & ASCII & Multiple representations \\
% {Network Monitoring}       &  ($\ge$ 15 sources,\appr{}15 GB/day)  & of missing values \\
%           & & Undocumented data \\ \hline
% \textbf{Netflow:}               & Data-dependent number of & Missed packets\\ 
% {Network Monitoring}  & fixed-width binary records & \\ 
%                       & ($\ge$1Gigabit/second) & \\ \hline
% \textbf{Gene Ontology data:}    & Variable-width  & \\
% Gene product information & ASCII records & White-space ambiguities\\\hline
% \textbf{Newick data}              & Fixed-width ASCII & Manual entry errors \\
% Immune system response simulation & in tree-shaped hierarchy 
% & \\
% \hline
% \end{tabular}
% \normalsize
% \caption{Selected ad hoc data sources.}
% \label{figure:data-sources}
% \end{center}
% \end{figure*}

\begin{figure}
  \centering
  \small
\begin{verbatim}
 2:3004092508||5001|dns1=abc.com;dns2=xyz.com|c=slow link;w=lost packets|INTERNATIONAL
 3:|3004097201|5074|dns1=bob.com;dns2=alice.com|src_addr=192.168.0.10;
 dst_addr=192.168.23.10;start_time=1234567890;end_time=1234568000;cycle_time=17412|SPECIAL
\end{verbatim}  
  \caption{Simplified network-monitoring data. Newlines 
inserted for legibility.}
  \label{fig:darkstar-records1}
\end{figure}

\cut{In addition to poor documentation and error-prone data sources, other
common characteristics of ad hoc data make basic processing tasks
challenging.}

\cut{A common phenomenon is for a
field in a data source to fall into disuse.  After a while, a new
piece of information becomes interesting, but compatibility issues
prevent data suppliers from modifying the shape of their data, so
instead they hijack the unused field, often failing to update the
documentation in the process.}

%% Sources, meaning, and handling of errors
Another challenge is the variety of errors and the
variety of application-dependent strategies for handling errors in ad
hoc data.  Some common errors, listed in \figref{figure:data-sources},
include undocumented data, corrupted data, missing data, and multiple
representations for missing values.  Some sources of errors
that we have encountered in ad hoc sources include malfunctioning
equipment, race conditions on log entry~\cite{wpp}, presence of
non-standard values to indicate ``no data available,'' human error
when entering data, and unexpected data values.  A wide range of
responses are possible when errors are detected, and they are highly
application dependent.  Possible responses range from halting processing
and alerting a human operator, to partitioning erroneous from valid
records for examination off-line, to simply discarding erroneous or
unexpected values.  One of the most challenging aspects of processing
ad hoc data is that erroneous data is often more important than
error-free data, because it may indicate, for example, that two
systems are failing to communicate.  Writing code that reliably
handles erros, however, is difficult and tedious.

%% High-volume
The high volume of ad hoc data sources is another challenge.
~\figref{figure:data-sources} gives the volume of several sources.
AT\&T's call-detail stream, for example, contains roughly 300~million
calls per day requiring approximately 7GBs of storage space.  Although
this data is eventually archived in a database, data analysts mine it
profitably before such archiving~\cite{kdd98,kdd99}.  More
challenging, the \ningaui{} project at AT\&T accumulates billing data
at a rate of 250-300GB/day, with occasional spurts of 750GBs/day, and
netflow data arrives from Cisco routers at rates over a Gigabit per
second~\cite{gigascope}!  Such volumes require that the data be
processed without loading it into memory all at once.  Not
surprisingly, flexible error-response strategies are especially
critical with high-volume sources, so that error detection does not
halt or delay normal processing.

%% EXISTING SOLUTIONS

% Contivo, Pervasive, SAS ... We need to say something for real in the
% final paper. 
{\em Need to rewrite the next sentence, I'm just not quite sure what
  needs to be said here, if anything.}  Commercial data-management
products for ad hoc data address some of these problems, but to our
knowledge, none can handle all the variability in formats that we have
encountered, nor do they support error-aware processing of high volume
sources.  Without tools adequate to the task, analysts often write
custom programs in \C{} or \perl{}.  Unfortunately, writing parsers
this way is tedious and error-prone, complicated by the lack of
documentation, convoluted encodings designed to save space, the need
to produce efficient code, and the need to handle errors robustly to
avoid corrupting down-stream data.  Moreover, the parser writers'
hard-won understanding of the data ends up embedded in parsing code,
making long-term maintenance difficult for the original writers and
sharing the knowledge with others nearly impossible.

%% OUR SOLUTION & CONTRIBUTIONS
\subsection{\padsmlbig{}}

Our solution to the challenges of ad hoc data processing is \padsml{},
a declarative language for describing data sources. With \padsml{}, a
data analyst can describe both the physical format of the data and any
of its relevant semantics constraints. From a description, the
\padsml{} compiler can then generate a large selection of robust data
processing tools specific to that description. In addition, a
description in \padsml{} serves as precise, formal, and concise
documentation for the data source.  Any reader who has encountered
data format descriptions written in English (or other spoken
languages) will quickly appreciate this benefit. Furthermore, as
modifying a description is sufficient to modify the behaviour of the
generated tools, the descriptions consistently remain up-to-date with
respect to the data processing tools.

One particularly essential tool that \padsml{} generates from a
description is a robust parser for the data source, expressed in the
functional programming language \ocaml{}.  The parser maps raw data
from the data source into two data structures in memory: a canonical
representation of the parsed data, and a \textit{parse descriptor}
(PD), a meta-data object detailing properties of the parsing process
for the corresponding representation. The structure of the parse
descsriptor mimics that of the representation, with a given element of
the parse descriptor precisely reporting on the corresponding portion
of the data representation. A particular role of the PD is to report
errors encountered in parsing the data.  This approach provides the
user with programmatic access to a detailed profile of the parsing
errors for the data source.

% Like the parser, the types of the representation and parse descriptor
% produced by the parser are derived automatically from the data
% description. The constructs of \padsml{} are based on the type
% structure of the ML family of languages. Therefore, the compiler can
% map the description into type declarations in ML with little change.


An essential benefit of the \pads{} approach to data processing is the
high return-on-investment that a user derives from writing a
description of their data. To this end, we aim to generate a broad
suite of data analysis and processing tools from a single
specification. To achieve this goal, it is critical that outside
developers be able to easily add their own tools - based on their
unique expertise - to our tool suite.  For \padsml{}, we have created
a new framework for tool generation whereby a large class of tool
generators can be developed external to the compiler.  New tools need
only match a clearly-specified, generic interface, specified as an ML
signature. Correspondingly, the compiler generates, for each
description, a tool that can take any tool that implements the
interface and specialize it for use with the particular description.
\emph{Could use last sentence here.}

\padsml{} is closely related to previous work on
\padsc{}~\cite{pads-pldi}~\footnote{We refer to the original \pads{}
  language as \padsc{} to help the reader distinguish between \padsc{}
  and \padsml{}.}, and can be seen as a natural evolution of that
work.  \padsml{} is different than \padsc{} in three key respects.
First, it is targeted at the ML family of languages. Second, \padsml{}
adds the ability to parameterize types by other types, resulting in
more concise and elegant descriptions though code reuse. Third, with
\padsml{}, we have taken a substantially different approach to
generating tools that process the data after parsing. In \padsc{}, all
tools are generated directly from within the compiler.  Therefore,
developing a new tool generator for \padsc{} is a complex and
difficult process requiring a deep understanding of the core compiler
code base.

\emph{brief overview of other related work here?}

In summary, this work makes the following contributions:
\begin{itemize}
\item We have designed a data description language for use with
  functional programming languages. In addition to providing
  functional programmers with a data description language, we
  feel that functional programming provides better support for data
  processing and transformation compared with conventional imperative
  languages like \C{} and \java{}. Therefore, the combination of
  \padsml{} and \ocaml{} is a significant step towards a unified
  language for data description, transformation and analysis.
\item \padsml{} adds the ability to parameterize types by other types
  and supports descriptions of variance and recursion in a data source
  in a single construct (closely related to ML's algebraic datatypes).
  Together, these features allow for more concise and elegant
  descriptions than other data description languages, including
  \padsc{}.
\item We have implemented \padsml{} by translating descriptions into
  \ocaml{} modules that can be used by any \ocaml{} program. We use an
  innovative ``types as modules'' implementation strategy, in which
  \padsml{} types become modules and \padsml{} type constructors
  become functors. This design exploits the strengths of the ML module
  system to provide the user with an intuitive and convenient
  interface to the generated libraries.
\item Our ``types as modules'' implementation strategy encountered the
  limits of the \ocaml{} module system in multiple ways. It therefore
  provides a natural, well-motivated challenge example for
  functional-programming researchers in type-directed programming and
  advanced module design.
\item \padsml{} provides better support than \padsc{} for development of tool
  generators by external parties. We present a generic interface
  against which tool generators can be written and a number of
  examples generators that match the interface.
\item We extend our earlier work on the Data Description Calculus
  (\ddcold{}) to include support for type-parameterized types. We also
  substantially simplify the theory of recursive types, resulting in
  an overall simplification in the meta-theory of the extended DDC.
  Finally, we have proven, for the extended calculus, that generated
  parsers produce data representations and parse descriptors of the
  correct type.
\end{itemize}

% \subsection{\padsmlbig{} Architecture}


% In the next section, we will describe, in detail, the \padsml{} approach
% to data and meta data, the \padsml{}
% syntax for data description, and illustrative examples. Next, in
% Section~\ref{sec:} we will elaborate on
% \padsml{}'s support for data transformation, including design,
% syntax and some examples.  Section~\ref{sec:related-work} will discuss
% the related work. A discussion of conclusions and future work is
% included in section~\ref{sec:conclusion}.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
