\section{Introduction}
\label{sec:intro}

{\em
To do:
\begin{itemize}
\item update text to include description of tools generated using the framework.
\end{itemize}
}
%% WHAT IS AD HOC DATA?

An {\em ad hoc} data format is any non-standard data format for which
parsing, querying, analysis, or transformation tools are not readily
available.  Despite the existence of standard
data formats like \xml{}, ad hoc data sources are ubiquitous,
arising in industries as diverse as finance, health care,
transportation, and telecommunications as well as in scientific
domains, such as computational biology and physics.
\figref{figure:data-sources} summarizes a variety of such formats,
including ASCII, binary, and Cobol encodings, with both fixed and
variable-width records arranged in linear sequences and in tree-shaped
hierarchies.  Even a single format can exhibit a great deal of
syntactic variability.  For example, \figref{fig:darkstar-records1}
contains two records from a network-monitoring application.  Note that
each record has a different number of fields (delimited by '$|$') and
that individual fields contain structured values (\eg{},
attribute-value pairs separated by '=' and delimited by ';').

Common characteristics of ad hoc data make it difficult to perform
even basic data-processing tasks.  To start, data analysts typically
have little control over the format of the data they are given.  The
data arrives ``as is,'' and the analysts can only thank the suppliers,
not request a more convenient format.  The documentation accompanying
ad hoc data is often incomplete, inaccurate, or missing entirely,
which makes understanding the data format more difficult.  Even basic
data-management tasks, like loading the data into a database, requires
understanding the format, so that data can be parsed and converted into
formats required for loading.

Another challenge is the range of errors that occur in ad hoc data.
Common errors, listed in the third column of
\figref{figure:data-sources}, include undocumented fields, corrupted
and missing data, and multiple representations for missing values.
Sources of errors include malfunctioning equipment, race conditions on
log entry~\cite{wpp}, the presence of non-standard values to indicate
``no data available,'' and human error when entering data.  A wide
range of responses are possible when errors are detected, from halting
processing and alerting a human operator, to partitioning erroneous
from valid records for examination off-line, to repairing erroneous or
unexpected values.  Erroneous data itself is often more important than
error-free data, because it may indicate, for example, that two
systems are failing to communicate.  However, writing code that
reliably handles both error-free and erroneous data is difficult and
tedious.

\begin{figure*}
\begin{center}
\scriptsize
\begin{tabular}{@{}|l|l|l|l|l|}
\hline
Name: Use                           & Representation    & Processing Problems \\ \hline\hline
Gene Ontology (GO)~\cite{geneontology}:                  & Variable-width    & White-space ambiguities \\
Gene Product Information 	      & ASCII records &  \\ \hline
SDSS/Reglens Data~\cite{mandelbaum+:reglens}:                & Floating point numbers, & Repeated multiplicative error \\
Weak gravitational lensing analysis   & among others & \\ \hline
Web server logs (CLF):                & Fixed-column      & Race conditions on log entry\\ 
Measuring web workloads               & ASCII records     & Unexpected values\\ \hline
AT\&T Call detail data:                          & Fixed-width       & Undocumented data\\
Phone call fraud detection            & binary records  & \\ \hline 
AT\&T billing data:                 & Cobol             &  Unexpected values\\ 
Monitoring billing process          &                   & Corrupted data feeds \\ \hline
Newick:   Immune                    & Fixed-width ASCII records & None \\ 
system response simulation          & in tree-shaped hierarchy &\\ \hline                                
OPRA:                               & Mixed binary \& ASCII records 
                                                       & 100-page informal \\
Options-market transactions         & with data-dependent unions & documentation \\ \hline
Palm PDA:                           & Mixed binary \& character & No high-level  \\
Device synchronization              & with data-dependent constraints & documentation available\\ \hline
\end{tabular}
\caption{Selected ad hoc data sources.}
\label{figure:data-sources}
\end{center}
\end{figure*}

% \begin{figure*}
% \begin{center}
% \begin{tabular}{@{}|l|l|l|}
% \hline
% \textbf{Name:} Use & Record Format (Size) 
% %& Size
%            & Common Errors \\ \hline\hline
% \textbf{Web server logs (CLF):}           & Fixed-column ASCII & Race conditions on log entry\\ 
% Measuring Web workloads  & ($\leq$12GB/week)  & Unexpected values\\ \hline
% \textbf{AT\&T provisioning data (\dibbler{}):} & Variable-width ASCII & Unexpected values \\ 
% Monitoring service activation  & (2.2GB/week) & Corrupted data feeds \\ \hline
% \textbf{Call detail:}                   & Fixed-width binary &  Undocumented data\\
% Fraud detection                         &   (\appr{}7GB/day) & \\ \hline 
% \textbf{AT\&T billing data (\ningaui{}):}      & Cobol      & Unexpected values\\ 
% Monitoring billing process  &  ($>$250GB/day) & Corrupted data feeds \\ \hline
% \textbf{IP backbone data (\darkstar{}):}  & ASCII & Multiple representations \\
% {Network Monitoring}       &  ($\ge$ 15 sources,\appr{}15 GB/day)  & of missing values \\
%           & & Undocumented data \\ \hline
% \textbf{Netflow:}               & Data-dependent number of & Missed packets\\ 
% {Network Monitoring}  & fixed-width binary records & \\ 
%                       & ($\ge$1Gigabit/second) & \\ \hline
% \textbf{Gene Ontology data:}    & Variable-width  & \\
% Gene product information & ASCII records & White-space ambiguities\\\hline
% \textbf{Newick data}              & Fixed-width ASCII & Manual entry errors \\
% Immune system response simulation & in tree-shaped hierarchy 
% & \\
% \hline
% \end{tabular}
% \normalsize
% \caption{Selected ad hoc data sources.}
% \label{figure:data-sources}
% \end{center}
% \end{figure*}

\begin{figure*}
  \centering
  \small
\begin{verbatim}
 2:3004092508||5001|dns1=abc.com;dns2=xyz.com|c=slow link;w=lost packets|INTERNATIONAL
 3:|3004097201|5074|dns1=bob.com;dns2=alice.com|src_addr=192.168.0.10;
 dst_addr=192.168.23.10;start_time=1234567890;end_time=1234568000;cycle_time=17412|SPECIAL
\end{verbatim}  
  \caption{Simplified network-monitoring data. We inserted the newline
    after the ';' to improve legibility.}
  \label{fig:darkstar-records1}
\end{figure*}

\cut{
Surprisingly,
few meta-language tools, such as data-description languages or parser
generators, exist to assist in management of ad hoc data.  And
although ad hoc data sources are among the richest for database and
data mining researchers, they often ignore such sources as the work
necessary to clean and vet the data is prohibitively expensive.}





%% High-volume
The high volume of ad hoc data sources is another challenge.
~\figref{figure:data-sources} gives the volume of several sources.
AT\&T's call-detail stream, for example, contains roughly 300~million
calls per day requiring approximately 7GBs of storage space.  Although
this data is eventually archived in a database, data analysts mine it
profitably before such archiving~\cite{kdd98,kdd99}.  More
challenging, the \ningaui{} project at AT\&T accumulates billing data
at a rate of 250-300GB/day, with occasional spurts of 750GBs/day, and
netflow data arrives from Cisco routers at rates over a Gigabit per
second~\cite{gigascope}!  Such volumes require that the data be
processed without loading it into memory all at once.  Not
surprisingly, flexible error-response strategies are especially
critical with high-volume sources, so that error detection does not
halt or delay normal processing.

%% EXISTING SOLUTIONS

To manage ad hoc data, analysts typically write custom programs in
\C{} or \perl{} to parse and manipulate the data. 
Unfortunately, writing such parsers is tedious and error-prone,
complicated by the lack of documentation, convoluted encodings
designed to save space, and the need to produce efficient code.
Handling errors is a no-win situation.  If analysts insert code
to detect all possible errors, then the error-related code dominates
the rest of the program. If they don't, they run the risk of
failing to detect a critical error and possibly corrupting valuable
down-stream data.  Moreover, the parser writers'
hard-won understanding of the data ends up embedded in parsing code,
making long-term maintenance difficult for the original writers and
sharing the knowledge with others nearly impossible.

\cut{
The range of application domains, 
the variability and irregularity of ad hoc formats, 
the lack of documentation,
the prevalence of errors, 
the volume of data,
and the lack of tools
make processing ad hoc data both interesting and challenging.
}

%% OUR SOLUTION & CONTRIBUTIONS
\subsection{\padsmlbig{}}

Our solution to the challenges of ad hoc data is \padsml{}, a
declarative language for describing data sources.  With \padsml{}, a
data analyst can describe both the physical format and semantic
properties of a data source.  A \padsml{} description serves as
precise, formal, and concise documentation for a data source, in
contrast to the often imprecise and informal descriptions of data
formats written in English.  From such a \padsml{} description, our
compiler generates robust data processing tools specific to the
description.  Because valuable tools are generated from a description,
an analyst is motivated to keep the description up-to-date in response
to format changes.  Hence a description serves as ``living'',
accurate, and reusable documentation of a data format.

The core tool that \padsml{} generates from a description is a robust
parser for the data source, expressed in the functional programming
language \ocaml{}.  The parser maps raw data into two data structures:
a canonical representation of the parsed data and a \textit{parse
descriptor} (PD), a meta-data object detailing properties of the
corresponding data representation.  The structure of a parse
descsriptor mimics that of the data representation, with a given
component of a parse descriptor reporting on the corresponding
component of the data representation.  One important use of parse
descriptors is to provide \padsml{} tools or user applications with
programmatic access to a detailed profile of errors detected during
parsing.

% Like the parser, the types of the representation and parse descriptor
% produced by the parser are derived automatically from the data
% description. The constructs of \padsml{} are based on the type
% structure of the ML family of languages. Therefore, the compiler can
% map the description into type declarations in ML with little change.

A key benefit of the \pads{} approach to data processing is the
high return-on-investment that users derive from writing 
descriptions of their data. To enable this return, we aim to generate
a rich collection of data analysis and processing tools from each
specification. Therefore, it is critical that outside
developers be able to easily add their own tools - based on their
expertise - to our tool suite.  For \padsml{}, we have created
a framework whereby a large class of tool
generators can be developed external to the compiler.  New tool
generators need only match a generic interface, specified as an ML
signature.  Correspondingly, the compiler generates for each
description a meta-tool that can take any tool generator that
implements the interface and specialize it for use with the particular
description.  Examples of such tools include {\em add examples}.

\padsml{} is closely related to previous work on
\padsc{}~\cite{fisher+:pads}~\footnote{We refer to the original \pads{}
  language as \padsc{} to help the reader distinguish between \padsc{}
  and \padsml{}.}, and can be seen as a natural evolution of that
work.  \padsml{} is different than \padsc{} in three key respects.
First, it is targeted at the ML family of languages. 
This change facilitates post-parsing processing, particularly data
transformations, which benefit enormously from ML's pattern matching
and generally high level of abstraction. 
Second, \padsml{}
adds the ability to parameterize types by other types, resulting in
more concise and elegant descriptions though code reuse. Third, with
\padsml{}, we have taken a substantially different approach to
generating tools. In \padsc{}, the compiler itself generates all
tools.  Therefore,
developing a new tool generator for \padsc{} is a complex and
difficult process requiring a deep understanding of the core compiler
code base.


In summary, this work makes the following contributions:
\begin{itemize}
\item We have designed a data description language for use with
  functional programming languages. In addition to providing
  functional programmers with a data description language, we
  feel that functional programming provides better support for data
  processing and transformation compared with conventional imperative
  languages like \C{} and \java{}. Therefore, the combination of
  \padsml{} and \ocaml{} is a significant step towards a unified
  language for data description, transformation, and analysis.
\item \padsml{} adds parametric polymorphism
  and supports descriptions of variance and recursion 
  in a single construct (closely related to ML's algebraic datatypes).
  Together, these features allow for more concise and elegant
  descriptions than other data description languages, including
  \padsc{}.
\item We have implemented \padsml{} by translating descriptions into
  \ocaml{} modules that can be used by any \ocaml{} program. We use a
  ``types as modules'' implementation strategy, in which
  \padsml{} types become modules and \padsml{} type constructors
  become functors. This design exploits the strengths of the ML module
  system to provide the user with an intuitive and convenient
  interface to the generated libraries.
\item Our ``types as modules'' implementation strategy encountered the
  limits of the \ocaml{} module system in multiple ways. It therefore
  provides a natural, well-motivated challenge example for
  functional-programming researchers in type-directed programming and
  advanced module design.
\item \padsml{} provides better support than \padsc{} for development of tool
  generators by external parties. We present a generic interface
  against which tool generators can be written and a number of
  examples generators that match the interface.
\item We extend our earlier work on the Data Description Calculus
  (\ddcold{})~\cite{fisher+:next700ddl} to include support for
  type-parameterized types. We also 
  substantially simplify the theory of recursive types, resulting in
  an overall simplification in the meta-theory of the extended DDC.
  Finally, we have proven for the extended calculus that generated
  parsers produce data representations and parse descriptors of the
  correct type.
\end{itemize}


\cut{
%% WHY IS PROCESSING IT HARD?
%% No control of source or target format
Once an analyst has the data, a common task is
converting the source format to a standard database loading format.
This transformation proceeds in three stages.  First, the analyst
writes a parser for the ad hoc format, using whatever (in)accurate
documentation may be available.  Second, he writes a program that 
detects and handles erroneous data records, selects records
of interest, and possibly normalizes records into a standard format,
for example, by reordering, removing, or transforming fields.
Unfortunately, the parsing, error handling, and transformation code is
often tightly interleaved.  This interleaving hides the knowledge of
the ad hoc format obtained by an analyst and severely limits the
parser's reuse in other applications.

\cut{In addition to poor documentation and error-prone data sources, other
common characteristics of ad hoc data make basic processing tasks
challenging.}

\cut{A common phenomenon is for a
field in a data source to fall into disuse.  After a while, a new
piece of information becomes interesting, but compatibility issues
prevent data suppliers from modifying the shape of their data, so
instead they hijack the unused field, often failing to update the
documentation in the process.
}

%% Sources, meaning, and handling of errors
Another challenge is the variety of errors and the
variety of application-dependent strategies for handling errors in ad
hoc data.  Some common errors, listed in \figref{figure:data-sources},
include undocumented data, corrupted data, missing data, and multiple
representations for missing values.  Some sources of errors
that we have encountered in ad hoc sources include malfunctioning
equipment, race conditions on log entry~\cite{wpp}, presence of
non-standard values to indicate ``no data available,'' human error
when entering data, and unexpected data values.  A wide range of
responses are possible when errors are detected, and they are highly
application dependent.  Possible responses range from halting processing
and alerting a human operator, to partitioning erroneous from valid
records for examination off-line, to simply discarding erroneous or
unexpected values.  One of the most challenging aspects of processing
ad hoc data is that erroneous data is often more important than
error-free data, because it may indicate, for example, that two
systems are failing to communicate.  Writing code that reliably
handles erros, however, is difficult and tedious.
}
% \subsection{\padsmlbig{} Architecture}


% In the next section, we will describe, in detail, the \padsml{} approach
% to data and meta data, the \padsml{}
% syntax for data description, and illustrative examples. Next, in
% Section~\ref{sec:} we will elaborate on
% \padsml{}'s support for data transformation, including design,
% syntax and some examples.  Section~\ref{sec:related-work} will discuss
% the related work. A discussion of conclusions and future work is
% included in section~\ref{sec:conclusion}.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
