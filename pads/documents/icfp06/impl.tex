\section{From \padsmlbig{} to \ocamlbig{}}
\label{sec:padsml-impl}

{\em
ToDo: Move detail about Traverse functor to Generic tools section.
}

We have implemented \padsml{} for use with \ocaml{}. The \padsml{}
compiler generates libraries in \ocaml{} source code that can then be
used by any \ocaml{} program. In this section, we describe the
contents of the generated libraries followed by some examples
demonstrating their use.


\subsection{Generated Libraries}
\label{sec:gen-code}

From each \padsml{} description, we generate a collection of types and
functions in \ocaml{}, including:
\begin{itemize}
\item The types of two data structures: one to contain parsed data in
  memory and the other to hold meta-data about the parsing process.
  These data structures are respectively called the
  \emph{representation} and the \emph{parse descriptor}.
\item A parsing function, which parses a data source to produce a
  representation and parse descriptor for the data.
\item A generic tool generator, based on the new tool development
  framework for \padsml{}. This framework is discussed in
  \secref{sec:gen-tool}.
\end{itemize} 

In general, the representation and parse-descriptor type definitions
are designed to closely resemble the original description.
The aim is to minimize the amount of effort a user must invest in
order to understand and use the data structures returned by the
parser.

Furthermore, the type of parse descriptor mimics the type of the
representation so that the parse descriptor can provide a parsing
report for every element of a corresponding representation. Parse
descriptors have two components: a header and a body. The header
reports on the parsing process that produced the representation. It
includes an error count that indicates the number of subcomponents
with errors; an error code that indicates the type of error, if any;
and the location of the data within the original data source. The body
of the parse descriptor contains the parse descriptors (if any) for
subcomponents of corresponding representations. The body for a
base-type parse descriptor is always of type \cd{unit}.

Below is a simple \padsml{} description of a character
and integer separated by a vertical bar.
\begin{code}\scriptsize
  \kw{ptype} Pair = Pchar * '|' * Pint\end{code} 
Here is a partial listing of the elements generated from that description.
\begin{code}\scriptsize
\kw{type} rep = Pchar.rep * Pint.rep
\kw{type} pd_body = Pchar.pd  * Pint.pd
\kw{type} pd = Pads.pd_header * pd_body

\kw{val} parse : Pads.handle -> rep * pd\end{code} 
This sample code and others that follow make use of a module
\cd{Pads} that contains types and functions that commonly occur in
generated and base-type modules. In particular, the above declarations use
\cd{Pads.pd_header}, which is the type of all parse-descriptor
headers, and \cd{Pads.handle}, which is the type of the (abstract)
handles used for data sources.
Note the close correspondence between the structure of the description
and that of the \cd{rep} and \cd{pd_body} types. In addition, we see
that the type of the parse function is defined in terms of the
\cd{rep} and \cd{pd} types.

Given the close relationship between the elements generated from a
description, it is natural to collect them together in a module. For
each named type, therefore, we generate a module with definitions like
those shown in the above example.
% For all generated modules, \cd{rep}, \cd{pd_body},\cd{pd} define the
% types of the data's representation, parse-descriptor body, and parse
% descriptor, respectively. The parsing function is named \cd{parse}.
In general, all types with base kind (i.e. those that are not
parameterized) match the following signature,
\cd{Type.S}:
\begin{code}\scriptsize
\kw{type} rep
\kw{type} pd\_body
\kw{type} pd = Pads.pd_header * pd_body

\kw{val} parse : Pads.handle -> rep * pd\end{code}

Modules, then, become the building blocks of the \padsml{} system.
Base types, too, are implementated with modules. Polymorphic types,
which map types to types, are implemented as functors from (type)
modules to (type) modules. It would even be appropriate to map
recursive types into recursive modules. Unfortunately, this approach
fails due to the limitations of the \ocaml{} implementation of
recursive modules. We would need support for inclusion of functors in
recursive modules in order to take this approach.

Given the signature \cd{Type.S} for types of base kind, we can now
show an example signature for a polymorphic type.
\begin{code}\scriptsize
\kw{ptype} (Alpha,Beta) ABPair = Alpha * '|' * Beta\end{code}
becomes
\begin{code}\scriptsize
\kw{module} ABPair (Alpha : Type.S) (Beta : Type.S) :
\kw{sig}
  \kw{type} rep = Alpha.rep * Beta.rep
  \kw{type} pd\_body = (Pads.pd_header * Alpha.pd\_body) * 
                 (Pads.pd_header * Beta.pd\_body)
  \kw{type} pd = Pads.pd_header * pd\_body

  \kw{val} parse : Pads.handle -> rep * pd
\kw{end}\end{code}

Once a description has been compiled into an \ocaml{} module, that
module can be used like any other.  More specifically, each named type
in a description file is mapped into an \ocaml{} module of the same
name.  The collection of modules is grouped together into a
single file (compilation unit) with a name corresponding to the name
of the original description file. For example, a description file
named ``foo.pml'' with three types inside results in a file ``foo.ml''
with three submodules, each corresponding to one named type.  In the
remainder of this section, we will demonstrate a number of uses of
generated modules, highlighting data processing, transformation, and
filtering.

\subsection{Example: Data Processing}
\label{sec:ex-process}

We begin with a simple example in which we process a triple of
integers. Below is their description:
\begin{code}\scriptsize
\kw{ptype} Source = Pint * '|' * Pint * '|' * Pint\end{code} Next, we
show a complete \ocaml{} program that finds the average of the three
integers. (Note that we assume that the name of the description file
is ``intTriple.pml,'' resulting in an \ocaml{} module \cd{IntTriple}.)
\begin{code}\scriptsize
\kw{open} Pads
\kw{let} ((i1,i2,i3),pd) = 
    parse_source IntTriple.Source.parse "input.txt"
\kw{let} avg = match get_pd_hdr pd with
    \{error_code = Good\} -> (i1 + i2 + i3)/3
  | _ -> 0\end{code}

In this program, we parse the triple, check that it is valid and then
average its elements. The function \cd{parse_source} takes a parsing
function for a data source and a file name in which the data is
stored, and parses the source. In order to ensure that the data is
valid, the program projects the parse descriptor header from the parse
descriptor \cd{pd} and checks that the error code is set to \cd{Good}.
This error code is defined in the \cd{Pads} module, and indicates that
the data is syntactically and semantically valid.

Notice that checking the parse descriptor of the triple is enough to
guarantee that there are no errors in any of the triple's
subcomponents. This property is generally true of all representations
and corresponding parse descriptors. That is, if the header of a parse
descriptor reports no errors, then none of its subcomponents will
report errors. In this way, we support a ``pay-as-you-go'' approach to
application error handling, as the parse descriptor for valid data
need only be consulted once, no matter the size of the corresponding
data. Only if there are errors within the structure does the user then
need to continue consulting the parse descriptor until the error is
located.

\subsection{Example: Filtering}
\label{sec:ex-filter}

An important set of tasks relating to ad hoc data are those
related to errors, including error analysis, repair, and removal.
Programmers might want to clean their data, \ie{}, filter out data
containing errors. In this case, they need only access parse
descriptors to facilitate this task.

\begin{figure}
\begin{code}\scriptsize
\kw{open} Pads
   ...
\kw{let} split_entry (entry,pd) =
   match get\_pd\_hdr pd with
     \{error_code = Good\} -> write_valid entry
   | _ => write_invalid entry\end{code}
\caption{Error filter for \dibbler{} data}
\label{fig:ex-data-clean}
\end{figure}

\figref{fig:ex-data-clean} provides a partial demonstration of
splitting a standard data source into two separate sources, one with
valid records and the other (potentially) invalid records.  The valid
entries may then be further processed or loaded into a database
without corrupting the valuable data therein.  A human might examine
the bad entries off-line to determine the cause of errors or to figure
out how to fix the corrupted entries.

We assume that functions \cd{write_valid} and \cd{write_invalid} are
defined elsewhere to write an entry to a stream of valid and invalid
entries, respectively. The \cd{split_entry} function, then, receives
an entry and its parse descriptor, and, based on the parse descriptor,
writes the entry to the appropriate stream.

\subsection{Example: Transformation}
\label{sec:ex-trans}

\begin{figure}
  \centering
  \begin{code}\scriptsize
...
\kw{ptype} Header = \{
       alarm : [ a : Puint32 | a = 2 or a = 3];
 ':';  start :  Timestamp Popt;
 '|';  clear :  Timestamp Popt;
 '|';  code: Puint32;
 '|';  src\_dns  :  Nvp("dns1");
 ';';  dest\_dns :  Nvp("dns2");
 '|';  service  : service
\}
\mbox{}
\kw{ptype} D\_alarm = \{
       header   : header;
 '|';  details  : details
 \}
\mbox{}
\kw{ptype} G\_alarm = \{
       header   : header;
 '|';  generic  : (Nvp\_a,Semicolon,Vbar) Plist
\}\end{code}
\caption{Normalized format for \darkstar{} data. All named types not
  explicitly included in this figure are unchanged from the original
  \darkstar{} description.}
\label{fig:normal-darkstar}
\end{figure}

\begin{figure}
\begin{code}\scriptsize
\kw{open} Regulus
\kw{open} RegulusNormal
\kw{module} RA = Raw\_alarm
\kw{module} DA = D\_alarm
\kw{module} GA = G\_alarm
\kw{module} Header = H

\kw{let} splitAlarm ra =
    let h = 
       \{H.alarm=ra.RA.alarm; H.start=ra.RA.start; 
         H.clear=ra.RA.clear; H.code=ra.RA.code;
         H.src\_dns=ra.RA.src\_dns; H.dest\_dns=ra.RA.dest\_dns;
         H.service=ra.RA.service\};
    in match ra with
        \{info=Details(d)\} -> 
        (Some \{DA.header = h; DA.details = d\}, None)
      | \{info=Generic(g)\} ->
        (None, Some \{GA.header = h; GA.generic = g\})    
  \end{code}
  \caption{Shredding \darkstar{} data based on the {\tt info} field.}
  \label{fig:ex-no-err-check}
\end{figure}

Once a data source has been parsed and cleaned, a natural desire is to
transform such data to make it more amenable to further analysis.  For
example, analysts often need to convert ad hoc data into a form
suitable for loading into an existing system, such as a relational
database or statistical analysis package. Desired transformations
include removing extraneous literals, inserting delimiters, dropping
or reordering fields, and normalizing the values of fields (\eg{}
converting all times into a specified time zone).

Because relational databases typically cannot store unions directly,
another important transformation is to convert data with variation
(\ie{}, datatypes) into a form that such systems can handle.
Typically, there are two choices for such a transformation.  The first
is to chop the data into a number of relational tables: one table for
each variation.  This approach is called \textit{shredding}. The
second is to create an ``uber'' table, with one ``column'' for each
field in any variation.  If a given field is not in a particular
variation, it is marked as missing. 

The description fragment in \figref{fig:normal-darkstar} and code
fragment in \figref{fig:ex-no-err-check} demonstrate shredding
\darkstar{} data with \padsml{} and \ocaml{}. We shred the data into
two different tables based on the \cd{info} field of \cd{Alarm}
records. In the process, we also reorder the fields, putting the
\texttt{service} field into the common \texttt{header}. Notice that, in
the normalized format, \cd{Alarm} has been replaced with \cd{D\_alarm}
and \cd{G_alarm}, neither of which contain any fields with variable
type.

\begin{figure}
  \centering
  \begin{code}\scriptsize
\kw{let} normalizeTimeToGMT t = 
    match t with
      \{time=t;timezone="GMT"\} => t
    | \{time=t;timezone="EST"\} => t + (5 * 60 * 60)
    | \{time=t;timezone="PST"\} => t + (8 * 60 * 60)
    | ... \end{code}
  \caption{Normalizing timestamps}
  \label{fig:ex-normalize}
\end{figure}

In \figref{fig:ex-normalize}, we show an additional example of data
transformation, where we normalize timestamp-timezone pairs into
simple timestamps in GMT time.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
