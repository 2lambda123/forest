
Modern data analysis tasks increasingly involving drawing information from
a wide variety of distributed data sources, processing that data locally
and analyzing ther results.  Consequently,
computational biologists, cosmologists, financial analysts, systems
adminstrators and distributed systems engineers are constantly building
new, ad hoc systems for gathering data from distributed sources and
archiving it locally along with provenance meta-data.  If the process
is to be robust, auxiliary subsystems are also required to monitor the
data acquisition process and to detect and catalogue errors.

As an example, consider the data
manipulated by CoMon~\cite{comon}, a system designed to monitor the
health, performance and security of PlanetLab~\cite{planetlab}.  Every
five minutes, CoMon attempts to contact each of 842 PlanetLab nodes
across 416 sites worldwide.%
%\footnote{Current data; PlanetLab membership varies over time.} %
% 
When all is well, which it never is, each node responds with
an ASCII data file in mail-header format containing information
ranging from the kernel version to the uptime to the memory usage to
the ID of the user with the greatest CPU utilization.  CoMon archives
this data in compressed form and processes the information
for display to PlanetLab users.  CoMon is an invaluable resource for
PlanetLab users who need to monitor the health and performance of their
applications or experiments.

\cut{ %%%%%% 
Almost all distributed systems have (or should have) similar
monitoring infrastructure.  Currently, the implementors of each new
distributed system usually have to build ``one-off'' monitoring tools,
which takes an enormous amount of time and expertise to do well.  A
substantial part of the difficulty comes from the diversity, quality,
and quantity of data these systems must handle.  Implementors cannot
ignore errors: they must properly handle network errors and partial
disconnects of the network.  They cannot ignore performance issues:
data must be fetched before it vanishes from remote sites and it must
be archived efficiently in ways that do not burn-out hard drives by
causing them to overheat.  In addition, new monitoring systems also
must interact with legacy devices, legacy software and legacy data,
preventing implementers from using robust off-the-shelf data management
tools built for standard formats like XML. 
}%%%%%%
% I don't think this sentence really fits here...
% XML-based tools also have
%the disadvantage of significant bloat (often 8-10 times the size of a
%more natural, even uncompressed representation) caused by using a
%generic representation.

Similar problems appear in the natural and social sciences,
including biology, physics and economics.  For example, systems such
as BioPixie~\cite{biopixie}, Grifn~\cite{grifn} and
Golem~\cite{golem}, built by computational biologists at Princeton,
routinely obtain data from a number of sources scattered
across the net.  Often, the data is archived and later analyzed or
mined for information about gene structure and regulation.
%Likewise, cosmologists access data from
%telescopes~\cite{sdss}. 
%Economists make use of the vast data
%repositories at \url{FedStats.org} amongst other sources.  
Figure~\ref{fig:exampledata} summarizes 
selected distributed ad hoc data sources.

% \vizGems: Many and varied machines \\

\begin{figure}[t]
\begin{center}
\begin{tabular}{|l|l|}
\hline
{\small {\bf Name/{\em Use}} } & {\small  {\bf Properties}} 
\\\hline\hline
{\small CoMon~\cite{comon} } & {\small  Multiple data sets} \\
{\small {\em  PlanetLab host}              } & {\small  Archiving every 5 minutes} \\
{\small {\em  monitoring}} & {\small  From evolving set of 800+ nodes} \\\hline
{\small CoBlitz~\cite{coblitz}   } & {\small  Multiple data sets } \\
{\small {\em  File transfer}        } & {\small  Archiving every 3 minutes } \\
{\small {\em  system monitoring}                                      } & {\small  From evolving set of 800+ nodes } \\\hline
{\small CoralCDN~\cite{coral}         } & {\small  Single Format } \\
{\small {\em  Log files from}  } & {\small  Periodic archiving } \\
{\small {\em  CDN monitoring}                  } & {\small  From evolving set of 250+ hosts } \\\hline
{\small AT\&T \vizGems{}                    } & {\small  Execute programs remotely to } \\
{\small {\em  Website host}                 } & {\small  collect data } \\
{\small {\em   monitoring}                   } & {\small  Varied fetch frequencies } \\\hline
{\small AT\&T \darkstar{}                   } & {\small  Diverse data sources} \\
{\small {\em  Network}       } & {\small  Archiving for future analysis } \\
{\small {\em  monitoring}	} & {\small  Per minute, hour, and day fetches} \\\hline
{\small AT\&T \ningaui{}                    } & {\small  Thousands of data sources} \\
{\small {\em  Billing auditing}        } & {\small  Archiving and error analysis} \\\hline
{\small GO DB~\cite{geneontology} } & {\small  Multiple Formats } \\
{\small {\em  Gene function info.}     } & {\small  Uploads daily, weekly, monthly } \\\hline
{\small BioGrid~\cite{biogrid}        } & {\small  XML and Tab-separated Formats } \\
{\small {\em  Curated gene and}  } & {\small  multiple data sets $\leq$ 50MB each } \\
{\small {\em  protein data}                              } & {\small  Monthly data releases } \\\hline
{\small NCBI~\cite{ncbi} } & {\small  Links to multiple bioinformatics  } \\
{\small {\em  Biotechnology info.} } & {\small  datasets} \\
\hline
\end{tabular}
\end{center}
\shrink 
\caption{Example distributed ad hoc data sources.}
\shrink 
\label{fig:exampledata}
\end{figure}

This paper presents a declarative specification language, called
\padsd{}, that allows users to
describe a collection of distributed data sources they wish to
gather and archive.  More specifically, the \padsd{} programmer will
specify: {\bf where} the data is located; {\bf when} to get the data;      
{\bf how} to obtain it; {\bf what preprocessing} the system should do when the data arrives;
{\bf what preprocessing} the system should do when the data arrives; and 
{\bf what format} the data source arrives in.

\cut{%%%%%  
\begin{itemize}
\item {\bf Where} the data is located.  
The data may be in a directory
on the current machine (perhaps written by another process), at some 
remote location, or at a collection of locations.
\item {\bf When} to get the data.  
The data may need to be fetched just 
once (right now!) or according to some schedule.
\item {\bf How} to obtain it.  
The data may be accessible through standard 
protocols such as \cd{http} or \cd{ftp} or it may be created via a
local or remote computation. 
\item {\bf What preprocessing} the system should do when the data arrives.  
The data may be compressed or encrypted;  privacy considerations may require 
the data be anonymized.
\item {\bf What format} the data source arrives in.  
The data may be
  in ASCII, binary, or EBCDIC. It may be tab- or comma-separated, or
  it may be in the kind of non-standard format that
  \pads{}~\cite{fisher+:pads,mandelbaum+:pads-ml} was designed to describe.
\end{itemize}
}%%%% 

The \padsd{} system then compiles these high-level specifications into
a collection of tools for archiving the data along with provenance metadata.
Our current tool suite includes an archiver, a printer,  a performance
monitor, an database loader, an accumulator, an alerter, a selector and an
RSS feed generator.  
\cut{ %%%%%%%%%%
\noindent
{\bf - A fetching engine} that collects distributed data on the specified 
schedule, archives it, and maintains a ``table of contents'' including
information about location of origin and time.\\
{\bf - A printer} that fetches, prints, and helps debug
specifications.\\
{\bf - A performance monitor} that measures fetch times and also helps
debug specifications. \\
{\bf - An RRD database loader} that takes the data and extracts specified 
pieces to load into an RRD database~\cite{rrdtool}.  The data is 
indexed by its arrival time and supports time-based queries. \\ 
\cut{
This information should be in section 4
For performance, as more recent data arrives, older data is discarded.}
{\bf - An accumulator} that maintains a statistical profile of the 
data and its error characteristics.  \\
\cut{This information should be in section 4
For numeric data, the system
tracks average values and standard deviations.  For other 
data, such as strings, urls, ip addresses, times, dates, and ad hoc 
enumerations, the system maintains information counts of the top $N$ most commonly occurring 
items.  For all data, the system tracks error rates and information about 
common errors.}
{\bf - An alert system} that generates alerts based on programmable 
conditions. \\
{\bf - A selector} that extracts and records specified 
subcomponents of a larger data source.\\
{\bf - An RSS feed generator} that wraps raw data in the appropriate 
headers to create an RSS feed from diverse ad hoc data sources.
}%%%%%%%%%%%%
The system can generate all of these tools from \padsd{} descriptions
and declarative tool configuration specifications.  
%Thus for common
%tasks, users can manage distributed data sources simply by writing
%high-level declarative specifications.  There
%are relatively few concepts to learn, no complex interfaces and no
%tricky boilerplate to master to initialize 
%the system or thread together tool libraries.  

\cut{
Because there is
so little ``programming'' involved, we refer to the act 
of writing simple specifications
and using pre-defined tools as the {\em off-the-shelf} 
mode of use.  However, to avoid sacrificing flexibility and to support extensibility,
\padsd{} supports two other modes of use.

The second mode is
for the {\em single-minded implementer}, who needs to build a new
application for a {\em specific} collection of distributed data
sources.  Such users need more than the built-in set of tools, 
and consequently the system provides support for
creating new tools by automatically generating libraries for fetching
data, for parsing and printing, for performing type-safe data
traversal, and for stream processing using classic functional
programming paradigms such as \cd{map}, \cd{fold} and \cd{iterate}.
These generated libraries make it straightforward to create custom tools
specific to particular data sources.  However, there is a steeper learning
curve in this mode than in off-the-shelf mode because a variety of
interfaces must be learned.  The average functional programmer may find
these interfaces relatively intuitive, but the computational scientist
who is not interested in functional programming may prefer to stick with
off-the-shelf uses. 

The third mode is for the {\em generic programmer}.  Generic
programmers may observe that they (or their colleagues) 
need to perform some task over and over again on different
data sets.  Rather than writing a program specific to a particular
data set, they use a 
separate set of interfaces supplied by the \padsd{} system to write a
single generic program to complete the task.  For example, the RRD database
loader is generic because it is possible to load data from any
specified source into the RRD tool without additional ``programming.''
The generic programming mode is the most difficult to use as it involves
learning a relatively complex set of interfaces for encoding
Generalized Algebraic Datatypes (GADTs)~\cite{xi:popl03} 
and Higher-Order Abstract Syntax (HOAS). 
These complexities are required to encode 
the dependent features of \padsd{} and to compensate for the lack of
built-in generic programming support in \ocaml{}. 
Still the reward for building generic tools is very high:
as more and more such tools are built, the life of the off-the-shelf
user becomes easier and easier.  We have already built eight useful
generic tools ourselves and will continue to build more as demand requires.
}
% The system can generate all of these tools from \padsd{} descriptions
% and declaritive tool configuration specifications.  Thus for common
% tasks, users can manage distributed data sources simply by writing
% declarative specifications.

% In addition to these standard tools, the system provides support for
% creating new tools by automatically generating libraries for fetching
% data, for parsing and printing, for performing type-safe data
% traversal, and for stream processing using classic functional
% programming paradigms such as \cd{map}, \cd{fold} and \cd{iterate}.
% The generated libraries make it straightforward to create custom tools
% specific to particular data sources.

% The system also provides support for creating new, {\em generic}
% programs, where a generic program is one that operates correctly over
% {\em any} well-specified data source.  For example, the RRD database
% loader is generic, because it is possible to load data from any
% specified source into the RRD tool without additional ``programming.''
% We used the generic support to build the various tools described earlier.

% The \padsd{} system supports three sorts of users, or perhaps more
% accurately, {\em three modes of use}.  We might term the first mode of
% use {\em quick-and-dirty}.  In this mode, users need only specify
% the relevant properties of their data, edit the simple tool
% configuration files, and watch the results pour in.  Such users do not
% have to learn programming interfaces or stitch together boilerplate
% code, which makes it very easy to get started.  On
% the other hand, the quick-and-dirty user is limited to the tools that
% others have built.   The second mode is
% for the {\em single-minded implementer}, who needs to build a new
% application for a {\em specific} collection of distributed data
% sources.  Such users need more than the built-in set of tools, 
% so they write specifications of their data sources and use the
% description-specific libraries generated by the system to implement
% their application. Since the application is specific to a particular
% collection of sources, it cannot be directly reused by others.  The
% third mode is for the {\em generic programmer}.  Generic
% programmers may observe that they (or their colleagues or fellow domain
% experts) need to perform some task over and over again on different
% data sets.  Rather than writing a program specific to a particular
% data set, they use a 
% separate set of interfaces supplied by the \padsd{} system to write a
% single generic program to complete the task.

In the remainder of this short paper, we describe our language and system
using Princeton's CoMon~\cite{comon} system as an example.
%CoMon~\cite{comon} monitors the health and status of
%PlanetLab~\cite{planetlab} by attempting to
%fetch data from each of PlanetLab's 800+ nodes every 5 minutes.  
%This data ranges from
%the node uptime to memory usage to kernel version.  
%The CoMon system takes this
%raw data and transforms it into two different forms, one of which is a
%per-node collection of statistics and the other of which is a
%per-slice ({\em i.e.,} per-application) collection of statistics.
%Researchers use CoMon to set up and diagnose their PlanetLab 
%experiments, querying the data to find lightly
%loaded nodes, nodes with drifting clocks or nodes with little
%remaining disk space or other conditions.  CoMon also monitors 
%nodes for various sorts of problems and generates reports of deviant 
%machines or user programs.  
Readers interested in further examples,
a semantics for the language as well as comprehensive related work
should refer to our technical report~\cite{zhu+:padsdtr}.


