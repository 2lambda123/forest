

An {\em ad hoc data source} is any semistructured data source for
which useful data analysis and transformation tools are not readily
available.  The data that constitutes a single, abstract source 
often comes from many different concrete, physical destinations
distributed across the Internet.  It often becomes available
over a range of times and in several, evolving formats.
Before users can extract the information they need from the data,
it must be fetched, archived locally for historical analysis, 
compressed, perhaps encrypted or anonymized, and monitored for errors 
or deviations from the norm.

Managing ad hoc data is a bane of the implementers of distributed
systems.  These systems may have hundreds or thousands of
heterogenous, distributed components.  Keeping these components
running smoothly is a continuous maintenance task of significant
complexity.  Consequently, each component in a well-designed
distributed system produces a continuous stream of log files that
measure its performance and health.  As an example, consider the data
manipulated by CoMon~\cite{comon}, a system designed to monitor the
health, performance and security of PlanetLab~\cite{planetlab}.  Every
five minutes, CoMon attempts to contact each of 842 PlanetLab nodes
across 416 sites worldwide.%
%\footnote{Current data; PlanetLab membership varies over time.} %
% 
When all is well, which it never is, each node responds with
an ASCII data file in mail-header format containing information
ranging from the kernel version to the uptime to the memory usage to
the ID of the user with the greatest CPU utilization.  CoMon archives
this data in compressed form and processes the information
for display to PlanetLab users.  CoMon is an invaluable resource for
PlanetLab users who need to monitor the health and performance of their
applications or experiments.

Almost all distributed systems have (or should have) similar
monitoring infrastructure.  Currently, the implementors of each new
distributed system usually have to build ``one-off'' monitoring tools,
which takes an enormous amount of time and expertise to do well.  A
substantial part of the difficulty comes from the diversity, quality,
and quantity of data these systems must handle.  Implementors cannot
ignore errors: they must properly handle network errors and partial
disconnects of the network.  They cannot ignore performance issues:
data must be fetched before it vanishes from remote sites and it must
be archived efficiently in ways that do not burn-out hard drives by
causing them to overheat.  In addition, new monitoring systems also
must interact with legacy devices, legacy software and legacy data,
prevening implementers from using robust off-the-shelf data management
tools built for standard formats like XML. 
% I don't think this sentence really fits here...
% XML-based tools also have
%the disadvantage of significant bloat (often 8-10 times the size of a
%more natural, even uncompressed representation) caused by using a
%generic reprensentation.

Similar problems appear in the natural and social sciences,
including biology, physics and economics.  For example, systems such
as BioPixie~\cite{biopixie}, Grifn~\cite{grifn} and
Golem~\cite{golem}, built by computational biologists at Princeton,
routinely obtain data from a number of sources scattered
across the net.  Often, the data is archived and later analyzed or
mined for information about gene structure and regulation.
%Likewise, cosmologists access data from
%telescopes~\cite{sdss}. 
%Economists make use of the vast data
%repositories at \url{FedStats.org} amongst other sources.  
Figure~\ref{fig:exampledata} summarizes 
selected distributed ad hoc data sources.


\begin{figure*}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline\hline
Name & Use & Properties 
\\\hline\hline
CoMon~\cite{comon} & PlanetLab host monitoring & Multiple data sets in mail-header formats\\
                                       && Archiving every 5 minutes \\
                                       && From evolving set of 800+ nodes \\\hline
CoBlitz~\cite{coblitz} & File transfer system monitoring & Multiple data sets \\
                                       && Archiving every 5 minutes \\
                                       && From evolving set of 800+ nodes \\\hline
CoralCDN~\cite{coral} & Log files from CDN monitoring & Single Format \\
                                       && Periodic archiving \\
                                       && From evolving set of 250+ hosts \\\hline
\vizGems{}       & Website Host Monitoring & Many and varied machines \\
                 &                         & Execute programs remotely to collect data\\
                 &                         & Varied fetch frequencies \\\hline
\darkstar{}      & AT\&T network monitoring & Diverse data sources\\
                                           && Archiving for future analysis \\
                                           && Per minute, hour, and day fetches\\\hline
\ningaui{}       & AT\&T billing auditing   & Thousands of data sources\\
                 &                          & Archiving and error analysis\\\hline
GO DB (Gene Ontology)~\cite{geneontology} & Gene Function Information & Multiple Formats \\
                                             && Uploaded in daily, weekly, monthly intervals \\\hline
BioGrid~\cite{biogrid} & Curated Gene and Protein Data & XML and Tab-separated Formats \\
          & & multiple data sets $<=$ 50MB each \\
          & & monthly data releases \\\hline
NCBI~\cite{ncbi} & National Center for Biotechnology Information & Links to multiple bioinformatics datasets \\
                                                     && and online databases\\
\hline\hline
\end{tabular}
\end{center}
\caption{Example ad hoc data sources}
\label{fig:exampledata}
\end{figure*}

This paper describes a system that facilitates the creation,
maintainance, and evolution of tools for processing ad hoc data from a
wide array of distributed data sources over varying periods of
time. The system, called \padsd{}, is a domain-specific language in
which software developers describe key aspects of the data sources
they wish to monitor, including any of the following.

\begin{itemize}
\item {\bf Where} the data is located.  The data may be in a directory
on the current machine (perhaps written by another process), at some 
remote location, or at a collection of locations.
\item {\bf When} to get the data.  The data may need to be fetched just 
once (right now!) or according to some schedule.
\item {\bf How} to obtain it.  The data may be accessible through standard 
protocols such as \cd{http} or \cd{ftp} or it may be created via a
local or remote computation. 
\item {\bf What preprocessing} the system should do when the data arrives.  The 
data may be compressed or encrypted;  privacy considerations may require 
the data be anonymized.
\item {\bf What format} the data source arrives in.  The data may be
  in ASCII, binary, or EBCDIC; It may be tab- or comma-separated, or
  it may be in the kind non-standard format that
  \pads{}~\cite{fisher+:pads,mandelbaum+:pads-ml} was designed to describe.
\end{itemize}

The \padsd{} system then compiles these high-level specifications into
a collection of programming libraries and end-to-end tools for
distributed systems monitoring.  Our current tool suite includes a
number of useful artifacts, inspired by the needs we have observed in
a variety of ad hoc monitoring systems:

\begin{itemize}
\item {\bf An archiver} that collects distributed data on the specified 
schedule, archives it, and maintains a ``table of contents.''
\item {\bf A printer} that fetches, prints, and helps debug
specifications.
\item {\bf A performance monitor} that measures fetch times and also helps
debug specifications.
\item {\bf A RRD database loader} that takes the data and extracts specified 
pieces to load into an RRD database~\cite{rrdtool}.  The data is 
indexed by its arrival time and supports time-based queries.  
\cut{
This information should be in section 4
For performance, as more recent data arrives, older data is discarded.}
\item {\bf An accumulator} that maintains a statistical profile of the 
data and its error characteristics.  
\cut{This information should be in section 4
For numeric data, the system
tracks average values and standard deviations.  For other 
data, such as strings, urls, ip addresses, times, dates, and ad hoc 
enumerations, the system maintains information counts of the top $N$ most commonly occurring 
items.  For all data, the system tracks error rates and information about 
common errors.}
\item {\bf An alert system} that generates alerts based on programmable 
conditions.
\item {\bf A selector} that extracts and records specified 
subcomponents of a larger data source.
\item {\bf An RSS feed generator} that wraps raw data in the appropriate 
headers to create an RSS feed from diverse ad hoc data sources.
\end{itemize}

The system can generate all of these tools from \padsd{} descriptions
and declarative tool configuration specifications.  Thus for common
tasks, users can manage distributed data sources simply by writing
high-level declarative specifications.  It is quick and it is easy. There
are relatively few concepts to learn, no complex interfaces and no
tricky boilerplate a programmer needs to master to initialize 
the system or thread together tool libraries.  Because it's so
easy to use, we refer to the act of writing simple specifications
and using pre-defined tools as the {\em quick-and-dirty} \padsd{} programming
mode.  However, to avoid sacrificing flexibility and to support extensibility,
\padsd{} supports two other {\em modes of use} in addition to the 
quick-and-dirty.

The second mode is
for the {\em single-minded implementer}, who needs to build a new
application for a {\em specific} collection of distributed data
sources.  Such users need more than the built-in set of tools, 
and consequently the system provides support for
creating new tools by automatically generating libraries for fetching
data, for parsing and printing, for performing type-safe data
traversal, and for stream processing using classic functional
programming paradigms such as \cd{map}, \cd{fold} and \cd{iterate}.
These generated libraries make it straightforward to create custom tools
specific to particular data sources.  However, there is a steeper learning
curve in this mode than in the quick-and-dirty because a variety of
interfaces must be learned.  The average functional programmer may find
these interfaces relatively intuitive, but the computational scientist
who is not interested in functional programming may prefer to stick with
the quick-and-dirty. 

The third mode is for the {\em generic programmer}.  Generic
programmers may observe that they (or their colleagues) 
need to perform some task over and over again on different
data sets.  Rather than writing a program specific to a particular
data set, they use a 
separate set of interfaces supplied by the \padsd{} system to write a
single generic program to complete the task.  For example, the RRD database
loader is generic, because it is possible to load data from any
specified source into the RRD tool without additional ``programming.''
The generic programming mode is the most difficult to use as it involves
learning the relatively complex set of interfaces involving encodings
of Generalized Algebraic Datatypes (GADTs) and Higher-Order Abstract Syntax 
(HOAS).  These complexities are required to encode \padsd{}'s dependent features and make up for O'Caml's lack of built-in generic programming support.
Still the reward for building generic tools is very high:
as more and more such tools are built, the life of the quick-and-dirty
hacker becomes easier and easier.  We have already built eight useful
generic tools ourselves and will continue to build more as we see fit.

% The system can generate all of these tools from \padsd{} descriptions
% and declaritive tool configuration specifications.  Thus for common
% tasks, users can manage distributed data sources simply by writing
% declarative specifications.

% In addition to these standard tools, the system provides support for
% creating new tools by automatically generating libraries for fetching
% data, for parsing and printing, for performing type-safe data
% traversal, and for stream processing using classic functional
% programming paradigms such as \cd{map}, \cd{fold} and \cd{iterate}.
% The generated libraries make it straightforward to create custom tools
% specific to particular data sources.

% The system also provides support for creating new, {\em generic}
% programs, where a generic program is one that operates correctly over
% {\em any} well-specified data source.  For example, the RRD database
% loader is generic, because it is possible to load data from any
% specified source into the RRD tool without additional ``programming.''
% We used the generic support to build the various tools described earlier.

% The \padsd{} system supports three sorts of users, or perhaps more
% accurately, {\em three modes of use}.  We might term the first mode of
% use {\em quick-and-dirty}.  In this mode, users need only specify
% the relevant properties of their data, edit the simple tool
% configuration files, and watch the results pour in.  Such users do not
% have to learn programming interfaces or stitch together boilerplate
% code, which makes it very easy to get started.  On
% the other hand, the quick-and-dirty user is limited to the tools that
% others have built.   The second mode is
% for the {\em single-minded implementer}, who needs to build a new
% application for a {\em specific} collection of distributed data
% sources.  Such users need more than the built-in set of tools, 
% so they write specifications of their data sources and use the
% description-specific libraries generated by the system to implement
% their application. Since the application is specific to a particular
% collection of sources, it cannot be directly reused by others.  The
% third mode is for the {\em generic programmer}.  Generic
% programmers may observe that they (or their colleagues or fellow domain
% experts) need to perform some task over and over again on different
% data sets.  Rather than writing a program specific to a particular
% data set, they use a 
% separate set of interfaces supplied by the \padsd{} system to write a
% single generic program to complete the task.



\paragraph*{Contributions.} To summarize, we contribute the
following:


\begin{itemize}
\item It outlines the design of a language for specifying the 
spatial, temporal and auxiliary properties of distributed ad hoc data
sources.  We are aware of no other research effort that has attempted
to design, implement or analyze such a language.

\item It provides a mechanism for automatic generation of eight different
data processing tools directly from high-level specifications.

\item It supports three modes of use:
the quick-and-dirty hacker, the single-minded implementer and the generic 
programmer, thereby optimizing both flexibility and ease-of-use.

\item It provides a formal denotational semantics for the language.

\item It reports on the implementation experience and performance.
\end{itemize}

\paragraph{Outline.}
In the remainder of the paper, we describe the two examples we will
use throughout the paper (\secref{sec:examples}), show how to describe
these data sources in \padsd{} (\secref{sec:informal}), describe
the generated tool infrastructure and its different modes of use
(\secref{sec:programming}), define a denotational semantics for the
language (\secref{sec:semantics}), discuss the implementation and
evaluate its performance (\secref{sec:implementation}), describe
related work (\secref{sec:related}), and conclude
(\secref{sec:conclusions}). 

