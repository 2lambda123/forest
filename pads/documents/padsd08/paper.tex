%\documentclass[fleqn]{article}
\documentclass[nocopyrightspace]{sigplanconf}

\usepackage{xspace,amsmath,math-cmds,
            math-envs,inference-rules,times,
            verbatim,alltt,multicol,proof,url}
\usepackage{epsfig}
\usepackage{code} 
%\setlength{\oddsidemargin}{0in}
%\setlength{\evensidemargin}{0in}
%\setlength{\textwidth}{6.5in}
%\setlength{\textheight}{8.5in}

\begin{document}
\title{Automatic Tool Generation for Monitoring Distributed Systems}

\authorinfo{
Daniel S. Dantas$^1$ \quad
Kathleen Fisher$^2$ \quad
Limin Jia$^1$ \\
Yitzhak Mandelbaum$^2$ \quad
David Walker$^1$ \quad
Kenny Q. Zhu$^1$
}{
$^1$ Princeton University \\
$^2$ AT\&T Labs Research
}{}

%% \authorinfo{Kathleen Fisher}{
%% 	   AT\&T Labs Research}
%%        {\mono{kfisher@research.att.com}}
%% \authorinfo{Yitzhak Mandelbaum}{
%% 	   AT\&T Labs Research}
%%        {\mono{yitzhakm@research.att.com}}
%% \authorinfo{David Walker}{
%% 	   Princeton University}
%%        {\mono{dpw@CS.Princeton.EDU}}
%% \authorinfo{Kenny Q. Zhu}{
%%            Princeton University}
%%        {\mono{kzhu@CS.Princeton.EDU}}

\input{definitions}

\maketitle{}

\begin{abstract}  
\input{abstract}
\end{abstract}


\section {Introduction}
\label{sec:intro}

An {\em ad hoc data source} is any semistructured data source for
which useful data analysis and transformation tools are not readily
available.  The data that constitutes a single, abstract source 
often comes from many different concrete, physical destinations
distributed across the Internet.  It may also become available
over a range of times and in several, evolving formats.
Before the users can extract the information they need from the data,
it usually must be fetched, archived locally for historical analysis, 
compressed, perhaps encrypted or anonymized, and monitored for errors 
or deviations from the norm.

Managing ad hoc data 
is a particular bane of the implementers of distributed systems.
Depending on the size, these systems may have hundreds or thousands
of heterogenous, distributed components.  Keeping all these components
up and running is an enourmous continuous maintenance task.  Consequently,
each component in a well-designed system will produce endless log files
that measure its performance and heath.  As an example, consider the data
manipulated by CoMon~\cite{comon}, a system designed to monitor the
health, performance and security of PlanetLab~\cite{planetlab}.  Every
five minutes, CoMon attempts to contact each of 842 PlanetLab nodes
across 416 sites worldwide.\footnote{Data as of writing this manuscript;
PlanetLab membership varies over time.}  
When all is running smoothly, which it
never is, each node responds by sending back an ASCII data file
in mail-header format containing information ranging from
the kernel version to the uptime to the memory usage to the id of the
user with the greatest CPU utilization.  CoMon archives this data in
compressed form and its backend processes the information for display
to PlanetLab users.  It is an invaluable resource for Planet users
who need to monitor the health and performance of their applications
or experiments.  

Almost all distributed systems have (or should have) similar sorts of
monitoring infrastructure.  Unfortunately, system implementers are
often left to hack ``one-off'' monitoring tools of their own, which
are invariably less reliable, unoptimized, insecure, and difficult or
impossible to evolve when new requirements become known.  A
substantial part of the difficulty simply comes from the diversity,
quality, and volume of data these systems must often handle. Often,
new monitoring systems also face the problem of having to interact
with legacy devices, legacy software and legacy data, leaving
implementers in a situation where they cannot use robust off-the-shelf
data management tools built for standard formats like XML.  XML-based
tools also have the disadvantage of significant bloat (often 8-10 times
the size of a more natural, even uncompressed representation) caused by
using a generic reprensentation.

Somewhat similar problems also appear
across the natural and social sciences, including biology,
physics and economics.  For example, systems such as BioPixie~\cite{biopixie}, Grifn~\cite{grifn} and Golem~\cite{golem}, built by
computational biologists at Princeton,
routinely obtain data supplied from a number of sources scattered 
across the net.  The data is often archived and later analyzed or mined 
for valuable information about gene structure and regulation.  Likewise, 
cosmologists need access to data uploaded from major telescopes~\cite{sdss}
and economists can make use of vast data repositories at FedStats.org
amongst other sources.  These and other selected ad hoc data sources 
are presented in Figure~\ref{fig:exampledata}.

\begin{figure*}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline\hline
Name & Use & Properties 
\\\hline\hline
CoMon~\cite{comon} & PlanetLab host monitoring & Multiple data sets in mail-header formats\\
                                       && Archiving every 5 minutes \\
                                       && From evolving set of ~800 nodes \\\hline
CoBlitz~\cite{coblitz} & File transfer system monitoring & Multiple data sets \\
                                       && Archiving every 5 minutes \\
                                       && From evolving set of ~800 nodes \\\hline
CoralCDN~\cite{coral} & Log files from CDN monitoring & Single Format \\
                                       && Periodic archiving \\
                                       && From evolving set of ~250 hosts \\\hline
\vizGems{}       & Website Host Monitoring & Many and varied machines \\
                 &                         & Execute programs remotely\\
                 &                         & to collect data\\\hline
\darkstar{}      & AT\&T network monitoring & Archiving for future analysis \\\hline
\ningaui{}       & AT\&T billing auditing   & Thousands of data sources\\
                 &                          & Archiving and error analysis\\\hline
GO DB (Gene Ontology)~\cite{geneontology} & Gene Function Information & Multiple Formats \\
                                             && Uploaded in daily, weekly, monthly intervals \\\hline
BioGrid~\cite{biogrid} & Curated Gene and Protein Data & XML and Tab-separated Formats \\
          & & multiple data sets $<=$ 50MB each \\
          & & monthly data releases \\\hline
NCBI~\cite{ncbi} & National Center for Biotechnology Information & Links to multiple bioinformatics datasets \\
                                                     && and online databases\\
\hline\hline
\end{tabular}
\end{center}
\caption{Example ad hoc data sources}
\label{fig:exampledata}
\end{figure*}

The purpose of our research is to develop a system that makes it easier to create,
maintain, and evolve tools for monitoring such distributed systems.  We propose to
do so by developing a domain-specific language, called \padsd, in which software developers specify
a number of key aspects of the data sources they wish to monitor including any of the
following.

\begin{itemize}
\item {\bf where} the data is located.  The data may be in some directory
on the current machine (perhaps placed there by another process) or at some remote location
or collection of locations.
\item {\bf when} to get the data.  The data may need to be fetched just once (right now!) or
according to some repeated schedule in time series indexed by minutes, days or months.
\item {\bf how} to obtain it.  The data may be accessible through standard protocols such as
http or ftp or it may be created through remote execution of a non-standard script. 
\item {\bf what preprocessing} the system should do when it arrives.  The data may be compressed
or encrypted and therefore need to be decompressed or decrypted before it can be processed.  Privacy 
considerations may require the data be anonymized in some way.
\item {\bf what format} the data source arrives in.  The data may be ASCII or binary; it may
be tab- or comma-separated.  The data may also be represented in some completely ad hoc, non standard 
format consisting of floating point numbers, integers, strings, vertical bars and curly-q's.  
\end{itemize}

These rich, high-level specifications are then compiled into a collection of programming libraries and
end-to-end tools for distributed systems monitoring.  Our current tool suite includes a number of useful
artifacts, inspired by the common needs we have observed in ad hoc monitoring systems:

\begin{itemize}
\item {\bf an archiver} that
collects distributed data on the specified schedule, archives it locally, and maintains a 
``table of contents.''
\item {\bf a database loader} that takes the data and extracts specified pieces to load into the
RRD database tool~\cite{rrdtool}.  The data is indexed by its arrival time and supports time-based
queries.  For performance, as more recent data arrives, older data is discarded.
\item {\bf an accumulator tool} that maintains a statistical profile of the data and its error characteristics.
For numeric data, information about average values and standard deviations are maintained.
For other kinds of data, such as strings, urls, ip addresses, times, dates, and ad hoc enumerations,
information counts of the top $N$ most commonly occurring items are maintained.  For all data, error rates
and information about common errors are maintained.
\item {\bf an alert system} that generates alerts based on programmable conditions.
\item {\bf a selector tool} that extracts and records specified subcomponents of a larger data source.
\item {\bf an RSS feed generator} that wraps data in the appropriate XML headers and creates a single RSS feed
from possibly multiple diverse ad hoc data sources.
\end{itemize}

In addition to these standard tools, the system provides support for
creating new tools by automatically generating a collection of
libraries.  The libraries include a run-time system for fetching data,
libraries for parsing data in a specified format and for printing data
in that format.  There is also infrastructure for type-safe data
traversal and stream processing using classic functional programming
paradigms such as map, fold and iterate.  The generated libraries make
it straightforward for programmers to create their own custom tool
specific to a single data source or collection of sources.  In
addition, there is advanced support for creating new, {\em generic}
programs, where a generic program is one that operates correctly over
{\em any} well-specified data source.  For example, the RRDtool loader
is generic, because it is possible to load data from any specified
source into the RRDtool without doing any substantial additional ``programming.''
Likewise, the alert system, selector, RSS feed generator, parsers,
printers and traversal libraries are all generic programs.

In the remainder of the paper, we will explain the design and
implementation of our system in further detail.  First, in Section~\ref{sec:related}
we explain the relationship with other research in this area.  Next, in Section~\ref{sec:examples},
we will outline two running examples we will use for expository purposes throughout the paper, one 
involving CoMon, a system built at Princeton to monitor PlanetLab, and a second involving
\ningaui{}, built at AT\&T for monitoring AT\&T's web hosting service.  After introducing the examples,
we will explain how to specify the attributes of the data sources they depend upon in 
Section~\ref{sec:informal}.  In the next section, we will explain the architecture of our compiler
and tool generation system.  Finally, in Section~\ref{sec:conclusions} we will touch upon future work
and conclude.

\section{Related Work}
\label{sec:related}
\input{related}

\section{Running Examples}
\label{sec:examples}
\input{examples.tex}

\section{\padsd{}: An Informal Introduction}
\label{sec:informal}
\input{informal.tex}

\section{\padsd{} Semantics}
\label{sec:semantics}
\input{language}

\section{\padsd{}:  Working with Feeds}
\label{sec:programming}
\input{programming}

\section{Conclusions}
\label{sec:conclusions}

\section*{Acknowledgments}

This material is based upon work 
supported by the NSF
   under grants 0612147 and 0615062.
Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the authors and do not
   necessarily reflect the views of the NSF.

\bibliographystyle{abbrv}
\bibliography{pads,vivek}

\end{document}

%%% Local Variables:
%%% mode: outline-minor
%%% End:

