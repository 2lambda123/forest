\section{Evaluation}\label{sec:eval}
We use sample files from twenty different ad hoc data sources to
evaluate our overall inference algorithm and the different
approaches to probabilistic tokenization.  
These data sources, many of which are published on the
web~\cite{padsweb}, are mostly system-generated log files of various 
kinds and a few ASCII spreadsheets describing business transactions. 
The size of these input files ranges from a few dozen lines to a few
thousand.  

To test a given tokenization approach on a particular sample file, we
first construct a statistical model using the given approach from the
other nineteen sample files.  We then use the resulting model to infer a
description for the selected file.  We repeat this process for all
three tokenization approaches (HMM, HMEM, and HSVM) and all twenty
sample files.  We use three metrics described in the following
sections to evaluate the results: {\em token accuracy},
{\em quality of description} and {\em execution time}.

\subsection{Token accuracy}
To evaluate tokenization accuracy for a model $M$ on a given sample
file, we compare the most likely sequence of tokens predicted by $M$,
denoted $S_m$, with the ideal token sequence, denoted $S$.  We define
$S$ to be the sequence of tokens generated by the hand-written \pads{}
description of the file.  We define three kinds of error rates, all
normalized by $|S|$, the total number of tokens in $S$:

\begin{eqnarray*}
\textrm{token error} & = & \frac{\textrm{number of misidentified tokens in $S_m$}}
	{|S|} \\[1ex]
\textrm{token group error} & = & \frac{\textrm{number of misidentified groups in $S_m$}}
	{|S|}\\[1ex]
\textrm{token boundary error} & = & \frac{\textrm{number of misidentified boundaries in $S_m$}}
	{|S|}
\end{eqnarray*}

\begin{table}[th]
\begin{center}
\begin{tabular}{|l||c|c|c|c||c|c|c|c||c|c|c|c|} \hline
Data source        & \multicolumn{4}{|c||}{Token Error (\%)} &
               \multicolumn{4}{|c||}{Token Group Error (\%)} &
               \multicolumn{4}{|c|}{Token Boundary Error (\%)} \\ \cline{2-13}
               & lex   & HMM   & HMEM  & HSVM & lex & HMM & HMEM &
               HSVM & lex & HMM & HMEM & HSVM \\\hline\hline
1967Transactions     & 30    & 30    & 18.93 & 18.93 & 11.07 &
               11.07 & 0     & 0     & 11.07 & 11.07 & 0     & 0     \\ \hline
ai.3000                    & 70.58 & 15.79 & 18.98 & 11.20 & 70.58 &
               14.68 & 17.26 & 10.27 & 53.53 & 12.34 & 4.79  & 4.00  \\ \hline
yum.txt                    & 29.41 & 13.33 & 21.80 & 0     & 29.41 &
               11.73 & 21.80 & 0     & 19.16 & 11.49 & 21.80 & 0     \\ \hline
rpmpkgs.txt                & 100   & 2.71  & 15.01 & 0.34  & 100   &
               2.14  & 14.67     & 0     & 98.98 & 0.23  & 14.67     & 0     \\ \hline
railroad.txt               & 67.51 & 9.47  & 6.48  & 5.58     &
67.51 &
               9.36  & 5.93  & 5.58     & 46.08 & 8.77  & 5.41  & 5.58     \\ \hline
dibbler.1000               & 15.72 & 43.40     & 11.91     & 0.00 &
15.72 & 36.78
                     & 11.91     & 0.00     & 4.54  & 13.33 & 13.15     & 0.00     \\ \hline
asl.log                    & 94.79 & 98.91 & 8.94  & 5.83  & 94.79 &
               98.91 & 8.94  & 5.83  & 83.28 & 98.54 & 6.27  & 3.29  \\ \hline
scrollkeeper.log           & 31.29 & 28.48 & 18.67 & 9.86  & 31.29 &
               18.77 & 8.96  & 0.12  & 8.96  & 17.83 & 8.96  & 0.12  \\ \hline
page\_log                  & 82.69 & 15.29 & 0     & 7.52  & 82.69 &
               15.29 & 0     & 7.52  & 64.70 & 5.64  & 0     & 5.64  \\ \hline
MER\_T01\_01.csv           & 84.56 & 23.09 & 31.32 & 15.40     &
84.49 &
               23.09 & 31.22 & 15.40     & 84.71 & 7.71  & 13.20 & 0.02 \\ \hline
crashreporter          & 59.73 & 7.91     & 4.99  & 0.19  & 59.73 &
7.91
                     & 4.96  & 0.14  & 53.35 & 7.91     & 4.92  & 0.14  \\ \hline
ls-l.txt                   & 33.73 & 18.70 & 19.96 & 6.65  & 33.73 &
               18.23 & 19.96 & 6.65  & 19.70 & 7.45  & 19.76 & 6.45  \\ \hline
windowserver\_last     & 77.32 & 14.98 & 10.16 & 3.24  & 77.32 &
               14.98 & 10.07 & 3.15  & 69.18 & 11.16 & 8.05  & 3.14  \\ \hline
netstat-an                 & 19.75 & 17.83 & 9.61  & 9.01  & 19.75 &
               15.44 & 5.95  & 5.95  & 12.51 & 14.90 & 5.80  & 5.20  \\ \hline
boot.txt                   & 34.04 & 25.40 & 9.37  & 2.77  & 34.04 &
               25.10 & 9.14  & 2.43  & 3.34  & 14.48 & 8.27  & 1.69  \\ \hline
quarterlyincome    & 83.37 & 5.52  & 1.98  & 1.98     & 83.37 &
               4.22  & 1.53  & 1.54     & 77.53 & 1.54  & 1.53  & 1.54     \\ \hline
corald.log            & 90.96 & 100   & 5.67  & 3.02     & 89.21 &
               98.25 & 3.93  & 1.27     & 81.76 & 97.80 & 1.27  & 1.27     \\ \hline
coraldnssrv.log       & 91.04 & 18.17 & 10.64 & 5.23  & 91.04 &
               18.17 & 9.33  & 5.22  & 83.07 & 14.37 & 4.11  & 3.92  \\ \hline
probed.log            & 1.74  & 27.99 & 16.50 & 16.50 & 1.74  &
               27.99 & 16.50 & 16.50 & 1.75  & 27.98 & 16.42 & 16.42 \\ \hline
coralwebsrv.log       & 86.73 & 100   & 8.75  & 23.99 & 86.74 &
               100   & 8.75  & 23.99     & 82.09 & 98.33 & 8.75  & 23.81     \\
               \hline
\end{tabular}
\caption{Tokenization errors}
\label{tab:error}
\end{center}
\end{table}
The token error rate measures the number of times a token appears in
$S$ but the same token does not appear in the same place in $S_m$.

To define the token group error rate, we must first explain token
groups.  Some tokens have feature vectors that make them difficult to
distinguish, for example, the tokens {\tt hex string} and {\tt id}
which both contain digits and alphabetic letters.  Intuitively, if the
model misclassifies a {\tt hex string} as an {\tt id}, it is doing
better than if it predicts an unrelated token, such as a {\tt date}.
Each token group collects tokens with similar feature vectures, so the
tokens {\tt hex string} and {\tt id} belong to the same token group,
but not {\tt date}.  The token group error rate then measures the
number of times the model predicts a token outside of the token group
in the corresponding position in $S$.  More concretely, it measures the number of times a
token from a particular token group appears in $S$ but no token from
the same group appears in the same location in $S_m$.  Note that by
definition the token group error rate will always be lower than the
token error rate. 

Finally, the token boundary error rate measures the number of times
there is a boundary between tokens in $S$ but no corresponding
boundary in $S_m$.  This relatively coarse measure is interesting
because boundaries are important to structure discovery. Even if the
tokens are incorrectly identified, if the boundaries are correct, the
correct structure can be still discovered.

\tblref{tab:error} lists the token error, token group error, and token
boundary error rates of the twenty benchmarks.  The {\tt lex} column
represents the results for the original \learnpads{} system.
The error rates are
high for the original system because the ideal \pads{} descriptions are
written using new tokens not available in the original system.
{\bf kenny: it might not be a good idea to include the lex columns?
ksf: It certainly does not make sense to include the numbers if they
are measuring something so different.  Is it possible to run the
original system with a tokenization file containing the new tokens?
}
For every data source except {\tt probed.log}, the 
statistical models, especially HMEM and HSVM, substantially decrease token errors.
{\bf ksf: this is meaningless unless we fix the lex column.  We should
add a more meaningful summary of the data in the table.}

\subsection{Quality of description}
We use the {\em Minimum Description Length Principle}
(MDL)~\cite{mdlbook} to quantitatively assess description quality.
This principle says that a useful measure of description quality is
the sum of the cost in bits of transmitting the description (the type
cost) and the cost in bits of transmitting the data \textit{given the
description} (the data cost).  In general, the type cost measures the
complexity of the description, while the data cost measures how
loosely a given description explains the data.  Increasing the type
cost generally reduces the data cost, and \textit{vice versa}, so the
challenge is to minimize their sum.
\tblref{tab:complexity}
shows the percentage change in the type and data costs of the 
descriptions inferred using probibalistic tokenization over the costs
of the descriptions learned using the original \learnpads{} system.

\begin{table}[th]
\begin{center}
\begin{tabular}{|l||c|c|c||c|c|c|}\hline
Data source & \multicolumn{3}{|c||}{Type Cost} &
\multicolumn{3}{|c|}{Data Cost}\\ \hline & HMM & HMEM & HSVM & HMM &
HMEM & HSVM \\ \hline 
1967Transactions & -15.99 & -27.03 & -27.03 & -2.97
& -2.80 & -62.51    \\\hline 
ai.3000 & -40.22 & +12.40 & -18.49 & +15.34 & +4.91 & -0.29 \\ \hline
yum.txt & -90.54 & +50.93 & -76.27 & +6.06 & -7.93 & -0.01  \\ \hline
rpmpkgs.txt & -40.82 & -79.12 & -91.86 & +0.73 & -0.04 & 1.47 \\ \hline
railroad.txt & -14.89 & +28.77 & -43.82 & -18.42 & -26.36 & -2.90   \\ \hline
dibbler.1000 & +155.50 & +17.83 & -74.35 & +66.13 & -22.11 & +23.72    \\ \hline
asl.log & -69.32 & +69.72 & +23.76 & +9.13 & -14.27 & -15.00 \\ \hline
scrollkeeper.log & -20.73 & -9.71 & -15.61 & +12.32 & +2.24 & +2.44  \\ \hline
page\_log & -8.46 & 0 & -8.51 & -9.42 & -11.67 & -10.21 \\ \hline
MER\_T01\_01.csv & -8.59 & -12.74 & -23.21 & -25.59 & -24.15 & +1.46 \\ \hline
crashreporter & -36.81 & +35.19 & -40.03 & +5.74 & -4.36 & -2.35 \\ \hline
ls-l.txt & -70.55 & -51.32 & -39.30 & +13.20 & -7.26 & -2.18 \\ \hline
windowserver\_last & -31.54 & +4.10 & -62.61 & +3.41 & -9.25 & -9.78  \\ \hline
netstat-an & +37.48 & -49.94 & -57.00 & -9.97 & +9.51 & +10.44 \\ \hline
boot.txt & -40.85 & -19.49 & -57.82 & +4.39 & -5.70 & -5.24 \\ \hline
quarterlyincome & -32.27 & +94.64 & +54.14 & -30.64 & -30.50 & +33.71    \\ \hline
corald.log & -40.32 & -33.86 & -5.53 & -5.71 & -29.64 & -29.81   \\ \hline
coraldnssrv.log  & -1.86 & -2.95 & -6.78 & +40.13 & +463.88 & +4.56 \\ \hline
probed.log & -14.61 & -33.48 & -33.48 & +59.53 & +63.18 & +63.18 \\ \hline
coralwebsrv.log & -15.39 & +89.54 & +45.73 & -3.07 & +53.62 & +68.92   \\
               \hline
\end{tabular}
\caption{Increase (+\%) or decrease (-\%) in type cost and data cost}
\label{tab:complexity}
\end{center}
\end{table}

In practise, better tokenization schemes usually result in simpler
descriptions (lower type costs), but looser fits (higher data costs).
\tblref{tab:complexity} 
shows that in all benchmarks except {\tt dibbler.1000}, {\tt coraldnssrv.log} and
{\tt probed.log}, at least one of the statistical models significantly
reduces the type cost while keeping the data cost stable. 
%This is not exactly consistent with the error rate
%evaluation, because even if most token sequences are correct, only a
%few wrong token sequences can cause a difference at the description
%level.

\tblref{tab:expert} shows the results of a qualitative assessment of
description quality, in which one of the authors judged how each
description compared to the original \learnpads{} results.
%Although the original \learnpads{}
%system is incapable of identifying tokens as well as \learnpads{} with
%statistical models, the structure discovery and refinement phases are
%still quite effective. 
%The new \learnpads{} system doesn't rely on the refinement rules that much. 
For benchmarks where the original system produces good descriptions, 
the new system outputs similarly good descriptions. For 
data sources for which the original system did poorly,
the new system offers improvements.
{\bf kf This claim does not seem to be supported by the data in the
  table. }

\begin{table}[th]
\begin{center}
\begin{tabular}{|l||c|c|c|}\hline
Data source & HMM & HMEM & HSVM \\ \hline 
1967Transactions & no difference & no difference & no difference   \\\hline 
ai.3000 & better & better & better \\ \hline
yum.txt & better &  better & better \\ \hline
rpmpkgs.txt & better & better & better\\ \hline
railroad.txt & slightly better & both complicated & both complicated  \\ \hline
dibbler.1000 & slightly worse & not much difference & not much difference   \\ \hline
asl.log & too concise & both complicated & both complicated \\ \hline
scrollkeeper.log  &  better & better & better \\ \hline
page\_log  & slightly worse & not much difference & not much difference \\ \hline
MER\_T01\_01.csv & too concise & not much difference & not much difference \\ \hline
crashreporter & slightly better & not much difference & better \\ \hline
ls-l.txt & better & better & better \\ \hline
windowserver\_last & slightly better & slightly better & better \\ \hline
netstat-an & better & better & better \\ \hline
boot.txt & better &  better & better \\ \hline
quarterlyincome & slightly better & slightly better & slightly better   \\ \hline
corald.log & better &  better & better \\ \hline
coraldnssrv.log  & not much difference &  not much difference & not much difference\\ \hline
probed.log & worse & not much difference & not much difference \\ \hline
coralwebsrv.log & worse & worse & worse \\\hline
\end{tabular}
\caption{Qualitative comparison of descriptions learned using probabilistic
  tokenization to descriptions learned by original \learnpads{}
  algorithm.}
\label{tab:expert}
\end{center}
\end{table}

\subsection{Execution time}
\begin{table}[th]
\begin{center}
\begin{tabular}{|l||c|c|c|}\hline
Data source & HMM & HMEM & HSVM \\ \hline 
1967Transactions & 72.96 & 678.373 & 1390.881   \\\hline 
ai.3000 & 1112.978 & 7565.64 & 27191.87 \\ \hline
yum.txt & 47.381 &  819.373 & 835.01\\ \hline
rpmpkgs.txt & 156.817 & 476.41 & 1489.92\\ \hline
railroad.txt & 19.873 & 209.927 & 527.549  \\ \hline
dibbler.1000 & 546.953 & 3431.927 & 5417.374   \\ \hline
asl.log & 2021.906 & 14245.55 & 52296.55 \\ \hline
scrollkeeper.log  &  50.062 & 565.278 & 1733.855 \\ \hline
page\_log  & 23.69 & 312.321 & 999.72 \\ \hline
MER\_T01\_01.csv & 10.931 & 351.63 & 516.667 \\ \hline
crashreporter & 499.632 & 39.572 & 1849.967 \\ \hline
ls-l.txt & 54.608 & 60.52 & 245.936 \\ \hline
windowserver\_last & 276.393 & 1354.663 & 3685.615 \\ \hline
netstat-an & 54.315 & 422.74 & 1192.808 \\ \hline
boot.txt & 42.012 &  767.893 & 1547.052 \\ \hline
quarterlyincome & 9.586 & 130.903 & 363.634   \\ \hline
corald.log & 188.036 &  881.527 & 329.826 \\ \hline
coraldnssrv.log  & 63.085 &  1553.705 & 6402.181\\ \hline
probed.log & 298.278 & 1647.448 & 4433.592 \\ \hline
coralwebsrv.log & 112.372 & 1175.747 & 4020.109 \\\hline
\end{tabular}
\caption{Time to produce inferred description in seconds}
\label{tab:time}
\end{center}
\end{table}

Compared to the original \learnpads{} system, inference using
statistical models requires extra time to construct \seqset{}s and
compute probabilities. \tblref{tab:time} summarizes the times required
to produce inferred descriptions for the twenty data sources.
{\bf kf: can we include a column for learnpads?}
We ran our experiments on a dual, dual-core 2.2GHz Opteron 275 
processor with 8GB RAM. 
It takes anywhere from a few seconds to several hours to
infer a description depending on the amount of test data
and the statistical model used. In general, the character-by-character
HMM model is the least time-consuming, while HSVM costs the most.
%Long execution times are the main disadvantage of \learnpads{} with
%statistical models. 

Because excution time is linear in the
number of lines in the data source, we can reduce the time by 
running the inferencing algorithm on only a small portion of the raw
data. 
Preliminary experiments show that of the twenty benchmarks, seven data sources
have more than 500 records.  Of these, we can generate
descriptions from just 10\% of the data that can
parse 95\% of records correctly.
For the data sources with fewer than 500 records, 
only 35\% of the data is needed to infer a description that parses 
95\% of the data correctly.

