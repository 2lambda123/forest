\section{Related Work}\label{sec:related}

Format inference of ad hoc data is related to three main areas: 
{\em classic grammar induction}, {\em XML schema inference},
and {\em information extraction}. Classic gramma induction aims at
discovering a grammar for a language using both positive and
negative examples or positive examples only \cite{vidal:gisurvey}. 
Since there is no negative examples in our application, only the latter
is relevant to this paper. However, because learning a perfect
grammar using positive examples only is impossible for any superfinite class 
of languages \cite{gold:inference}
which includes regular expressions and context-free grammar (and also
\pads), previous research has focused on (1) limiting the class of
languages to non-superfinite class 
\cite{angluin:revesible-language-inference,bex+:dtd-inference}, or 
(2) learning an approximation using heuristics or 
probablistic models \cite{hong:thesis,Chen95bayesiangrammar}.
XML schema inference involves learning a schema such as DTD or XSD
from a collection of XML documents \cite{bex+:dtd-inference,garofalakis+:xtract}.
In information extraction, the goal is to find interesting and relevant
bits of information from large amount of data such as 
web pages \cite{kushmerick-phd1997,arasu+:sigmod03} and 
free text \cite{soderland:whisk,Pinto+:texttables,borkar+:text-segmentation}.
These areas do not typically suffer from the token ambiguity problem that 
we see in ad hoc data, because tags cleanly divide \xml{} and web-based data, 
while spaces and known punctuation symbols separate natural language text.
In contrast, the separators and token types found in ad hoc data sources such as
web logs and financial records are far more variable and ambiguous.  

%Classic grammar induction and structure discovery problems have been
%studied for decades. \xml{}-documents are a typical source of
%text-based data that possibily adheres to some structures represented by a
%schema. ~\cite{garofalakis+:xtract} and ~\cite{bex+:inferring-xml-schema}
%infers two \xml{} schema formalisms respectively -
%\em{Document Type Definitions} (DTDs) and \em{XML Schema Definitions}
%(XSDs). ~\cite{bex+:dtd-inference} is focused on inferring
%concise DTDs by considering two class of DTDs with special
%properties. In recent researches, statistical methods have been widely
%used in grammar induction. ~\cite{kushmerick-phd1997} automatically learns the procedure
%for extracting content from the web in the context of inductive
%learning and accesses its learnability by
%fitting the problem into PAC learning model. ~\cite{hong:thesis}
%resolves a similar problem by explicit domain rules and a
%hill-climbing inference algorithm. ~\cite{soderland:whisk} tackles
%both semi-structured and free-text data, and uses covering algorithms.
%It also shares a methodology with inductive learning. For free-text,
%some syntactic and semantic pre-processing analysis is conducted in
%addition to text extraction rules. ~\cite{Chen95bayesiangrammar}
%employs a greedy heuristic search algorithm within a Bayesian
%framework for probabilistic context-free grammar induction. 
%

In addition to applying statisitical models such as HMMs and Conditional
Random Fields (CRFs) to grammar induction problems 
\cite{borkar+:text-segmentation,Pinto+:texttables}, researchers have also
used these models extensively in other areas such as bioinformatics 
\cite{kulp96generalized} and natural language processing \cite{Heeman99:speech}.  
In this paper, we contribute to the literature on statistical
data processing by analyzing the effectiveness of statistical models
in a new application area, that of ad hoc data, which contains
markedly different characteristics from the most frequently studied
data processing domains.

The algorithm presented in this paper is a variant of the algorithm described
in our earlier paper~\cite{fisher+:dirttoshovels}.  We have made
substantial modifications, however, to incorporate probabilistic
parsing information into the tokenization and structure discovery
phases.  We have also added a new phase to the algorithm that
simplifies overly complex descriptions. Our earlier paper contains an
extensive comparison of our basic grammar induction
algorithm to others that have appeared in the literature.

%
%Besides grammar induction, researchers have also witnessed successes of
%statistical and machine learning methods in a variety of other areas.
%~\cite{borkar+:text-segmentation} extracts certain fields in records
%from text using nested HMMs. ~\cite{Pinto+:texttables} utilizes
%Conditional Random Fields (CRFs) to dig rich and overlapping features
%with the purpose of identifying row positions and types in text-based
%tables. Computational biology and natural language processing are also
%two areas in which statistical methods play an important
%role. ~\cite{kulp96generalized} predicts exons and introns in DNA
%sequences by generalized HMMs. An early step of natural language
%understanding is to assign part-of-speech (POS) tags to segments of
%speech sentences and ~\cite{Heeman99:speech} employs decision tree
%algorithms to address this task. These problems typically deal with
%sequencial data, extract features from parts of the data and apply
%machine learning methods to predict certain properties of these
%parts. The data in our token ambiguity problem is different from these
%data, and the learning methods in our system must achieve high
%precision, while still only costs tolerable execution time for large
%amount of data.

