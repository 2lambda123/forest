\cut{
  - HMM
  - Hierarchical Maximum entropy
  - Hierarchical Support Vector Machine
  each: give the formula -> intuition, examples to show where we work well
  and where we work badly
}


\section{Statistical Models}\label{sec:stats}

\cut{As discussed in Section \ref{sec:algo}, our new algorithm considers all possible
token sequences rather than the only one defined by a fixed set of
regular expressions. Therefore a seqset that contains all legal token
sequences is constructed for each chunk. But, the histogram-computing
module afterwards requires one single token sequence from each chunk
and uses these tokens as the basic units to extract structure
information.}

The key to solving the token ambiguity problem is to select the best
token sequence from the many possible sequences represented by a
\seqset{}. Machine learning techniques,
which build probabilistic models from labeled training data,
are applicable to the token ambiguity problem for the following reasons.

\begin{itemize}
%\item Training corpus can be construct efficiently. Instead of
%assigning tags token by token, system designers
%only need to write the correct descriptions by hand for the data
%formats selected as training sources. We've devised a tagging tool to
%label the tokens automatically given the data source and the
%corresponding description;
\item Large amounts of ad hoc data exist from which we can learn token models.
\item The \pads{} infrastructure allows us to label large amounts of training data quickly.
\item Common tokens with distinctive features appear frequently in
  multiple sources.  For example, URLs usually start with  ``\texttt{http://}'',
  and items that look like times, such as ``\texttt{04:12:59},'' make
  certain strings more likely to be parts of dates, such as
  ``{\tt March}''.  This observation means that we can build a
  model of various tokens based on training data and then apply those
  models to new data sources.
\item We can use efficient procedures from machine learning, such as
  the Viterbi algorithm~\cite{rabiner89:hmm}, to find the most likely token
  sequences from learned models.
\end{itemize}
\cut{
In the training corpus, examples are chunks annotated with token
sequences, each token with its associated substring. A statistical
model is trained ahead of time. In the structure discovery phase of
\learnpads{}, the goal is to label the chunks with token sequences.
}

Many different machine learning algorithms
exist~\cite{Seymore99learninghidden,Pinto+:texttables,borkar+:text-segmentation,rabiner89:hmm}.
We have experimented with three different techniques for 
building probabilistic models of tokens, which
we describe in the following subsections.

\subsection{Character-by-character Hidden Markov Model (HMM)}\label{subsec:hmm}

The first technique, called the Hidden Markov Model
(HMM)~\cite{rabiner89:hmm}, represents the true state of a system as a
finite collection of ``hidden states.''  The states are hidden because
we cannot directly detect which state the system is in, and instead
must try to infer the state from observing what it emits.  
Furthermore, the hidden 
state is allowed to change upon each observation.  In our setting,
each token type corresponds to a hidden state, and each character to
an observation.
\figref{fig:hmm} shows a first-order HMM representation of a substring
from {\tt yum.txt}.  The qualifier ``first-order'' indicates that the
current state depends only upon the previous state. 
Each observation, indicated as a shaded node, is a single character. 
Each hidden state, indicated as a white node, 
is the name of the token that describes the
corresponding observable character. We use arrows to denote
both the dependence among the hidden states and the emission of
observations by the hidden states.
We call the hidden state in the character-by-character HMM 
a {\em partial token} because the corresponding character is often
only a part of a full token.


\begin{figure}[th]
\begin{center}
\epsfig{file=hmm.eps, width=0.7\columnwidth}
\end{center}
\caption{HMM of a partial token sequence from string ``2.2.13-4''}\label{fig:hmm}
\end{figure}

 
We use $\mathbf{C}_i$ to denote an observable evidence variable, 
and $\mathbf{T}_i$ to denote an unobservable state variable, i.e. the
token name. 
To avoid bias toward certain characters in the model, we use
a fixed-length character feature vector instead of the character itself
as the observable evidence variable. Each feature vector contains a
fixed number of boolean features about a single character, such as
%Note that we use character
%feature vectors as opposed to plain characters as observations,
%because in many cases, we don't want the model biased towards
%certain characters. For example, if most {\em time} tokens in the
%training corpus are drawn from the software running in a certain
%range of time, some digits will never appear to be the first
%character in {\em time}. To avoid this bias, we define a mapping
%from characters to fix-length vectors and use these vectors to
%represent features of characters. Examples of features are:
\begin{itemize}
\item is a numeric digit
\item is an upper-case alphabetic letter
\item is white space
\item is a special punctuation, ``{\tt .}''
\item is a punctuation other than ``{\tt .}'', ``{\tt
,}'', ``{\tt ?}'', ``{\tt /}'', ``{\tt $\backslash$}" and ``{\tt :}''
\end{itemize}

The model contains the transition matrix $\mathbf{P}(T_i|T_{i-1})$,
the sensor matrix $\mathbf{P}(C_i|T_i)$, and the initial
probabilities $\mathbf{P}(T_i|begin)$. These parameters are obtained
from the training data by the following formula:

\begin{eqnarray*}
\mathbf{P}(T_i|T_{i-1}) & = & \frac{\textrm{number of token }T_i\textrm{
following token }T_{i-1}}{\textrm{number of token }T_{i-1}} \label{eqn:1}\\
\mathbf{P}(C_i|T_i) & = & \frac{\textrm{number of character feature
vector }C_i\textrm{
annotated with token }T_i}{\textrm{number of token }T_i} \\
\mathbf{P}(T_1|begin) & = & \frac{\textrm{number of token
}T_1\textrm{ begin the first token}}{\textrm{number of training
chunks}} \label{eqn:2}
\end{eqnarray*}

Therefore, suppose the number of characters in the chunk is $n$, our
goal is to compare $\mathbf{P}(T_1, T_2, ..., T_n|C_1, C_2, ..., C_n)$
with respect to different token sequences $T_1, T_2, ..., T_n$ for the
same $C_1, C_2, ..., C_n$ characters. This probability can be
calculated by:

\begin{eqnarray*}
\mathbf{P}(T_1, T_2, ..., T_n|C_1, C_2, ..., C_n) & \propto & \mathbf{P}(T_1, T_2, ..., T_n, C_1, C_2, ..., C_n)\\
& = & \mathbf{P}(T_1|begin) \cdot
\prod_{i=2}^{n}{\mathbf{P}(T_i|T_{i-1})}
\end{eqnarray*}

\cut{
To guarantee linear-time efficiency, the Viterbi algorithm to compute
the most likely sequence is employed. In our problem, not every
combination of tokens as a sequence is a legal parse given a chunk. So
the Viterbi algorithm has to be modified so that the most likely sequence
returned by the algorithm is the one among all possible parses. If we
consider this requirement as a constraint to the output, in fact, a
variety of modified Viterbi algorithms are deployed in the
\learnpads{} system to satisfy corresponding constraints at different
places. Due to the space limitation, we'll show only one of them in
Subsection \ref{subsec:hmem}.
}

The original Viterbi algorithm for HMM is modified slightly here 
to ensure that the most probably token sequence returned forms a legal
parse of the data chunk. 

Because the character-by-character HMM is first order and employs only 
single character features, it is not able to capture complex features
in the data such as  
a substring ``{\tt http://}'' which indicates a strong likelihood 
of being part of an URL.
%This HMM assumes every partial token is only dependent on its previous
%partial token. So the first-order Markov Model is a simple model that
%is unable to catch complicated features. 
One obvious solution is
increasing the order of HMM but since the token length is variable in
our application, it is not clear what the order should be, not to mention
increasing the order also increases the complexity exponentially. 
In stead, in the next two subsections, we propose two new hybrid
methods that combine existing classification techniques within the
HMM framework.

\subsection{Hierarchical Maximum Entropy Model (HMEM)}\label{subsec:hmem}

\begin{figure}[th]
\begin{center}
\epsfig{file=hmem.eps, width=0.6\columnwidth}
\end{center}
\caption{HMEM of a token sequence from string ``2.2.13-4''}\label{fig:hmem}
\end{figure}

The upper level mode of a Hierarchical Maximum Entropy Model 
is an HMM while a lower level token model 
is a Maximum Entropy model~\cite{Berger96:ME}\cite{megaweb}.
Fig.\ref{fig:hmem} shows an example of HMEM.
%We can see that the graph of Hierarchical Maximum Entropy Model is
%similar to the one in Subsection \ref{subsec:hmm}, because the upper
%level of this hierarchical model is still an HMM. 
The main difference between HMEM and HMM is
that the observable evidence variables are not single character feature vectors, 
but substrings representing the entire tokens.
As such, there is no need to concatenate consecutive hidden state
variables to form a token as in the case of HMM.

We use $\mathbf{S}_i$ to denote substrings
inside the same token, and $\mathbf{T}_i$ to denote tokens annotating
substrings. Suppose the number of tokens in the chunk is $l$, the
target probability is computed by:

\begin{eqnarray*}
& \mathbf{P}(T_1, T_2, ..., T_n|S_1, S_2, ..., S_l)  \propto &
\mathbf{P}(T_1|begin) \cdot \prod_{i=2}^{l}{\mathbf{P}(T_i|T_{i-1})}
\cdot \prod_{i=1}^{l}\mathbf{P}(S_i|T_i)
\end{eqnarray*}

The transition matrix and initial probability are calculated in the
same way as shown in Equation \ref{eqn:1} and Equation \ref{eqn:2}
in Subsection \ref{subsec:hmm}. Then the question comes to how to
estimate $\mathbf{P}(S_i|T_i)$.

By Bayes rule,
\begin{eqnarray*}
\mathbf{P}(S_i|T_i) = \frac{\mathbf{P}(T_i|S_i) \cdot
\mathbf{P}(S_i)}{\mathbf{P}(T_i)}
\end{eqnarray*}.

Because we construct our training corpus by gathering data sources 
from real world applications, the two priors $\mathbf{P}(S_i)$ and
$\mathbf{P}(T_i)$ can be quite different between the training and
testing data so that they're not good parameters for prediction. As
a result, we use Maximum Entropy method to compute
$\mathbf{P}(T_i|S_i)$ and use this probability to estimate
$\mathbf{P}(S_i|T_i)$ directly, e.g.:

\begin{eqnarray*}
\mathbf{P}(T_1, T_2, ..., T_n|S_1, S_2, ..., S_l) \propto
\mathbf{P}(T_1|begin) \cdot \prod_{i=2}^{l}\mathbf{P}(T_i|T_{i-1})
\cdot \prod_{i=1}^{l}\mathbf{P}(T_i|S_i)
\end{eqnarray*}

Below are examples of token features used to estimate
$P(T_i|S_i)$:

\begin{itemize}
\item number of punctuation "{\tt .}"
\item number of total characters in the token
\item number of all punctuations in the token
\item has substring "{\tt am}", "{\tt pm}", etc
\item has substring "{\tt January}", "{\tt Jan}", "{\tt january}",
"{\tt jan}", etc
\item is a sequence of integers separated by "{\tt .}"
\item previous character is a punctuation
\end{itemize}

Note that in this equation, the number of terms in the product is
controlled by the number of tokens, as opposed to the number of
characters in HMM. So sequences with more tokens will produce more terms, 
which makes the algorithm bias towards shorter token sequences. 
This is evident from the example below.
Suppose the data chunk {\tt s} and the probabilities are as
follows:

\begin{figure}[t]
\begin{centercode}
s  = ``Dec 10 04:07:51 Updated''
P( date | ``Dec 10'' ) = 0.8
P( white | `` `` ) = 1.0
P( time | ``04:07:51'' ) = 0.9
P( word | ``Updated'') = 0.8
P( white | others ) = 0.7
P( others | white ) = 0.8
\end{centercode}
\caption{Probability assumptions of a simple example}
\end{figure}

Suppose {\tt others} is short for tokens other than white space and
punctuations. Under this configuration, the conditional probability
of
\begin{verbatim}
P( date white time white word | ``Dec 10 04:07:51 Updated'' ) 
\end{verbatim}
is 0.181.

If we had a token {\tt message} that can parse the entire chunk,
even though the probability of token {\tt message} given the entire
string is small, for example, 0.3, the shorter token sequence {\tt
message} would still be selected. In order to avoid such bias, the
equation is modified as follows:

\begin{eqnarray*}
&& log \mathbf{P}(T_1, T_2, ..., T_n|S_1, S_2, ..., S_l) \\ 
& \propto & \frac{log \mathbf{P}(T_1|begin) +
\sum_{i=2}^{l}log \mathbf{P}(T_i|T_{i-1}) + \sum_{i=1}^{l}log
\mathbf{P}(T_i|S_i)}{l}
\end{eqnarray*}

The average log likelihood guarantees the shorter token sequences
will not always be selected, unless the average of all conditional
probabilities $\mathbf{P}(T_i|S_i)$ exceeds a threshold.
The above change gives rise to a {\em modified Viterbi Algorithm},
which take into account the number of tokens in the sequence.
The modified algorithm has a slightly revised notation. 

Suppose the number of characters
in the chunk is $n$ and the number of tokens is $l$. Let's use $C_i$
to denote the the character at
position $i$, and use $PT_i$ to denote the corresponding partial
token. Note that if position $i$ is the destination end of an edge in
the \seqset{}, there must be a token whose ending position is
$i$ in a legal token sequence. Let's use $T_i$ to denote the token
that ends at position $i$ and
use $S_i$ to denote the associated substring. 

$\mathbf{P}(PT_1, PT_2, ..., PT_i|C_1, C_2, ..., C_i, k)$ denotes the
probability of partial token sequence $PT_1, PT_2, ..., PT_i$
conditioned on substring $C_1, C_2, ..., C_i$, in which the token
sequence length is $k$. The recursive function of forward messages is
divided into 2 cases:

$\\ \displaystyle{\max_{PT_1, PT_2, ..., PT_i}}\log\mathbf{P}(PT_1, PT_2, ..., PT_i,
PT_{i+1}|C_1, C_2, ..., C_{i+1}, k+1) \propto$

$\left\{
  \begin{array}{ll}
    \log \mathbf{P}(S_{i+1}|T_{i+1}) + \\
\displaystyle{\max_{T_{i+1-\delta}}}(\log\mathbf{P}(T_{i+1}|T_{i+1-\delta})
+ \displaystyle{\max_{PT_1, ...,
  PT_{i-1}}}\log\mathbf{P}(PT_1, ..., PT_i|C_1, ..., C_i, k)), 
& \\ \qquad
  \hbox{if $i+1$ is the end of an edge in \seqset{},
  $\delta$ is the length of token $T_{i+1}$;} \\\\
    \displaystyle{\max_{PT1, ..., PT_i}}\log\mathbf{P}(PT_1, ..., PT_i|C1, ...,
  C_i, k+1)
\qquad \qquad \qquad \qquad \qquad \hbox{otherwise.}
  \end{array}
\right. $

$\\$
 
At each position $i$, the forward messages store not only token
sequences with different last tokens, but also the ones with different
length. At the last position $n$, compute 

\[\displaystyle{\max_{l}}\log \frac{\mathbf{P}(TP_1, TP_2, ...,
TP_n|C_1, C_2, ..., C_n,l)}{l}\]
\noindent
and select $l$ as well as the last token in the most likely token
sequences. After tracing backwards through the forward message chain,
the most likely token sequences is obtained. The modified Viterbi
algorithm is still linear to the length of the substring.


%When testing the accuracy of HMEM, we found an empirical
%disadvantage of this model. Training data are collected from their
%original sources without many extra efforts except running the
%automatic token tagging tool. In order to make sure there're
%sufficient data to train the models, every record is directly put into
%the training corpus. But on the other hand, much more work needs to be
%done to find a perfect training corpus that can reflect the true
%distribution of tokens needs. For example, in the training data,
%suppose we have only 1\% {\tt url} tokens, it doesn't mean the
%chance {\tt url} tokens appear in the testing data source is also
%small. 
Then Maximum Entropy model used in HMEM is a
generative model that simulates the procedure of generating the data, and
estimates the target conditional probability by a joint
probability. Therefore it biases towards tokens with more
occurrences in the training data which affects the accuracy of inferencing. 
For instance, when there're not enough
{\tt hostname} tokens in the training data, HMEM can't identify {\tt
hostname} tokens effectively and confuses them with {\tt id}
tokens. We will show next that the Hierarchical
Support Vector Machine (HSVM), with a discriminative model as the
lower level model, solves this problem.

\subsection{Hierarchical Support Vector Machines (HSVM)}\label{subsec:hsvm}

HSVM is exactly the same as HMEM except its lower
level employs Support Vector Machines (SVM)~\cite{CC01a} to classify
the tokens. 
Basically, SVM measures the target conditional probability $\mathbf{P}(T_i|S_i)$ 
by generating hyperplanes that divide the feature vector space according to the
positions of training data points. The hyperplanes are positioned such that the
data points (feature vectors in our case) are separated into classes with
the maximum margin between any two classes. The data points that lie on
the margins (or boundaries) of each class are called {\em support vectors}. 

The empirical evaluation in the next section shows that HSVM is effective
in detecting less occurring tokens. The time complexity of SVM 
is roughly proportional to the number of support vectors produced at training
step. Our system generally requires a large number of support vectors 
and thus HSVM is usually more time-consuming than HMEM.
