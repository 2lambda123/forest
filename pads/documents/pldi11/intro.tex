\section {Introduction}
\label{sec:intro}

\dpw{argg: telling the story is slightly de-anonymizing.  What to do?}%
Databases are an effective, time-tested technology for storing
structured and semi-structured data.  Nevertheless, many computer
users eschew the benefits of structured databases and store important
semi-structured information in collections of conventional files,
scattered across a conventional file system.  For example, the
Princeton Computer Science Department, which should know better,
stores records of undergraduate student grades in a structured set of
directories and uses scripts to compute averages and
study grading trends.  Similarly, Michael Freedman collects
sets of log files from Coral, a distributed content distribution
network~\cite{freedman+:coral,freedman:coral-experience}.  
The logs are orangized in hierarchical
directory structures based on machine name, time and date.  Freedman
mines the logs for information on system security, health and performance.  At
Harvard, Vinothan Manoharan, a physics professor, stores 
his experimental data in sets of files and
extracts information using python scripts.  At AT\&T, vast structured
repositories contain networking information,
phone call detail and billing data.  And there are
many other examples across the computational sciences and social
sciences, in computer systems research, in computer
systems administration, and in industry.

There are a number of reasons that users
choose to implement ad hoc data bases in this manner.  One of the most 
prevalent
is that using databases usually requires paying substantial up-front 
costs such as: (1) finding and evaluating the
appropriate database
software (and possibly paying for it); (2) learning how to load data into
the database; (3) possibly writing programs that transform the raw
data in to a form that allows it to be loaded appropriately; 
(4) learning how to access the data once it is in the database; and 
(5) interfacing the database with a conventional programming language
for query post-processing, visualization, debugging, or other tasks.

Rather than paying these costs, programmers often 
dump their data into the file system, using a combination of
directory structure, file names and file contents
to structure the data.  The ``query language'' is often a shell script
or conventional programming language.  
Unfortunately, programming with such roll-your-own 
databases can have a number of negative consequences.  
First, there is generally no documentation
of file system structure.  This means that the data and its organization
can be hard to understand,
new users take longer to come up to speed on the infrastructure and there
is risk that if the system administrator leaves his or her post, knowledge
of the data schema will be lost.  Second, over time, the repository
structure tends to evolve: new elements are added and old formats are changed,
sometimes accidentally.  This can cause hacked-up data processing
tools to break or to return erroneous results. Naturally, it also makes
understanding the data even more difficult.  Third, there is often no 
systematic means for detecting data errors.  Data errors are immensely
important as they can often be signals that other parts of the system --
hardware, servers or other elements of the tool chain -- are broken.
Fourth, every analysis is more-or-less
built from scratch.  There is no auxiliary 
query or tool support and no debugging support.  Any tools that are
built tend to be ``one-off'' tools that are inflexible, unreuseable
and difficult to modify.  Fifth, dealing with large data sets, which
are common in many settings, imposes extra difficulties.  One example
of the simplest kind is that standard tools 
such as \cd{ls} simply break when more than 256 files appear on the
command line.  Hence, programmers must tediously break up their data
and process it in smaller sets.  
%While such problems are not
%technically insurmountable, they all add up to substantial amounts
%of time and programming resources that could be better spent if there
%was a better way.

In this paper, we propose a better way:  A novel specification language,
programming environment and toolkit for processing file system fragments.
This language, called \forest{}, is embedded directly in \haskell{}.
\dpw{should the font for \haskell{} be the same as the font for \forest{}?}%
It allows programmers to define the expected shape of file systems or
file system fragments, to materialize actual file system fragments
into a \haskell{} program as strongly-typed 
\haskell{} data structures, and to support efficient, robust 
programming with these
data structures by generating useful type class instances, interfaces for
file system meta data, programming libraries
and robust shell tools.  

One of the primary goals of the system is
to make programming with such file system fragments
as lightweight, easy and robust as programming with any other strongly-typed,
structured, in-memory \haskell{} data structure.  In addition, however,
\forest{} specifications are useful because they are {\em executable 
documentation} of expected file system structure.  For example, Unix file
systems should be laid out as the
Filesystem Hierarchy Standard Group has described in their informal
standards document~\cite{fsh}.  \forest{} provides a formal language
by which many of these standards may be documented precisely and
a checker that allows users to verify that their installation
conforms to the standard. As another example, the 
\pads{} website~\cite{padsweb}
contains a relatively complex set of scripts and data files used to
implement online demos.  
\dpw{note: need anonymization of pads web site?}
When modifying these demos to add new examples
or features, it is almost always the case that an important
file will be forgotten or some set of permissions will be set incorrectly.
%The debugging process through the web interface is cumbersome and unintuitive.
A \forest{} description can be used to document and check that
all necessary files are present and that their permissions are set properly.
\dpw{we should find a citation for a security example that requires certain
directories to not contain certain executables, for example.}

As well as serving as documentation, checking for consistency, and 
providing basic programming
support by (lazily) materializing file system fragments in memory, \forest{}
provides substantial auxiliary support for
programmers.  The goal is for programmers to obtain a whole range
benefits by writing one simple, compact file system specification.
The automatically generated auxiliary support includes:
\begin{enumerate}
\item a set of new type declarations that characterize
the shape of the file system fragment as it appears in memory; 
\item a set of data structures that characterize errors and other metadata
concerning the describe file system fragment;
\item libraries for analyzing
data and errors so that programmers may quickly detect
and correct deviations from their expectations;
\item a set of \haskell{} type class instances that make it possible for
programmers to query, analyze, and transform file system data using generic
libraries immediately;
\item a set of domain-specific tools, such as file system visualization,
access control analysis, automatic file system shape analysis and description
generation;
\item robust, drop-in, specification-directed replacements for standard
Unix-style command-line tools such as \cd{ls}, \cd{grep}, \cd{tar} and
others.
\end{enumerate}
In summary, this paper makes the following contributions.
\begin{itemize}
\item {\bf Conceptual}:  this paper proposes the novel {\em idea} 
of extending a modern programming language with
{\em tightly integrated linguistic features for describing file system 
fragments}
and for automatically generating programming infrastructure from such 
descriptions.

\item {\bf Language Design}: this paper describes our 
  carefully chosen \forest{}
  programming features and illustrates their application to
  real-world examples.
  The design is expressive, concise and effectively integrated into
  Haskell.  It is also backed by a formal semantics for a core calculus,
  inspired by classical tree logics.

\item {\bf Tool Generation Architecture}: this paper describes the architecture of
  our tool generation infrastructure and the way it interacts with and 
  exploits the powerful generic programming features of \haskell{}.

\item {\bf Case Study in Domain-Specific Language Design}: \forest{}
  is fully implemented and its design 
  acts as a case study in extensive, practical, domain-specific
  language design in modern languages by combining a number of
  experimental features of \haskell{} such as quasi-quoting and \template{}.
  Moreover, our \forest{} design and implementation experience 
  has had practical impact on the \haskell{} implementation itself:  
  the \haskell{} team modified and extended
  \template{} in response to our needs.  Several of these modifications
  are now available in the most recent release of \haskell{}.
  \dpw{Most recent release?  Head release? Future releases? Please correct me.}%
\end{itemize}

%Overall, \forest{} substantially lowers pragmatic barriers to programming 
%with data on disk.

%% Tool Notes:

%% \begin{itemize}
%% \item Core tools implemented within the compiler. (loader)
%% \item Generic tools, useable with all descriptions, and implemented as
%%   third-party libraries.
%% \begin{itemize}
%% \item Unix-like shell tools: grep, ls, tar, cp, rm
%% \item description-generation tool using universal description
%% \item simple directory visualization by generating dot files and highlighting errors
%% \item pretty printing tools
%% \end{itemize}
%% \item Custom programming against typed interface and querying using Haskell generic programming libraries
%% \end{itemize}
