
\paragraph*{Dealing with errors.}  
In 1967, Gold~\cite{gold:inference}
proved that learning a grammar for any remotely
sophisticated class of languages, such as the regular languages, 
is impossible if one is only given positive 
example data.\footnote{A positive example is a data source
known to be in the grammar to be learned.  
A negative example is one known {\em not} to be in the target grammar.
Learning with positive examples and negative examples is possible.
Unfortunately, given that data analysts are unlikely to have access to
ad hoc data that they know {\em does not} 
satisfy the format they are interested in learning,
we are forced to tackle the more difficult problem of learning from 
positive examples only.}
Given this negative theoretical result, and the practical fact
that it is hard to be sure that training data
is sufficiently rich to witness all possible variation in the data,
errors in inference are inevitable.  Fortunately, however, detecting
and recovering from errors in ad hoc data is one of the primary strengths
of the \pads{} system.  

To determine exactly how accurate an
inferred description is on any new data source, a user may run the
accumulator tool.  This tool catalogs exactly how many deviations
from the description there were overall in the data source
as well as the error rate in every individual field.
Hence, using this tool, a programmer can immediately and reliably
determine the effectiveness of inference for their data.
If there is a serious 
problem, the user can easily edit the generated description by hand
-- identification of a problem field, a minor
edit and recompilation of tools might just take 5 minutes.  Hence,
even imperfectly-generated descriptions have great value in terms of
improving programmer productivity.  Moreover, all \pads{}-generated 
parsers and tools
have error detection, representation and recovery techniques.
For instance, when converting data to \xml{}, errors encountered
are represented explicitly in the \xml{} document, allowing users to query
the data for errors if they choose.  Before graphing ad hoc data,
an analyst may use the accumulator tool to check if any errors occur
in the fields to be graphed.  If not, there is no reason to edit
the description at all -- graphing the correct fields may proceed 
immediately.

\paragraph*{Future work.}  
Discovering tokens like ``IP address'' and ``date'' is highly beneficial
as such tokens act as compact, highly 
descriptive, human-readable abstractions. 
Unfortunately, these tokens are also often 
mutually ambiguous.  For instance, an IP address, a 
floating point number 
and a phone number can all be represented as some number of digits 
separated by periods.  At the moment, we disambiguate between them 
in the same way that lex does, by taking the first, longest match. 
In select cases, when we cannot disambiguate in the tokenization phase, we
try to correct problems using domain-specific rewriting rules in the 
structure refinement phase.    
To improve tokenization in the future, we plan to look at learning
probabilistic models of a broad range of token types.  
We also
intend to explore finding new tokens from the data itself,
possibly by identifying abrupt changes in entropy~\cite{hutchens98finding}.

%From an experimental perspective,

% Hence, while our system allows users to
% customize the tokenization scheme through a configuration file,
% this customization requires work, and 
% for the computational biologist not accustomed to writing or 
% reading regular expressions, it creates a barrier to usage.  
% Moreover,
% since tokenization ambiguities are often unavoidable, even after tweaking
% tokenization configurations, system results
% are dependent upon our simplistic disambiguation 
% scheme.\footnote{We disambiguate
% like lex does, taking the first, longest match in the token list.}  
% Indeed, to avoid ambiguities that arise from having
% floating-point numbers be tokens, we do not identify floats during
% the tokenization face, but instead introduce floats through a number of 
% custom
% rewriting rules in the rewriting phase.  

% At the moment, we see two possible solutions to this problem.  The
% first solution is to attempt to identify new tokens from the data 
% source being learned, perhaps by identifying significant changes in entropy
% as suggested by.  This 

% thoughts on problems with tokenization; information theory; partial
% descriptions; user interface; recursion; more experimentation with a
% broader range of formats
