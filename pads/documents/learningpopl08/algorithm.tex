% \subsection {Algorithm Overview}

\begin{figure}
\begin{center}
\epsfig{file=archi.eps, width=.9\columnwidth}
\caption{Architecture of the format inference engine}
\vspace*{-5mm}
\label{fig-archi}
\end{center}
\end{figure}

Figure \ref{fig-archi} gives an overview of our format inference
architecture. The input data ({\em i.e.,} training set) is first
divided into {\em chunks} as specified by the user.  Intuitively, a chunk
is a unit of repetition in the data source.  Our tool currently supports
chunking on a line-by-line basis as well as on a file-by-file basis.  

Each chunk is broken down into a
series of {\em tokens} by a lexer.
Each token can be a punctuation symbol, a
number, a date, a time, or a number of other basic items.  Every token
type has a corresponding base type in the \ir{}, though the 
converse is not true -- there are
a number of base types that are not used as tokens.

Our learning system has a default tokenization scheme skewed toward systems
data, but users may specify a different scheme for their own domain
through a configuration file.  For example, computational biologists
may want to add DNA strings {\tt CATTGTT...} to the default tokenization 
scheme.  The configuration file is essentially
a list of name-regular expressions pairs -- see Figure~\ref{fig:configfile}
for a small example.  The system uses the file to generate 
part of the system's lexer, a
collection of new \ir{} base types, and a series of type 
definitions that are incorporated into the final \pads{} specification.  

After tokenization, the algorithm enters the {\em structure discovery} 
phase.  This phase uses a top-down, divide-and-conquer
scheme to guess an approximate, initial format for the data.
To assess the quality of this initial approximation, the
algorithm computes an information-theoretic {\em score} for the
structure.  This score is used in the next phase to guide
{\em rewriting rules} that iteratively refine the format.  

Once the format has been refined to the extent possible, the resulting
\ir{} is printed in \pads{} syntax.  Template programs that use the
new description, including the \xml{}-converter, a simple
statistics tool we call the {\em accumulator}, a database tool
based on Oetiker's RRDTool~\cite{rrdtool}, and the \padx{}
query engine, are also all generated at this point.  Hence, starting
with data alone, our end-to-end algorithm quickly generates an entire suite
of fully functional data processing tools, ready to use at the command line.

The following subsections describe the main components of the algorithm
in more detail, including the structure discovery phase, the 
information-theoretic scoring function, the structure refinement phase
and the end products of the algorithm.

\begin{figure}
\begin{verbatim}
def triplet [0-9]{1,3}
def doublet [0-9]{1,2}
def hexdoub [0-9a-fA-F]{2}
...
exp Pemail {str1}@{hostname}
exp Pmac   ({hexdoub}(: | \-)){5}{hexdoub}
exp PbXML  \<([a-zA-Z])+\>
exp PeXML  \<\/[^>]+\>
\end{verbatim}
\caption{Tiny fragment of the default configuration file. 
The configuration command {\tt exp} specifies the definition on that
line should be  ``exported'' to
the learning tool whereas command {\tt def} implies the definition
is local and used only in other {\tt exp} or {\tt def} in the
configuration file.}
\label {fig:configfile}
\end{figure}

%% \subsection {Chunking and Tokenization}




%% \begin{figure}
%% \begin{center}
%% \begin{tabular}{|l|l|}
%% \hline
%% Name   &  Description               \\ \hline\hline
%% Pint  &   Integer \\ 
%% Palpha & Alpha-numeric string including '$\_$' and '$-$' \\
%% Pip & IP address \\
%% Pemail & Email address \\
%% Pmac & Mac address \\
%% Pdate & Simple date format \\
%% Ptime & Simple time format \\
%% Ppath & File system path \\
%% Phostname & Hostname \\
%% Purl  & URL \\
%% PbXML & Beginning XML tag \\
%% PeXML & Ending XML tag \\
%% Pother & Punctuation character \\\hline
%% \end{tabular}

%% \caption{Basic token types in default configuration.}
%% \label{figure:base-types}
%% \end{center}
%% \end{figure}
 



%%     *  mention system parameterization for multiple tokenizations for different domains
%%     * mention current skew towards systems data
%%     * ignore problems in this section -- save that for discussion/future work section
%%     * mention stream of tokens generated for running example 

\subsection {Structure Discovery}

{\em note: change algorithm so that structs only partition on 1 token
 in the middle...explain in the magic section or later that this
 is a simplification and we do many tokens at the same time}

Given a collection of tokenized chunks, the goal of the structure
discovery phase is to find a candidate description ``nearby'' the
final solution.  In the following phase, the rewriting rules transform
and refine the candidate to produce that final solution.  The overall
structure of this phase of the algorithm was directly inspired by the
work of Arasu and Garcia-Molina~\cite{arasu+:sigmod03}.  However, the
details are quite different as Arasu is trying to extract information
from \html{} web pages as opposed to describing ad hoc data.

\paragraph*{Algorithm Basics.}
Our algorithm operates by analyzing the collection of tokenized chunks,
and guessing what the top-level type constructor is.  Based on this guess,
it will partition the chunks and recursively analyze each partition.
Using the guess and the results of recursive application of the
procedure, the algorithm builds a description for the data.
We will explain how the magical ``guessing'' works
in a moment, but for now, Figure~\ref{fig:structure-discovery} 
presents an outline of how the overall procedure operates.  

As an example, recall the Crashreporter.log data presented in
Figure~\ref{fig:example}.  After tokenization, assuming a chunk is
a line of data, the first two chunks contain the following token sequences:
{\small
\begin{verbatim}
   Pdate ' ' Ptime ' ' Pint ' ' Palpha '[' Pint 
     ']' ':' ...

   Pdate ' ' Ptime ' ' Pint ' ' Palpha '[' Pint 
     ']' ':' ...
\end{verbatim}
}
\noindent
Given these token sequences, our inference algorithm might guess that
the top-level type constructor is a struct:  
\[
\mbox{\small
\cd{struct \{}$T_0$\cd{; Pdate; }$T_1$\cd{; Ptime; }$T_2$\cd{; ':'; }$T_3$\cd{;\}}
}
\]
The types $T_0$, $T_1$, $T_2$ and $T_3$ are not yet known -- they will
be constructed recursively.  $T_0$ is constructed by recursively applying
the discovery procedure to the data in the chunks that precedes
the first occurrence of \cd{Pdate}.  In this case, there is no such
data, so the inference procedure returns the trivial type \cd{Pempty} for
$T_0$.  $T_1$ is constructed by applying discovery to the data between
\cd{Pdate} and \cd{Ptime}.  Each chunk contains one token in this case,
the space token.  Hence the discovery procedure easily decides
$T_1$ is \cd{' '}. $T_2$ is discovered by recursively analyzing the following
two chunks.
{\small
\begin{verbatim}
   ' ' Pint ' ' Palpha '[' Pint ']' 
   ' ' Pint ' ' Palpha '[' Pint ']'
\end{verbatim}
}
\noindent
Finally, the structure of $T_3$ is discovered by recursively analyzing 
all data following the ':' token.

As a second example, consider the Sirius data from Figure~\ref{fig:example}.
Here the token streams have the following structure:
{\small
\begin{verbatim}
   Pint '|' Pint '|' ... '|' Pint '|' Pint
   Pint '|' Pint '|' ... '|' Palpha Pint '|' Pint
\end{verbatim}
}
\noindent
In this case, our algorithm guesses that the top-level structure involves
an array and hence predicts this shape:
\[
\mbox{\small
\cd{struct \{}$T_{\mathrm{first}}$\cd{; array \{}$T_{\mathrm{body}}$\cd{\};}$T_{\mathrm{last}}$\cd{;\}}
}
\]
\noindent
At this point in the analysis, predicting exactly where an array
begins and ends is difficult.  Consequently, the algorithm allows for
some slop prior to beginning the array ($T_{\mathrm{first}}$) and some
slop at the end of the array ($T_{\mathrm{last}}$).  If this slop
turns out to be unnecessary, the rewriting rules will clean up the
mess in the next phase.  The array itself has elements with type
$T_{\mathrm{body}}$.  In order to determine the type of the body
elements, the algorithm examines the following collection of chunks:
{\small
\begin{verbatim}
   Pint '|' 
   ... 
   Pint '|' 
   Pint '|'
   ...
   Palpha Pint '|'
\end{verbatim}
}
\noindent
In this case, the two large initial chunks are partitioned 
into a series of many small array-element chunks for recursive 
analysis based on occurrences of the \cd{'|'} token.

So far so good, but how does the guessing work?  Why does the
algorithm decide the Sirius data is basically an array but the
Crashreporter.log is a struct? After all, all the Sirius chunks start
with a Pint, which is similar to all the Crashreporter chunks starting
with a Pdate.  Likewise, Crashreporter contains many occurrences of
the \cd{' '} token, which, like the \cd{'|'} in Sirius, might serve as
an array separator.

\paragraph*{The Magic.}
In order to make informed guesses about the structure of the chunks
under consideration, every recursive iteration of the algorithm computes 
a histogram for each token appearing in the data.
More specifically, a histogram for token $t$
plots the number of chunks (on the $y$-axis)
having a certain number of occurrences of the token (on the $x$-axis). 
Figure~\ref{fig:histograms} presents a number of histograms computed
from the top-level analysis of Crashreporter.log and Sirius chunks.

Intuitively, histograms with high {\em coverage}, meaning the token appears
in almost every chunk and has a {\em narrow} distribution, meaning the variation in
the number of times a token appears in different chunks is low, are
good candidates for defining structs.  Similarly, histograms with
high {\em coverage} and wide distribution, are good candidates for defining
arrays.  Finally, histograms with low coverge or intermediate width
represent tokens that form part of a union.  For example, histogram (a)
from Figure~\ref{fig:histograms} ... {\em explain once we have data}

To make the intuitions above precise, we must define a number of
properties of histograms.  First, a histogram $h$ is a list of pairs
of natural numbers $(x,y)$ where $x$ denotes the token frequency and
$y$ denotes the number of records with that frequency.  
All first elements of pairs in the list must be unique.  
The {\em width} of a
histogram ({\em width}($h$)) is the number of elements in the list
excluding the zero-bar ({i.e.,} excluding element $(0,y)$).  
A histogram
$\normal{h}$ is in normal form when the first element of the list is
the zero column and all subsequent elements are sorted in descending
order by the $y$ component.  For example, if $h_1$ is the histogram
$[(0,5), (1,10), (2,25), (3,15)]$ then {\em width}($h_1$) is 3 and its
normal form $\normal{h_1}$ is $[(0,5), (2, 25), (3,15), (1,10)]$.

We often refer to $y$ as the {\em mass} of the element $(x,y)$
and given a histogram $h$, we refer to the mass of the $i^{\mathrm th}$
using the notation $h[i]$.  For instance, $h_1[3] = 15$ and 
$\bar{h_1}[3] = 10$.  The {\em residual mass} ({\em rm}) of a column $i$ in 
a normalized histogram $h$ is the mass of all the columns to the right of 
$i$ plus the mass of the zero-bar.  Mathematically, 
{\em rm}$(\bar{h},i) = h[0] + \sum_{j=i+1}^{\mathit{width}(h)} h[j]$.
For example, {\em rm}$(\bar{h_1},1) = 5 + 15 + 10 = 30$.
The residual mass is the primary characterization of ``narrowness''
of a histogram.  Those histograms with low residual mass of the first
column ({\em i.e.,} {\em rm}$(\bar{h_1},1)$ is small) 
are good candidates for structs.

In order to distinguishing between
structs, unions and arrays,
we also need to define the {\em coverage} of a histogram, which
is simply the sum of the non-zero histogram elements.  Mathematically,
{\em coverage}$(\bar{h}) = \sum_{j=1}^{\mathit{width}(h)} h[j]$.

Finally, recursive partitioning of the data works better when
groups of tokens with similar distributions are considered together
-- when two tokens have the same distribution, it is almost always the 
case that they form part of the same type constructor.  There are a 
number of ways to define {\em similarity} of histograms.  We have
chosen a symmetric form of {\em relative entropy}.\footnote{Suggested
by Rob Schapire, personal communication, May 2007.}  
The (plain) relative entropy
of two normalized histograms $\normal{h_1}$ and $\normal{h_2}$, 
written \relativee{\normal{h_1}}{\normal{h_2}}, is
defined as follows.
\[
 \relativee{\normal{h_1}}{\normal{h_2}} 
   = \sum_{j=1}^{\mathit{width}(\normal{h_1})} \normal{h_1}[j]*log(\normal{h_1}[j]/\normal{h_2}[j])
\]
To create a symmetric form, we first find the average of the two
histograms in question (written $\addh{h_1}{h_2}$)
by summing corresponding columns and dividing by two.  This
has the effect of preventing the denominator from being zero in the 
final relative entropy computation.  At last, the symmetric
relative entropy is:
\[
 \srelativee{\normal{h_1}}{\normal{h_2}} 
   = \frac{1}{2}  \relativee{\normal{h_1}}{\addh{\normal{h_1}}{\normal{h_2}}}
   +  \frac{1}{2}  \relativee{\normal{h_2}}{\addh{\normal{h_1}}{\normal{h_2}}}
\]

Now that we have defined the relevant properties of histograms,
we can explain the method used to recursively guess type constructors.

\begin {enumerate}
\item Group related histograms into sets as follows.  
A histogram $h_1$ belongs to
group $G$ provided there exists another histogram $h_2$ in $G$
such that $\srelativee{\normal{h_1}}{\normal{h_2}} < 
\mathrm{ClusterTolerance}$.  We have found a ClusterTolerance
of 0.01 is effective.  A histogram dissimilar to all others may form 
its own group.

\item Identify a struct by:
\begin {enumerate}
\item  Considering all groups $G$, in descending order by {\em StructScore}.
The first group $G$ that contains any tokens that satisfy the
 following minimum criteria will be declared a struct on this 
iteration of the recursive algorithm:
\begin {itemize}
\item maximum residual mass
\item minimum coverage
\end{itemize}
If no tokens in any group satisfies the struct criterion, check for arrays.
\end{enumerate}
\item Identify an array by:
\begin {enumerate}
\item  Considering all groups $G$, in descending order by {\em ArrayScore}.
The first group $G$ that contains any tokens that satisfy the
 following minimum criteria will be declared an array on this 
iteration of the recursive algorithm:
\begin {itemize}
\item minimum coverage
\end{itemize}
If no tokens in any group satisfies the array criterion, find a union.
\end{enumerate}
\item Identify a union by partitioning the current set of chunks according
to the first token in each chunk.
\end{enumerate}

\begin {figure*}

a) histogram for ' '
histogram for ':'
...

Omit telling the reader which histograms are for which tokens in the figure.
We'll explain in the text...

May 6 of them 3 each from Crashreporter.log and Sirius?
We'll pick carefully.
\caption{Selected Histograms for Crashreporter.log and Sirius}
\label{fig:histograms} 
\end{figure*}

\paragraph*{Additional Tricks.}
Grouping

\input{discoveryalg.tex}

%%     *  role = quickly find a description in the "approximate area" of the correct description
%%     * explain structure of a generic "top-down" inference algorithm -- perhaps give pseudocode
%%     * explain our heuristics: generation of histograms, choice of struct, array, union, base type
%%     * grouping construct (introduce additional example as needed)
%%     * show (part of?) description of running example 

\subsection {Information-Theoretic Scoring}

\input{scoring}

\subsection {Structure Refinement}

%\input{rules}

%    *  role = starting with the candidate structure, search for nearby descriptions that optimize an information theoretic scoring function
%    * explain the 3 parts: value independent, value dependent, value independent
%    * give (partial) list of rules used -- we need to work on notation for explaining these rules
%    * illustrate several transformations using the running example
%    * compare example after rewriting to the example from subsection 3.4 above
%    * optional subsubsection: theory suggesting our algorithm is "correct" (we'd need a semantics for our IR then)

\input{refinement} 

\subsection {Finishing Up}

    * printing pads syntax and invoking toolchain
