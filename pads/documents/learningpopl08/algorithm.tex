% \subsection {Algorithm Overview}

\begin{figure}
\begin{center}
\epsfig{file=archi.eps, width=.9\columnwidth}
\caption{Architecture of the format inference engine}
\vspace*{-5mm}
\label{fig-archi}
\end{center}
\end{figure}

Figure \ref{fig-archi} gives an overview of our format inference
architecture. The input data ({\em i.e.,} training set) is first
divided into {\em chunks} as specified by the user.  Intuitively, a chunk
is a unit of repetition in the data source.  Our tool currently supports
chunking on a line-by-line basis as well as on a file-by-file basis.  

Each chunk is broken down into a
series of {\em tokens} by a lexer.
Each token can be a punctuation symbol, a
number, a date, a time, or a number of other basic items.  Every token
type has a corresponding base type in the \ir{}, though the 
converse is not true -- there are
a number of base types that are not used as tokens.

Our learning system has a default tokenization scheme skewed toward systems
data, but users may specify a different scheme for their own domain
through a configuration file.  For example, computational biologists
may want to add DNA strings {\tt CATTGTT...} to the default tokenization 
scheme.  The configuration file is essentially
a list of name-regular expressions pairs -- see Figure~\ref{fig:configfile}
for a small example.  The system uses the file to generate 
part of the system's lexer, a
collection of new \ir{} base types, and a series of type 
definitions that are incorporated into the final \pads{} specification.  

After tokenization, the algorithm enters the {\em structure discovery} 
phase.  This phase uses a top-down, divide-and-conquer
scheme to guess an approximate, initial format for the data.
To assess the quality of this initial approximation, the
algorithm computes an information-theoretic {\em score} for the
structure.  This score is used in the next phase to guide
{\em rewriting rules} that iteratively refine the format.  

Once the format has been refined to the extent possible, the resulting
\ir{} is printed in \pads{} syntax.  Template programs that use the
new description, including the \xml{}-converter, a simple
statistics tool we call the {\em accumulator}, a database tool
based on Oetiker's RRDTool~\cite{rrdtool}, and the \padx{}
query engine, are also all generated at this point.  Hence, starting
with data alone, our end-to-end algorithm quickly generates an entire suite
of fully functional data processing tools, ready to use at the command line.

The following subsections describe the main components of the algorithm
in more detail, including the structure discovery phase, the 
information-theoretic scoring function, the structure refinement phase
and the end products of the algorithm.

\begin{figure}
\begin{verbatim}
def triplet [0-9]{1,3}
def doublet [0-9]{1,2}
def hexdoub [0-9a-fA-F]{2}
...
exp Pemail {str1}@{hostname}
exp Pmac   ({hexdoub}(: | \-)){5}{hexdoub}
exp PbXML  \<([a-zA-Z])+\>
exp PeXML  \<\/[^>]+\>
\end{verbatim}
\caption{Tiny fragment of the default configuration file. 
The configuration command {\tt exp} specifies the definition on that
line should be  ``exported'' to
the learning tool whereas command {\tt def} implies the definition
is local and used only in other {\tt exp} or {\tt def} in the
configuration file.}
\label {fig:configfile}
\end{figure}

%% \subsection {Chunking and Tokenization}




%% \begin{figure}
%% \begin{center}
%% \begin{tabular}{|l|l|}
%% \hline
%% Name   &  Description               \\ \hline\hline
%% Pint  &   Integer \\ 
%% Palpha & Alpha-numeric string including '$\_$' and '$-$' \\
%% Pip & IP address \\
%% Pemail & Email address \\
%% Pmac & Mac address \\
%% Pdate & Simple date format \\
%% Ptime & Simple time format \\
%% Ppath & File system path \\
%% Phostname & Hostname \\
%% Purl  & URL \\
%% PbXML & Beginning XML tag \\
%% PeXML & Ending XML tag \\
%% Pother & Punctuation character \\\hline
%% \end{tabular}

%% \caption{Basic token types in default configuration.}
%% \label{figure:base-types}
%% \end{center}
%% \end{figure}
 



%%     *  mention system parameterization for multiple tokenizations for different domains
%%     * mention current skew towards systems data
%%     * ignore problems in this section -- save that for discussion/future work section
%%     * mention stream of tokens generated for running example 

\subsection {Structure Discovery}

Inspired in part by the work of Arasu on
information extraction from web pages~\cite{arasu+:sigmod03}. 

    *  role = quickly find a description in the "approximate area" of the correct description
    * explain structure of a generic "top-down" inference algorithm -- perhaps give pseudocode
    * explain our heuristics: generation of histograms, choice of struct, array, union, base type
    * grouping construct (introduce additional example as needed)
    * show (part of?) description of running example 

\subsection {Information-Theoretic Scoring}

    *  role = evaluate the "goodness" of the description relative to data
    * explain the information-theoretic principles
    * give the formulas 

\subsection {Structure Refinement}

%\input{rules}

    *  role = starting with the candidate structure, search for nearby descriptions that optimize an information theoretic scoring function
    * explain the 3 parts: value independent, value dependent, value independent
    * give (partial) list of rules used -- we need to work on notation for explaining these rules
    * illustrate several transformations using the running example
    * compare example after rewriting to the example from subsection 3.4 above
    * optional subsubsection: theory suggesting our algorithm is "correct" (we'd need a semantics for our IR then)

\input{refinement} 

\subsection {Finishing Up}

    * printing pads syntax and invoking toolchain
