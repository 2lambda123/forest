% \subsection {Algorithm Overview}

\begin{figure}
\begin{center}
\epsfig{file=archi.eps, width=\columnwidth}
\caption{Architecture of the automatic tool generation engine} \shrink
\label{fig-archi}
\end{center}
\end{figure}

Figure \ref{fig-archi} gives an overview of our automatic
tool generation architecture. The process 
%of generating a suite of data processing tools 
begins with raw data, shown in blue at the top left, which we pipe
into the format inference engine
(circumscribed by dotted lines in the picture).  
This engine produces a syntactically correct \pads{}
description for the data
through a series of phases:
chunking and tokenization, structure discovery, information-theoretic
scoring, and structure refinement.
The system then feeds the generated \pads{} description into the
\pads{} compiler.  The compiler generates libraries, which the system
then links to generic programs for various tasks including a data
analysis tool ({\em a.k.a.,} the {\em accumulator}) and an
ad-hoc-to-\xml{} translator.  At this point, users can apply these
generated tools to their original raw data or to other data with the
same format.
The following subsections describe the main components of the
inference algorithm in more detail.  We 
illustrate the effect of each phase on our running examples and
present the output of some of the generated tools.

\subsection{Chunking and Tokenization}
The learning system first divides the input data, which we refer
to as the {\em training set}, into {\em chunks} as specified by the
user. Intuitively, a chunk is a unit of repetition in the data source.
It is primarily by analyzing sequences of such chunks for commonalities
that we are able to infer data descriptions.  Our tool currently supports
chunking on a line-by-line basis as well as on a file-by-file basis.  

We use a lexer to break each chunk into a series of {\em simple
tokens}, which are intuitively atomic pieces of data such as
numbers, dates, times, alpha-strings, or punctuation symbols.
Every simple token has a corresponding
base type in the \ir{}, though the converse is not true -- there are 
base types that are not used as tokens.  Nevertheless, since
simple tokens have a very close correspondence with base types,
we often use the word {\em token} interchangeably with {\em base type}.
  
Parenthetical syntax, including quotation marks, curly braces, square brackets,
parentheses and \xml{} tags,
often provides very important hints about the structure
of an ad hoc data file.  Therefore, whenever the lexer encounters 
such parentheses, it creates a {\em meta-token}, which is a compound
token that 
represents the pair of parentheses and all the tokens 
within.\footnote{If parenthetical elements
are not well-nested, the meta-tokens are discarded and replaced with
ordinary sequences of simple tokens.}  For example, in Crashreporter.log,
the syntax \cd{[2164]} will yield the meta-token \cd{[*]} instead of
the sequence of three simple tokens \cd{[}, \cd{Pint}, and \cd{]}.  
The structure discovery algorithm eliminates all meta-tokens during
its analysis; whenever it encounters a context consisting of
matching meta-tokens, it cracks open the meta-tokens so it can analyze
the underlying structure.

Our learning system has a default tokenization scheme skewed toward systems
data, but users may specify a different scheme for their own domain
through a configuration file.  For example, computational biologists
may want to add DNA strings {\tt CATTGTT...} to the default tokenization 
scheme.  The configuration file is essentially
a list of name, regular expressions pairs.  The system uses the configuration
file to generate part of the system's lexer, a
collection of new \ir{} base types, and a series of type 
definitions that are incorporated into the final \pads{} specification.  

%% After tokenization, the algorithm enters the {\em structure discovery} 
%% phase.  This phase uses a top-down, divide-and-conquer
%% scheme to guess an approximate, initial format for the data.
%% To assess the quality of this initial approximation, the
%% algorithm computes an information-theoretic {\em score} for the
%% structure.  This score is used in the next phase to guide
%% {\em rewriting rules} that iteratively refine the format.  

%% Once the format has been refined to the extent possible, the resulting
%% \ir{} is printed in \pads{} syntax.  Template programs that use the
%% new description, including the \xml{}-converter, a simple
%% statistics tool we call the {\em accumulator}, a database tool
%% based on Oetiker's RRDTool~\cite{rrdtool}, and the \padx{}
%% query engine, are also all generated at this point.  Hence, starting
%% with data alone, our end-to-end algorithm quickly generates an entire suite
%% of fully functional data processing tools, ready to use at the command line.



% \begin{figure}
% \begin{verbatim}
% def triplet [0-9]{1,3}
% def doublet [0-9]{1,2}
% def hexdoub [0-9a-fA-F]{2}
% ...
% exp Pemail {str1}@{hostname}
% exp Pmac   ({hexdoub}(: | \-)){5}{hexdoub}
% exp PbXML  \<([a-zA-Z])+\>
% exp PeXML  \<\/[^>]+\>
% \end{verbatim}
% \caption{Tiny fragment of the default configuration file. 
% The configuration command {\tt exp} specifies the definition on that
% line should be  ``exported'' to
% the learning tool whereas command {\tt def} implies the definition
% is local and used only in other {\tt exp} or {\tt def} in the
% configuration file.}
% \label {fig:configfile}
% \end{figure}

%% 




%% \begin{figure}
%% \begin{center}
%% \begin{tabular}{|l|l|}
%% \hline
%% Name   &  Description               \\ \hline\hline
%% Pint  &   Integer \\ 
%% Palpha & Alpha-numeric string including '$\_$' and '$-$' \\
%% Pip & IP address \\
%% Pemail & Email address \\
%% Pmac & Mac address \\
%% Pdate & Simple date format \\
%% Ptime & Simple time format \\
%% Ppath & File system path \\
%% Phostname & Hostname \\
%% Purl  & URL \\
%% PbXML & Beginning XML tag \\
%% PeXML & Ending XML tag \\
%% Pother & Punctuation character \\\hline
%% \end{tabular}

%% \caption{Basic token types in default configuration.}
%% \label{figure:base-types}
%% \end{center}
%% \end{figure}
 



%%     *  mention system parameterization for multiple tokenizations for different domains
%%     * mention current skew towards systems data
%%     * ignore problems in this section -- save that for discussion/future work section
%%     * mention stream of tokens generated for running example 

\subsection {Structure Discovery}

Given a collection of tokenized chunks, the goal of the structure
discovery phase is to quickly find a candidate description ``close'' to
a good final solution.  The rewriting phase then analyzes, refines and
tranforms this candidate to produce the final description.
The high-level form of our structure discovery algorithm was
inspired by the work of Arasu and 
Garcia-Molina on information extraction from web pages~\cite{arasu+:sigmod03};
however, the context, goals and algorithmic details of our
work are entirely different.

%However, the details are
%entirely different as Arasu's algorithm operates over well-formed
%\html{} trees whereas our work focuses on the mucky world of ad hoc
%data.

\paragraph*{Structure Discovery Basics.}
Our algorithm operates by analyzing the collection of tokenized chunks
and guessing what the top-level type constructor should be.  Based on
this guess, it partitions the chunks and recursively analyze each partition
to determine the best description for that partition.
Figure~\ref{fig:structure-discovery} outlines the
overall procedure in Pseudo-ML.  The \cd{oracle} function,
whose implementation we hide for now, does most of the hard work by
conjuring one of four different sorts of prophecies.  

The \cd{BaseProphecy} simply reports that the top-level type
constructor is a particular base type.

The \cd{StructProphecy} specifies that the top-level description is a
struct with $k$ fields.  It also
specifies a list, call it \cd{css}, with $k$ elements.  The
$i^{\mathrm{th}}$ element in \cd{css} is the list of chunks
corresponding to the $i^{\mathrm{th}}$ field of the struct.  The
oracle derives these chunk lists from its original input. More
specifically, if the oracle guesses there will be $k$ fields, then
each original chunk is partitioned into $k$ pieces. The
$i^{\mathrm{th}}$ piece of each original chunk is used to recursively
infer the type of the $i^{\mathrm{th}}$ field of the struct.

The \cd{ArrayProphecy} specifies that the top-level structure involves
an array.  However, predicting exactly where an array begins and ends
is difficult, even for the magical oracle.  Consequently, the
algorithm actually generates a three-field struct, where the first
field allows for slop prior to the array, the middle field is the
array itself, and the last field allows for slop after the array.  If
the slop turns out to be unnecessary, the rewriting rules will clean
up the mess in the next phase.

Finally, the \cd{UnionProphecy} specifies that the top-level structure
is a union type with $k$ branches.  Like a \cd{StructProphecy}, the
\cd{UnionProphecy} carries a chunks list, with one element for each
branch of the union.  The algorithm uses each element to recursively
infer a description for the corresponding branch of the union. 
Intuitively, the oracle produces the union chunks list by ``horizontally''
partitioning the input chunks, whereas it partitions struct chunks
``vertically'' along field boundaries. 

As an example, recall the Crashreporter.log data from
Figure~\ref{fig:example}.  Assuming a chunk is a line of data, the two
chunks in the example consist of the token sequences
(recall \cd{[*]} and \cd{(*)} are meta-tokens):

{\small
\begin{verbatim}
Pdate ' ' Ptime ' ' Pint ' ' Palpha [*] ':' ...
'-' ' ' Palpha [*] ':' ' ' Palpha (*) ' ' ...
\end{verbatim}
}%
\noindent
Given these token sequences, 
the oracle will predict that the top-level type constructor is a struct 
with three fields: one for the tokens before the token \cd{[*]}, one
for the \cd{[*]} tokens themslves, and one for the tokens after the token
\cd{[*]}. Based on this determination, the oracle will divide the
original chunks into three sets as follows.

{\small
\begin{verbatim}
Pdate ' ' Ptime ' ' Pint ' ' Palpha        (set 1)       
'-' ' ' Palpha 

[*]                                        (set 2)       
[*]

':' ...                                    (set 3)       
':' ' ' Palpha (*) ' ' ...
\end{verbatim}
}%

\noindent
On recursive analysis of set 1, the oracle again suggests a struct is the top-level type,
generating two more sets of chunks: 

{\small
\begin{verbatim}
Pdate ' ' Ptime ' ' Pint ' '               (set 4)       
'-' ' '

Palpha                                     (set 5)       
Palpha 
\end{verbatim}
}%

\noindent
Now, since every chunk in set 5 contains exactly one base type
token, the recursion bottoms out with the oracle claiming it has
found the base type \cd{Palpha}.  When analyzing set 4, 
the oracle detects insufficient commonality between chunks and decides
the top-most type constructor is a union. It partitions set 4 into
two more sets, with each group containing only 1 chunk (either
\{\cd{Pdate ' ' ...}\} or \{\cd{'-' ' '}\}).  The algorithm analyzes
the first set to determine the type of the first branch of the union
and the second set to determine the second branch of the union.
With no variation in either branch, the algorithm quickly discovers an
accurate type for each. 

Having completely discovered the type of the data in set 1, we turn
our attention to set 2. 
To analyze this set, the algorithm cracks open the \cd{[*]} meta-tokens
to recursively analyze the underlying data, a process which yields 
\cd{struct \{'['; Pint; ']';\}}. Analysis of Set 3 proceeds in a similar 
fashion.

%% The types $T_0$, $T_1$, $T_2$ and $T_3$ are not yet known -- they will
%% be constructed recursively.  $T_0$ is constructed by recursively applying
%% the discovery procedure to the data in the chunks that precedes
%% the first occurrence of \cd{Pdate}.  In this case, there is no such
%% data, so the inference procedure returns the trivial type \cd{Pempty} for
%% $T_0$.  $T_1$ is constructed by applying discovery to the data between
%% \cd{Pdate} and \cd{Ptime}.  Each chunk contains one token in this case,
%% the space token.  Hence the discovery procedure easily decides
%% $T_1$ is \cd{' '}. $T_2$ is discovered by recursively analyzing the following
%% two chunks.
%% {\small
%% \begin{verbatim}
%%    ' ' Pint ' ' Palpha '[' Pint ']' 
%%    ' ' Pint ' ' Palpha '[' Pint ']'
%% \end{verbatim}
%% }
%% \noindent
%% Finally, the structure of $T_3$ is discovered by recursively analyzing 
%% all data following the ':' token.

As a second example, consider the Sirius data from Figure~\ref{fig:example}.
Here the chunks have the following structure:

{\small
\begin{verbatim}
   Pint '|' Pint '|' ... '|' Pint '|' Pint
   Pint '|' Pint '|' ... '|' Palpha Pint '|' Pint
\end{verbatim}
}%

\noindent
In this case, the oracle prophecies that the top-level structure involves
an array and partitions the data into sets of chunks for the
array preamble, the array itself, and the array postamble.  The
preamble chunks all have the form \{\cd{Pint '|'}\} while the
postamble chunks all have the form \{\cd{Pint}\}, so the algorithm
easily determines the corresponding types. The algorithm discovers the
type of the array elements by analyzing the residual list of chunks

{\small
\begin{verbatim}
Pint '|' 
... 
Pint '|' 
Pint '|'
...
Palpha Pint '|'
\end{verbatim}
}%

\noindent
The oracle constructs this chunk list by removing the preamble and
postamble tokens from all input chunks, concatenating the
remaining tokens, and then splitting the resulting list into one chunk
per array element. It does this splitting by assuming that the chunk
for each array element ends with a \cd{'|'} token.  

So far so good, but how does the guessing work?  Why does the
algorithm decide the Sirius data is basically an array but 
Crashreporter.log is a struct? After all, the Sirius chunks all have
a \cd{Pint}, just as all the Crashreporter.log chunks have a bracket
meta-token \cd{[*]}. Likewise, Crashreporter.log contains many occurrences of
the \cd{' '} token, which might serve as
an array separator as the \cd{'|'} token does in the Sirius data.

\input{discoveryalg2.tex}

\paragraph*{The Magic.}
To generate the required prophecy for a given list of chunks, the
oracle computes a histogram of the frequencies of all tokens appearing
the input.  
More specifically, the histogram for token $t$
plots the number of chunks (on the $y$-axis)
having a certain number of occurrences of the token (on the $x$-axis). 
Figure~\ref{fig:histograms} presents a number of histograms computed
during analysis of the Crashreporter.log and Sirius chunk lists.

Intuitively, tokens associated 
with histograms with high {\em coverage}, meaning the token appears
in almost every chunk, and {\em narrow} distribution, meaning the variation in
the number of times a token appears in different chunks is low, are
good candidates for defining structs.  Similarly, histograms with
high coverage and {\em wide} distribution are good candidates for defining
arrays.  Finally, histograms with low coverge or intermediate width
represent tokens that form part of a union.  


\begin {figure*}
\begin{center}
\begin{minipage}[t]{0.59\columnwidth}
\epsfig{file=histogram1.eps, width=\columnwidth}
\end{minipage}
\hfill
\begin{minipage}[t]{0.53\columnwidth}
\epsfig{file=histogram2.eps, width=\columnwidth}
\end{minipage}
\hfill
\begin{minipage}[t]{0.295\columnwidth}
\epsfig{file=histogram3.eps, width=\columnwidth}
\end{minipage}
\hfill
\begin{minipage}[t]{0.59\columnwidth}
\epsfig{file=histogram4.eps, width=\columnwidth}
\end{minipage}
%
%a) histogram for ' '
%histogram for ':'
%...
%
%Omit telling the reader which histograms are for which tokens in the figure.
%We'll explain in the text...
%
%May 6 of them 3 each from Crashreporter.log and Sirius?
%We'll pick carefully.
\caption{Histograms (a), (b), (c), (d), (e), (f) and
(g) are generated from top-level analysis of Crashreporter.log tokens.
The corresponding tokens are (a) {\tt [*]}, 
(b)  {\tt Pint}, (c) {\tt PDate}, (d) {\tt PTime}, (e) {\tt -}, (f) {\tt Palpha} and
(g) {\tt Pwhite}.  Histograms (h) {\tt Palpha}, (i) {\tt Pint}, and 
(j) {\tt Pwhite} are generated from analysis of Crashreporter.log from
set 1 (the second level of recursion).  Histogram (k) is generated from
top-level analysis of the {\tt |} token from the Sirius data.} \shrink
\label{fig:histograms} 
\end{center}
\end{figure*}

Concretely, consider histogram (a)
from Figure~\ref{fig:histograms}.  It is a perfect struct candidate--
it has a single column that covers 100\% of the records.  Indeed,
this histogram corresponds to the \cd{[*]} token in Crashreporter.log.
Whenever the oracle detects such a histogram, it will always prophecy
a struct and partition the input chunks according to the associated
token. All of the other top-level histograms for Crashreporter.log
contain variation and hence are less certain indicators of data source
structure. 

As a second example, consider the top-level histograms (f), (b) and
(g) for tokens \cd{Palpha}, \cd{Pint} and \cd{Pwhite}, respectively, 
and compare them with the corresponding histograms (h),
(i) and (j) computed for the same tokens from chunk set 1, defined in the
previous subsection.  The histograms for chunk set 1 have far less variation
than the corresponding top-level histograms.  In particular, notice
that histogram (h) for token \cd{Palpha} is a perfect struct histogram
whereas histogram (f) for token \cd{Palpha}
contains a great deal of variation.  This example illustrates the
source of the power of our divide-and-conquer algorithm-- if the
oracle can identify {\em even one token} at a given level as
defining a good partition for the data, the histograms for the next
level down become substantially sharper and more amenable to analysis.

As a third example, consider histogram (k).  This histogram illustrates
the classic pattern for tokens involved in arrays-- it has a very long
tail.  And indeed, the \cd{|} token in the Sirius data does act like a
separator for fields of an array. 

To make the intuitions discussed above precise, we must define a number of
properties of histograms.  First, a histogram $h$ for a token $t$ is a list of pairs
of natural numbers $(x,y)$ where $x$ denotes the token frequency and
$y$ denotes the number of chunks with that frequency.  
All first elements of pairs in the list must be unique.  
The {\em width} of a
histogram ({\em width}($h$)) is the number of elements in the list
excluding the zero-column (\ie{} excluding element $(0,y)$).  
A histogram
$\normal{h}$ is in normal form when the first element of the list is
the zero column and all subsequent elements are sorted in descending
order by the $y$ component.  For example, if $h_1$ is the histogram
$[(0,5), (1,10), (2,25), (3,15)]$ then {\em width}($h_1$) is 3 and its
normal form $\normal{h_1}$ is $[(0,5), (2, 25), (3,15), (1,10)]$.

We often refer to $y$ as the {\em mass} of the element $(x,y)$,
and given a histogram $h$, we refer to the mass of the $i^{\mathrm th}$
element of the list 
using the notation $h[i]$.  For instance, $h_1[3] = 15$ and 
$\normal{h_1}[3] = 10$.  The {\em residual mass} ({\em rm}) of a column $i$ in 
a normalized histogram $h$ is the mass of all the columns to the right of 
$i$ plus the mass of the zero-column.  Mathematically, 
{\em rm}$(\normal{h},i) = \normal{h}[0] + \sum_{j=i+1}^{\mathit{width}(\normal{h})} \normal{h}[j]$.
For example, {\em rm}$(\normal{h_1},1) = 5 + 15 + 10 = 30$.
The residual mass characterizes the ``narrowness''
of a histogram.  Those histograms with low residual mass of the first
column ({\em i.e.,} {\em rm}$(\normal{h_1},1)$ is small) 
are good candidates for structs.

To distinguishing between structs, arrays and unions,
we also need to define the {\em coverage} of a histogram, which
intuitively is the number of chunks containing the corresponding token.
Mathematically, it is simply the sum of the non-zero histogram elements:
{\em coverage}$(\normal{h}) = 
                  \sum_{j=1}^{\mathit{width}(\normal{h})} \normal{h}[j]$.

Finally, our algorithm works better when the oracle considers groups
of tokens with similar distributions together because with very high
probability such tokens form part of the same type constructor. 
To determine when two histograms are {\em similar}, we use
a symmetric form of {\em relative entropy} \cite{Lin91:divergence}.
%\footnote{Suggested by Rob Schapire, personal communication, May 2007.}  
The (plain) relative entropy
of two normalized histograms $\normal{h_1}$ and $\normal{h_2}$, 
written \relativee{\normal{h_1}}{\normal{h_2}}, is
defined as follows.
\[
 \relativee{\normal{h_1}}{\normal{h_2}} 
   = \sum_{j=1}^{\mathit{width}(\normal{h_1})} \normal{h_1}[j]*log(\normal{h_1}[j]/\normal{h_2}[j])
\]
To create a symmetric form, we first find the average of the two
histograms in question (written $\addh{h_1}{h_2}$)
by summing corresponding columns and dividing by two.  This technique
prevents the denominator from being zero in the 
final relative entropy computation.  Using this definition, the symmetric
relative entropy is:
\[
 \srelativee{\normal{h_1}}{\normal{h_2}} 
   = \frac{1}{2}  \relativee{\normal{h_1}}{\addh{\normal{h_1}}{\normal{h_2}}}
   +  \frac{1}{2}  \relativee{\normal{h_2}}{\addh{\normal{h_1}}{\normal{h_2}}}
\]

Now that we have defined the relevant properties of histograms,
we can explain how the oracle prophecies given a list of chunks.

\begin {enumerate}
\item Prophecy a base type when each chunk contains the same simple
  token. If each chunk contains the same meta-token, prophecy a struct
  with three fields: one for the left paren, one for the body, and one
  for the right paren.

\item Otherwise, compute normalized histograms for the input and group
  related ones into clusters using agglomerative clustering:  A
  histogram $h_1$ belongs to group $G$ provided there exists another
  histogram $h_2$ in $G$ such  that
  $\srelativee{\normal{h_1}}{\normal{h_2}} <  \mathrm{ClusterTolerance}$.  
  where $\mathrm{ClusterTolerance}$ is a parameter of the algorithm. We do not
  require all histograms in a cluster to 
  have precisely the same histogram to allow for errors in the data.
  A histogram dissimilar to all others will form its own group.
  We have found a $\mathrm{ClusterTolerance}$ of 0.01 is effective.  

\item Determine if a struct exists by first ranking the groups by the
  minimum residual mass of all the histograms in each group. Find the first group in this
  ordering with histograms $h$ satisfying the following criteria:
\begin {itemize}
\item $\mathit{rm}(h) < \mathrm{MaxMass}$
\item $\mathit{coverage}(h) > \mathrm{MinCoverage}$
\end{itemize}
where constants $\mathrm{MaxMass}$ and $\mathrm{MinCoverage}$ are
parameters of the algorithm.  
This process favors groups of histograms with high
coverage and narrow distribution.  
If histograms $h_1$, \ldots, $h_n$ from
group $G$ satisfy the struct criteria, the oracle will prophecy some
form of struct. It uses the histograms $h_1$, \ldots, $h_n$ and the associated
tokens $t_1$, \ldots, $t_n$ to calculate the number of fields and the
corresponding chunk lists.  We call $t_1$, \ldots, $t_n$ the {\em
  identified} tokens for the input.
Intuitively, for each input chunk, the oracle puts
all tokens upto but not including the first token $t$ from the set of
identifed tokens into the chunk list for the first field.  It puts $t$ in
the chunk list for the second field. It puts all tokens upto the next
identified token into the chunk list for the third field and so on. Of
course, the identified tokens need not appear in the same order in all
input chunks, nor in fact must they all appear at all.  To handle
this variation when it occurs, the oracle prophecies a union instead
of a struct, with one branch per token ordering and one branch for all
input chunks that do not have the full set of identified tokens.


\item Identify an array by sorting all groups in descending 
order by coverage of the highest coverage histogram in the group.
Find the first group in this ordering with any histograms that satisfy
the following minimum criteria:
\begin {itemize}
\item $\mathit{width}(h) > 3$
\item $\mathit{coverage}(h) > \mathrm{MinCoverage}$
\end{itemize}
This process favors histograms with wide distribution and high coverage.
If histograms $h_1$, \ldots, $h_n$ with corresponding tokens 
$t_1$, \ldots, $t_n$ satisfy the array critera, the oracle will
prophecy an array.   It will 
partition each input chunk into (1) a preamble subsequence 
that contains the first occurrence of each identified token, 
(2) a set of element subsequences, with each subsequence containing
one occurrence of the identified tokens, and
(3) a postamble subsequence that contains any remaining tokens from
the input chunk.

\item If no other prophecy applies, identify a union. Partition
  the input chunks according to the first token in each chunk. 
\end{enumerate}

%%     *  role = quickly find a description in the "approximate area" of the correct description
%%     * explain structure of a generic "top-down" inference algorithm -- perhaps give pseudocode
%%     * explain our heuristics: generation of histograms, choice of struct, array, union, base type
%%     * grouping construct (introduce additional example as needed)
%%     * show (part of?) description of running example 

\subsection {Information-Theoretic Scoring}
\label{sec:score}

\input{scoring}

\subsection {Structure Refinement}

%\input{rules}

%    *  role = starting with the candidate structure, search for nearby descriptions that optimize an information theoretic scoring function
%    * explain the 3 parts: value independent, value dependent, value independent
%    * give (partial) list of rules used -- we need to work on notation for explaining these rules
%    * illustrate several transformations using the running example
%    * compare example after rewriting to the example from subsection 3.4 above
%    * optional subsubsection: theory suggesting our algorithm is "correct" (we'd need a semantics for our IR then)

\input{refinement} 

\subsection {Final Products}

The previous subsections outline the central technical elements of our
algorithms.  The main tasks remaining include converting the internal
representation into a syntactically correct \pads{} description, feeding
the generated description to the \pads{} compiler and producing a 
collection of scripts that conveniently package the freshly-generated 
libraries with the \pads{} run-time system and tools.  At the end of 
this process, users have a number of programming libraries
and many powerful tools at their disposal.
Perhaps the most powerful tools are the \padx{} query 
engine~\cite{fernandez+:padx} and the \xml{} converter, which allow  
users to write arbitrary XQueries over the data source
or to convert the data to \xml{} for use by other software. Other
useful tools include the accumulator tool, mentioned earlier,
converters to translate data into a form suitable for
loading into a relational database or Excel spreadsheet, and
a custom graphing tool that pushes data into \cd{gnuplot} for data
visualization.
\figref{fig:final-products} gives snapshots of the output of a couple
of these tools. 

\begin{figure}
\begin{center}
{\small
\begin{verbatim}
<Struct_114>
  <var_7>
    <var_6>
      <var_0><val>Wed Jun 21</val></var_0>
      <var_2><val>10:24:44</val></var_2>
      <var_4><val>2006</val></var_4>
    </var_6>
  </var_7>
  <var_11><val>crashdump</val></var_11>
  <var_14><val>1524</val></var_14>
  <var_19><val>crashdump</val></var_19>
  <var_63>
    <var_113_crashdump19>
      <var_68>
      </var_68>
      <var_73><val>started</val></var_73>
      <var_75>
      </var_75>
    </var_113_crashdump19>
  </var_63>
</Struct_114>
\end{verbatim}
}

(a)

\epsfig{file=ai.3000.eps, width=0.9\columnwidth} 

(b)
\caption{(a) XML output snipet of crashreporter.log  (b) ai.3000 web transaction volumn 
during 19:00 - 08:55(+1 day) visualization} \shrink
\end{center}
\label{fig:final-products}
\end{figure}

%Of course, the end products of the inference algorithm 
%also include the generated description and the
%programming libraries including parser and printer.
