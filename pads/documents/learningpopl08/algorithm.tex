% \subsection {Algorithm Overview}

\begin{figure}
\begin{center}
\epsfig{file=archi.eps, width=\columnwidth}
\caption{Architecture of the automatic tool generation engine}
\vspace*{-5mm}
\label{fig-archi}
\end{center}
\end{figure}

Figure \ref{fig-archi} gives an overview of our automatic
tool generation architecture. The process of generating a new
suite of data processing tools 
begins with raw data, shown in blue at the top left.  The
raw data is first piped into our format inference engine
(circumscribed by dotted lines in the picture).  The
format inference engine is implemented as a series of phases that 
eventually produce a syntactically correct \pads{} description
for the data.  Next, the generated \pads{} description is fed into the 
\pads{} compiler.  The compiler generates libraries that are linked to generic
programs for various tasks including a data analysis tool
({\em a.k.a.,} the {\em accumulator}) and an ad-hoc-to-\xml{} translator.
At this point, the user can apply these generated tools to their original 
raw data, or to other data with the same format.  

The following subsections describe the main components of the
inference algorithm in more detail, including chunking and
tokenization, the structure discovery phase, the information-theoretic
scoring function and the structure refinement phase.  We also
illustrate the action of each phase on our running examples and
present the final end products.

\subsection{Chunking and Tokenization}

The input data, which we also refer to as the {\em training set}, is first
divided into {\em chunks} as specified by the user.  Intuitively, a chunk
is a unit of repetition in the data source.  It is primarily by analyzing
a large number of repeated syntax elements that we are able to infer a usable
description for a data source.  Our tool currently supports
chunking on a line-by-line basis as well as on a file-by-file basis.  

Each chunk is broken down into a series of {\em simple tokens} by a lexer.
Each simple token can be a punctuation symbol, a number, a date, a time, or a
number of other basic items.  Every simple token type has a corresponding
base type in the \ir{}, though the converse is not true -- there are a
number of base types that are not used as tokens.  Nevertheless, since
simple tokens have a very close correspondence with base types,
we often use the word {\em token} interchangeably with {\em base type}.
  
Parenthetical syntax including quotation marks, curly braces, square brackets,
parentheses and \xml{} tags
often provides very important hints about the structure
of an ad hoc data file.  Therefore, whenever the lexer encounters 
a corresponding pair of parentheses, it 
creates what we call a {\em meta-token}, which is a single token that
represents the pair of parentheses and all simple tokens 
within.\footnote{If parenthetical elements
are not well-nested, the meta-tokens are discarded and replaced with
ordinary sequences of simple tokens.}  For example, in Crashreporter.log,
the syntax \cd{[2164]} will form the meta-token we write \cd{[*]} as
opposed to the three simple tokens \cd{[}, \cd{Pint}, and \cd{]}.  Meta-tokens
do not stay intact throughout the inference procedure:  As the structure 
discovery algorithm encounters each meta-token, it is cracked open and its 
underlying structure is analyzed.

Our learning system has a default tokenization scheme skewed toward systems
data, but users may specify a different scheme for their own domain
through a configuration file.  For example, computational biologists
may want to add DNA strings {\tt CATTGTT...} to the default tokenization 
scheme.  The configuration file is essentially
a list of name-regular expressions pairs -- see Figure~\ref{fig:configfile}
for a small example.  The system uses the file to generate 
part of the system's lexer, a
collection of new \ir{} base types, and a series of type 
definitions that are incorporated into the final \pads{} specification.  

%% After tokenization, the algorithm enters the {\em structure discovery} 
%% phase.  This phase uses a top-down, divide-and-conquer
%% scheme to guess an approximate, initial format for the data.
%% To assess the quality of this initial approximation, the
%% algorithm computes an information-theoretic {\em score} for the
%% structure.  This score is used in the next phase to guide
%% {\em rewriting rules} that iteratively refine the format.  

%% Once the format has been refined to the extent possible, the resulting
%% \ir{} is printed in \pads{} syntax.  Template programs that use the
%% new description, including the \xml{}-converter, a simple
%% statistics tool we call the {\em accumulator}, a database tool
%% based on Oetiker's RRDTool~\cite{rrdtool}, and the \padx{}
%% query engine, are also all generated at this point.  Hence, starting
%% with data alone, our end-to-end algorithm quickly generates an entire suite
%% of fully functional data processing tools, ready to use at the command line.



\begin{figure}
\begin{verbatim}
def triplet [0-9]{1,3}
def doublet [0-9]{1,2}
def hexdoub [0-9a-fA-F]{2}
...
exp Pemail {str1}@{hostname}
exp Pmac   ({hexdoub}(: | \-)){5}{hexdoub}
exp PbXML  \<([a-zA-Z])+\>
exp PeXML  \<\/[^>]+\>
\end{verbatim}
\caption{Tiny fragment of the default configuration file. 
The configuration command {\tt exp} specifies the definition on that
line should be  ``exported'' to
the learning tool whereas command {\tt def} implies the definition
is local and used only in other {\tt exp} or {\tt def} in the
configuration file.}
\label {fig:configfile}
\end{figure}

%% 




%% \begin{figure}
%% \begin{center}
%% \begin{tabular}{|l|l|}
%% \hline
%% Name   &  Description               \\ \hline\hline
%% Pint  &   Integer \\ 
%% Palpha & Alpha-numeric string including '$\_$' and '$-$' \\
%% Pip & IP address \\
%% Pemail & Email address \\
%% Pmac & Mac address \\
%% Pdate & Simple date format \\
%% Ptime & Simple time format \\
%% Ppath & File system path \\
%% Phostname & Hostname \\
%% Purl  & URL \\
%% PbXML & Beginning XML tag \\
%% PeXML & Ending XML tag \\
%% Pother & Punctuation character \\\hline
%% \end{tabular}

%% \caption{Basic token types in default configuration.}
%% \label{figure:base-types}
%% \end{center}
%% \end{figure}
 



%%     *  mention system parameterization for multiple tokenizations for different domains
%%     * mention current skew towards systems data
%%     * ignore problems in this section -- save that for discussion/future work section
%%     * mention stream of tokens generated for running example 

\subsection {Structure Discovery}

Given a collection of tokenized chunks, the goal of the structure
discovery phase is to find a candidate description ``nearby'' a good
final solution.  In a later phase of the algorithm, the rewriting rules 
analyze, refine
and transform the candidate to produce that final solution.  The
high-level structure of the algorithm we use to discover candidate
descriptions was inspired by the work of Arasu and
Garcia-Molina on information extraction from web pages~\cite{arasu+:sigmod03}.
However, the context, goals and algorithmic details involved in our
work are entirely different.

%However, the details are
%entirely different as Arasu's algorithm operates over well-formed
%\html{} trees whereas our work focuses on the mucky world of ad hoc
%data.

\paragraph*{Structure Discovery Basics.}
Our algorithm operates by analyzing the collection of tokenized chunks
and guessing what the top-level type constructor is.  Based on this guess,
it will partition the chunks and recursively analyze each partition
to determine what the component types are.
Figure~\ref{fig:structure-discovery} presents an outline of how the
overall procedure operates.  The \cd{oracle} function,
whose implementation we leave hidden for now,
does most of the hard work in the algorithm by conjuring up 
one of four different sorts of prophecies.  

The \cd{BaseProphecy} simply reports that the top-level type
constructor is some specified base type.

The \cd{StructProphecy} specifies that the top-level description is a
struct.  In addition, if the struct is to have $k$ fields then the
\cd{StructProphecy} will carry a list (call it \cd{css}) with $k$
elements.  The $i^{\mathrm{th}}$ element in the list is a set of
chunks used recursively to discover the type of the $i^{\mathrm{th}}$
field of the struct.  These sets of chunks are derived from the
original input to the oracle function. More specifically, if the
oracle guesses there will be $k$ fields in the struct, then each original
chunk is partitioned into $k$ pieces. The $i^{\mathrm{th}}$ piece of
each original chunk is used to infer the $i^{\mathrm{th}}$ field of
the struct.

The \cd{ArrayProphecy} specifies that the top-level structure will
involve an array.  However, at this point in the inference algorithm,
predicting exactly where an array begins and ends is difficult, even
for the magical oracle.  Consequently, the algorithm actually
generates a three-field struct, where the first field allows for slop
prior to beginning the array, the middle field is the array itself,
and the last field allows for slop after the array is over.  If this
slop turns out to be unnecessary, the rewriting rules will clean up
the mess in the next phase.

Finally, the \cd{UnionProphecy} specifies that the top-level structure will
involve a union type.  Like a \cd{StructProphecy}, the \cd{UnionProphecy}
carries a \cd{chunks list} with it and each element of the list is 
used recursively to infer a branch of the union.  Merging all elements
of the list will result in the original input.

As an example, recall the Crashreporter.log data presented in
Figure~\ref{fig:example}.  After tokenization, assuming a chunk is
a line of data, the two chunks from the example contain the following 
token sequences (recall \cd{[*]} and \cd{(*)} are meta-token encapsulating
everything within the respective parens):
{\small
\begin{verbatim}
Pdate ' ' Ptime ' ' Pint ' ' Palpha [*] ':' ...
'-' ' ' Palpha [*] ':' ' ' Palpha (*) ' ' ...
\end{verbatim}
}
\noindent
Given these token sequences, 
our oracle will guess that the top-level type constructor is a struct 
with three fields and divide up our original chunks into three sets
as follows.
{\small
\begin{verbatim}
Pdate ' ' Ptime ' ' Pint ' ' Palpha        (set 1)       
'-' ' ' Palpha 

[*]                                        (set 2)       
[*]

':' ...                                    (set 3)       
':' ' ' Palpha (*) ' ' ...
\end{verbatim}
}
\noindent
On recursive analysis of set 1, the oracle again suggests a struct is the top-level structure,
generating two more sets of chunks: 
{\small
\begin{verbatim}
Pdate ' ' Ptime ' ' Pint ' '               (set 4)       
'-' ' '

Palpha                                     (set 5)       
Palpha 
\end{verbatim}
}
\noindent
Now, since every chunk in set 5 contains exactly one base type
token, the recursion naturally bottoms out with the oracle claiming it has
found the base type \cd{Palpha}.  On the other hand, when analyzing set 4, 
the oracle detects insufficient commonality between chunks and decides
the top-most type constructor must be a union. It recursively analyzes 
the first chunk in set 4 to determine the type of one 
branch of the union and the second chunk in set 4 to determine the type of
the other branch of the union.  With no variation in either branch,
the algorithm quickly discovers an accurate type for each.

Backing out of the recursion, having completely discovered the
type of the data in set 1, we turn our attention to set 2.
On recursive analysis of this set, the meta-tokens are cracked open
and this piece of the data is recursively identified as having the type 
\cd{struct \{'['; Pint; ']';\}}.  Analysis of Set 3 proceeds in a similar 
fashion to the others.

%% The types $T_0$, $T_1$, $T_2$ and $T_3$ are not yet known -- they will
%% be constructed recursively.  $T_0$ is constructed by recursively applying
%% the discovery procedure to the data in the chunks that precedes
%% the first occurrence of \cd{Pdate}.  In this case, there is no such
%% data, so the inference procedure returns the trivial type \cd{Pempty} for
%% $T_0$.  $T_1$ is constructed by applying discovery to the data between
%% \cd{Pdate} and \cd{Ptime}.  Each chunk contains one token in this case,
%% the space token.  Hence the discovery procedure easily decides
%% $T_1$ is \cd{' '}. $T_2$ is discovered by recursively analyzing the following
%% two chunks.
%% {\small
%% \begin{verbatim}
%%    ' ' Pint ' ' Palpha '[' Pint ']' 
%%    ' ' Pint ' ' Palpha '[' Pint ']'
%% \end{verbatim}
%% }
%% \noindent
%% Finally, the structure of $T_3$ is discovered by recursively analyzing 
%% all data following the ':' token.

As a second example, consider the Sirius data from Figure~\ref{fig:example}.
Here the chunks have the following structure:
{\small
\begin{verbatim}
   Pint '|' Pint '|' ... '|' Pint '|' Pint
   Pint '|' Pint '|' ... '|' Palpha Pint '|' Pint
\end{verbatim}
}
\noindent
In this case, our algorithm guesses that the top-level structure involves
an array and hence partions the data into sets of chunks for the
array preamble, the array itself, and the array postamble.  The
preamble chunks have the form \cd{Pint '|'} and the
postamble chunks have the form \cd{Pint}, so their types easily discovered.  
The type for the array elements will be discovered and through analysis of the
following list of chunks.  The oracle managed to split the two
large initial chunks into a series of many smaller array-element chunks
by noticing the \cd{'|'} token might serve as an array separator.
{\small
\begin{verbatim}
Pint '|' 
... 
Pint '|' 
Pint '|'
...
Palpha Pint '|'
\end{verbatim}
}
So far so good, but how does the guessing work?  Why does the
algorithm decide the Sirius data is basically an array but the
Crashreporter.log is a struct? After all, all the Sirius chunks start
with a Pint, which is similar to all the Crashreporter chunks starting
with a Pdate.  Likewise, Crashreporter contains many occurrences of
the \cd{' '} token, which, like the \cd{'|'} in Sirius, might serve as
an array separator.

\paragraph*{The Magic.}
In order to make informed guesses about the structure of the chunks
under consideration, every recursive iteration of the algorithm computes 
a histogram for each token appearing in the data.
More specifically, a histogram for token $t$
plots the number of chunks (on the $y$-axis)
having a certain number of occurrences of the token (on the $x$-axis). 
Figure~\ref{fig:histograms} presents a number of histograms computed
from the top-level analysis of Crashreporter.log and Sirius chunks.

Intuitively, histograms with high {\em coverage}, meaning the token appears
in almost every chunk and has a {\em narrow} distribution, meaning the variation in
the number of times a token appears in different chunks is low, are
good candidates for defining structs.  Similarly, histograms with
high {\em coverage} and wide distribution, are good candidates for defining
arrays.  Finally, histograms with low coverge or intermediate width
represent tokens that form part of a union.  For example, histogram (a)
from Figure~\ref{fig:histograms} ... {\em explain once we have data}

To make the intuitions above precise, we must define a number of
properties of histograms.  First, a histogram $h$ is a list of pairs
of natural numbers $(x,y)$ where $x$ denotes the token frequency and
$y$ denotes the number of records with that frequency.  
All first elements of pairs in the list must be unique.  
The {\em width} of a
histogram ({\em width}($h$)) is the number of elements in the list
excluding the zero-bar ({i.e.,} excluding element $(0,y)$).  
A histogram
$\normal{h}$ is in normal form when the first element of the list is
the zero column and all subsequent elements are sorted in descending
order by the $y$ component.  For example, if $h_1$ is the histogram
$[(0,5), (1,10), (2,25), (3,15)]$ then {\em width}($h_1$) is 3 and its
normal form $\normal{h_1}$ is $[(0,5), (2, 25), (3,15), (1,10)]$.

We often refer to $y$ as the {\em mass} of the element $(x,y)$
and given a histogram $h$, we refer to the mass of the $i^{\mathrm th}$
using the notation $h[i]$.  For instance, $h_1[3] = 15$ and 
$\bar{h_1}[3] = 10$.  The {\em residual mass} ({\em rm}) of a column $i$ in 
a normalized histogram $h$ is the mass of all the columns to the right of 
$i$ plus the mass of the zero-bar.  Mathematically, 
{\em rm}$(\bar{h},i) = h[0] + \sum_{j=i+1}^{\mathit{width}(h)} h[j]$.
For example, {\em rm}$(\bar{h_1},1) = 5 + 15 + 10 = 30$.
The residual mass is the primary characterization of ``narrowness''
of a histogram.  Those histograms with low residual mass of the first
column ({\em i.e.,} {\em rm}$(\bar{h_1},1)$ is small) 
are good candidates for structs.

In order to distinguishing between
structs, unions and arrays,
we also need to define the {\em coverage} of a histogram, which
is simply the sum of the non-zero histogram elements.  Mathematically,
{\em coverage}$(\bar{h}) = \sum_{j=1}^{\mathit{width}(h)} h[j]$.

Finally, recursive partitioning of the data works better when
groups of tokens with similar distributions are considered together
-- when two tokens have the same distribution, it is almost always the 
case that they form part of the same type constructor.  There are a 
number of ways to define {\em similarity} of histograms.  We have
chosen a symmetric form of {\em relative entropy} \cite{Lin91:divergence}.
\footnote{Suggested by Rob Schapire, personal communication, May 2007.}  
The (plain) relative entropy
of two normalized histograms $\normal{h_1}$ and $\normal{h_2}$, 
written \relativee{\normal{h_1}}{\normal{h_2}}, is
defined as follows.
\[
 \relativee{\normal{h_1}}{\normal{h_2}} 
   = \sum_{j=1}^{\mathit{width}(\normal{h_1})} \normal{h_1}[j]*log(\normal{h_1}[j]/\normal{h_2}[j])
\]
To create a symmetric form, we first find the average of the two
histograms in question (written $\addh{h_1}{h_2}$)
by summing corresponding columns and dividing by two.  This
has the effect of preventing the denominator from being zero in the 
final relative entropy computation.  At last, the symmetric
relative entropy is:
\[
 \srelativee{\normal{h_1}}{\normal{h_2}} 
   = \frac{1}{2}  \relativee{\normal{h_1}}{\addh{\normal{h_1}}{\normal{h_2}}}
   +  \frac{1}{2}  \relativee{\normal{h_2}}{\addh{\normal{h_1}}{\normal{h_2}}}
\]

Now that we have defined the relevant properties of histograms,
we can explain the method used to recursively guess type constructors.

\begin {enumerate}
\item Group related histograms into sets as follows.  
A histogram $h_1$ belongs to
group $G$ provided there exists another histogram $h_2$ in $G$
such that $\srelativee{\normal{h_1}}{\normal{h_2}} < 
\mathrm{ClusterTolerance}$.  We have found a ClusterTolerance
of 0.01 is effective.  A histogram dissimilar to all others may form 
its own group.

\item Identify a struct by:
\begin {enumerate}
\item  Considering all groups $G$, in descending order by {\em StructScore}.
The first group $G$ that contains any tokens that satisfy the
 following minimum criteria will be declared a struct on this 
iteration of the recursive algorithm:
\begin {itemize}
\item maximum residual mass
\item minimum coverage
\end{itemize}
If no tokens in any group satisfies the struct criterion, check for arrays.
\end{enumerate}
\item Identify an array by:
\begin {enumerate}
\item  Considering all groups $G$, in descending order by {\em ArrayScore}.
The first group $G$ that contains any tokens that satisfy the
 following minimum criteria will be declared an array on this 
iteration of the recursive algorithm:
\begin {itemize}
\item minimum coverage
\end{itemize}
If no tokens in any group satisfies the array criterion, find a union.
\end{enumerate}
\item Identify a union by partitioning the current set of chunks according
to the first token in each chunk.
\end{enumerate}

\begin {figure*}
\begin{minipage}[t]{0.5\columnwidth}
\epsfig{file=histogram1.eps, width=\columnwidth}
\end{minipage}
\begin{minipage}[t]{0.5\columnwidth}
\epsfig{file=histogram2.eps, width=\columnwidth}
\end{minipage}
\begin{minipage}[t]{0.5\columnwidth}
\epsfig{file=histogram3.eps, width=\columnwidth}
\end{minipage}
\begin{minipage}[t]{0.5\columnwidth}
\epsfig{file=histogram4.eps, width=\columnwidth}
\end{minipage}
%
%a) histogram for ' '
%histogram for ':'
%...
%
%Omit telling the reader which histograms are for which tokens in the figure.
%We'll explain in the text...
%
%May 6 of them 3 each from Crashreporter.log and Sirius?
%We'll pick carefully.
\caption{Selected Histograms for Crashreporter.log and Dibbler.1000}
\label{fig:histograms} 
\end{figure*}

\paragraph*{Additional Tricks.}
Grouping

\input{discoveryalg2.tex}

%%     *  role = quickly find a description in the "approximate area" of the correct description
%%     * explain structure of a generic "top-down" inference algorithm -- perhaps give pseudocode
%%     * explain our heuristics: generation of histograms, choice of struct, array, union, base type
%%     * grouping construct (introduce additional example as needed)
%%     * show (part of?) description of running example 

\subsection {Information-Theoretic Scoring}

\input{scoring}

\subsection {Structure Refinement}

%\input{rules}

%    *  role = starting with the candidate structure, search for nearby descriptions that optimize an information theoretic scoring function
%    * explain the 3 parts: value independent, value dependent, value independent
%    * give (partial) list of rules used -- we need to work on notation for explaining these rules
%    * illustrate several transformations using the running example
%    * compare example after rewriting to the example from subsection 3.4 above
%    * optional subsubsection: theory suggesting our algorithm is "correct" (we'd need a semantics for our IR then)

\input{refinement} 

\subsection {Finishing Up}

    * printing pads syntax and invoking toolchain
