
An {\em ad hoc data source} is any semistructured data source
for which useful data analysis and transformation tools
are not readily available. XML, HTML and CSV are {\em not} 
ad hoc data sources as there are numerous programming libraries,
query languages, manuals and other resources dedicated to
helping analysts manipulate data in these formats.
However, despite the prevalence of standard formats,
massive quantities of legacy ad hoc data persist in fields ranging from
computational biology to finance to physics to networking to healtcare and
systems administration.  Moreover, engineers and scientists are continuously
producing new ad hoc formats ---despite the presence of existing 
standards--- because it is often expedient to do so.  Over time, these
expedient formats become difficult to work with because of missing
documentation, a lack of tools, and corruption caused by repeated,
poorly thought-through redesign, reuse and extension.

The goal of the \pads{}
project~\cite{padsweb,fisher+:pads,fisher+:popl06,mandelbaum+:pads-ml}
is to improve the productivity of data analysts who need to cope with
new and evolving ad hoc data sources on a daily basis.  Our central
technology is a domain-specific language in which programmers can
specify the structure and expected properties of ad hoc data sources,
whether they be ASCII, binary, Cobol or a mixture of formats.  These
specifications, which resemble extended type declarations from
conventional programming languages, are compiled into a suite of
programming libraries, such as parsers and printers, and end-to-end
data processing tools including an XML-translator, a query
engine~\cite{fernandez+:padx}, a 
simple statistical analysis, and others.  Hence, the most important
benefit of using \pads{} is that  a single declarative description
may be used to generate many useful data processing tools completely
automatically.

On the other hand, the most important impediment to using \pads{}
is the time and expertise needed
to write a \pads{} description for a new ad hoc data source.
For data experts possessing clear, unambiguous documentation about a
simple data source, writing a \pads{} description may take 
anywhere from a few minutes to a few hours.  However,
it is relatively common to encounter ad hoc data sources that 
contain valuable information, yet have little or no documentation.
Understanding the structure of the data and creating descriptions for such 
sources can easily take days or weeks depending upon the complexity
and volume of the data in question.  In one specific example, Fisher
spent approximately three weeks (off and on) attempting to understand
and describe an important data source used internally at AT\&T.  One
of the stumbling points in this case was that the data source suddenly 
switched formats after approximately 1.5 million entries.  Of course,
if dealing with the vagaries of ad hoc data sources is
time-consuming and error-prone for experts, it is even worse for
novice users.

To improve the productivity of experts and to 
make the \pads{} toolkit accessible to new users with 
little time to learn the specification language,
we have developed an automatic format inference engine.
This format inference engine reads arbitrary ASCII data sources
and produces an accurate, human-readable \pads{} description of the source.
These machine-produced descriptions give experts a running start
in any data analysis task as the libraries generated from these
descriptions may be incorporated directly into an ordinary C program.
The inference engine is also directly connected to the rest of the
\pads{} infrastructure, making it possible for first-time users,
with no knowledge of the \pads{} domain-specific language, 
to translate the data into a form suitable for loading into a relational database, 
to convert the data into \xml{},
to query it in XQuery,
to detect errors in additional data from the same source,
and to draw graphs of various data components, all
with just a ``push of a button.''

To summarize, this paper makes three main contributions.

\begin{itemize}
\item We have developed a new, multi-phase algorithm 
that infers the format of complex, ad hoc data sources,
producing compact and accurate \pads{} descriptions.

\item We have incorporated the inference algorithm into 
a modular software system that uses sample data to
generate a toolkit of useful data processing tools,
without requiring any human intervention.
 
\item We have evaluated the correctness and performance of
our system on a range of ASCII data sources.
\end{itemize}

%% Given the difficulties 

%% Our end
%% goal is to provide users with an end system that allows them to
%% automatically generate sufficiently accurate \pads{} descriptions 
%% that these descriptions may be fed into our compiler to generate
%% useful programming libraries and data 
%% processing and visualization tools. 

%% The goal of this project is to provide a generic framework that includes
%% languages and tools to seamlessly automate data stream analysis. 
%% Given some samples of the data stream, our prototype system produces 
%% an intermediate
%% representation of the structure of the data through structure discovery
%% and refinement, and translates that representation into a
%% declarative data-description language, \padsc{}. \padsc{} is 
%% expressive enough to describe a variety of data feeds 
%% including ASCII, binary, EBCDIC, Cobol, and mixed data formats.  
%% From \padsc{}, a suite of tools can generated with functions for 
%% parsing, manipulating, and summarizing the data. All these can be 
%% done with a ``push of a button.''   


% Transactional data streams, such as sequences of stock-market buy/sell orders,
% credit-card purchase records, web server entries, and electronic fund
% transfer orders, can be mined very profitably.  As an example,
% researchers at AT\&T have built customer profiles from streams of
% call-detail records to significant financial effect~\cite{kdd99}.   
% Often such streams are high-volume: AT\&T's call-detail stream contains
% roughly 300~million calls per day requiring approximately 7GBs of
% storage space.  Typically, such stream data arrives ``as is'' in
% \textit{ad hoc} formats with poor documentation.  In addition, the
% data frequently contains errors.  The appropriate response to such
% errors is application-specific. 
% %Some applications can simply discard
% %unexpected or erroneous values and continue processing.  For other
% %applications, however, errors in the data can be the most interesting
% %part of the data.  

% Understanding a new data stream and producing a suitable parser are
% crucial first steps in any use of stream data.  Unfortunately, writing
% parsers for such data is a difficult task, both tedious and
% error-prone. It is complicated by lack of documentation, convoluted
% encodings designed to save space, the need to handle errors
% robustly, and the need to produce efficient code to cope with the
% scale of the stream.  Often, the hard-won understanding of the data
% ends up embedded in parsing code, making long-term maintenance
% difficult for the original writer and sharing the knowledge with
% others nearly impossible.

%\paragraph*{notes.}
%this is the introduction.  * explain system goals from a user
%perspective * explain what kinds of questions users can ask and get
%answered concerning data that they have. explain who, exactly, would
%want to use our system and what tasks exactly can our system help a
%target user achieve. explain goals not mechanisms * list contributions


%% \begin{figure}
%% \begin{center}
%% \begin{tabular}{|l|l|}
%% \hline
%% Name \& Use   &  Representation               \\ \hline\hline
%% Web server logs (CLF):  &  Fixed-column ASCII records \\ 
%% Measure web workloads &                             \\ \hline
%% AT\&T provisioning data: & Variable-width ASCII records  \\ 
%% Monitor service activation &                              \\ \hline
%% Call detail: Fraud detection  &  Fixed-width binary records \\  \hline 
%% AT\&T billing data: & Various Cobol data formats  \\ 
%% Monitor billing process   &                             \\ \hline
%% %IP backbone data:  & ASCII   \\
%% %Monitor network performance  &        \\ \hline
%% Netflow:                        & Data-dependent number of     \\ 
%% Monitor network performance  & fixed-width binary records  \\ \hline
%% Newick:   Immune                 & Fixed-width ASCII records \\ 
%% system response simulation & in tree-shaped hierarchy\\ \hline                                
%% Gene Ontology:             & Variable-width ASCII records \\
%% Gene-gene correlations     & in DAG-shaped hierarchy \\ \hline
%% %HL7:             & Variable-width ASCII records \\
%% %Medical lab results     &  \\ \hline
%% CPT codes: Medical diagnoses & Floating point numbers \\ \hline
%% SnowMed: Medical clinic notes & keyword tags  \\ \hline

%% \end{tabular}


%% \caption{Selected ad hoc data sources.}
%% \label{figure:data-sources}
%% \end{center}
%% \end{figure}
 
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
