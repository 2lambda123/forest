@inproceedings{1164139,
 author = {Geert Jan Bex and Frank Neven and Thomas Schwentick and Karl Tuyls},
 title = {Inference of concise DTDs from XML data},
 booktitle = {VLDB '06: Proceedings of the 32nd international conference on Very large data bases},
 year = {2006},
 pages = {115--126},
 location = {Seoul, Korea},
 publisher = {VLDB Endowment},
 }

[meta-note: even though this is not a directly related paper, it does seem
pretty cool and merits a deeper read than I gave it.  There may be ideas
in here that we could exploit at some point.]

This paper describes how to infer a DTD from raw XML data.
Some of the elements of the problem are similar to our problem.
In particular, the scheme is inferred without any intervention
from positive examples only.  However, other elements are quite different:

-- the authors are working with well-structured XML data as opposed to
ad hoc data. Ad hoc data has tokenization problems and 
is not a well-structured, tree-shaped collection of tags.

-- We evaluate the effectiveness of our techniques on ad hoc data.
It is impossible to know how the core ideas in this xml inference
algorithm might work on ad hoc data.

-- this paper shows how to infer a DTD, but does not show how to use it
to automatically generate end-to-end tools (accumulator, grapher, query
engine, xml-transformer).

-- a DTD can be approximated as context-free grammar where the right-hand
sides are regular expressions.  Consequently, in this context, the core 
problem solved by the authors is an inference mechanism for two subclasses
of regular expressions:
  -- SOREs -- where every atomic element can occur at most once in the 
                regular expression
  -- CHAREs -- where every regular expression is a sequence of "factors"

-- even omitting constraints, dependencies and switches, the sorts of
grammars that we infer are not restricted to SOREs or CHAREs.  It
would be interesting to investigate what would happen if we try to
make such a restriction or whether it's simply impossible 
to come up with any reasonable description in such cases.

-- the algorithms used in our paper are completely different

=========================================

@inproceedings{DBLP:conf/vldb/BexNV07,
  author    = {Geert Jan Bex and
               Frank Neven and
               Stijn Vansummeren},
  title     = {Inferring XML Schema Definitions from XML Data},
  booktitle = {VLDB},
  year                  = {2007},
  pages     = {998-1009},
  ee                = {http://www.vldb.org/conf/2007/papers/research/p998-bex.pdf},
  crossref  = {DBLP:conf/vldb/2007},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

-- this paper builds upon the paper above.  in the previous paper,
the DTD definition could not depend upon the parent node.  In this work,
the authors infer "k-local" SOREs where the definition of the SORE can
depend upon its parent, grant-parent, great-grand-parent, etc. to a maximum of
k levels up for some fixed constant k.  This allows the authors to infer
the more powerful XML Schema Definitions (as opposed to just DTDs) for 
their data.

-- these k-local SOREs include an element of dependency -- dependency on 
parents and grandparents.  Since ad hoc data doesn't have the same kind of
nesting structure as XML, we tend to find dependencies between siblings
and represent those as "switches".

-- most of the comparison with the previous paper also applies...

=======================================

InstanceToSchema tool
http://www.xmloperator.net/i2s/

this is open source software written in Java and released under a 
BSD-style license.  It infers RELAX NG schema for xml. 
I couldn't find any papers describing the inference techniques
in any detail.  I didn't look too hard.  There were no obvious links 
from the web page.

=======================================

On schema Discovery
Miller, et el.
http://citeseer.ist.psu.edu/miller03schema.html

This paper uses a clustering algorithm based on attributes to have some
basic mining of constraints on database relations. The purpose of this
is to discovery new database schema for data with errors. This is obviously
quite different from what we are doing. No stats analysis is done here.
The clustering is based on some distance metric defined on the DB attributes.

========================================

@inproceedings{DBLP:conf/webdb/GubanovB06,
  author    = {Michael Gubanov and
               Philip A. Bernstein},
  title     = {Structural text search and comparison using automatically
               extracted schema},
  booktitle = {WebDB},
  year                  = {2006},
  ee                =
{http://db.ucsd.edu/webdb2006/camera-ready/paginated/01-161.pdf},
  crossref  = {DBLP:conf/webdb/2006},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

This paper extracts schema from unstructured text such as instruction
manuals through the use of natural language processing techniques. In 
particular, it separate the nouns and verbs from each sentence and
try to classify the subject or concept of the sentences by the nouns and
generate attributes of the schema with the verbs or actions used. The
counting of distinct words here seems analogous to the building of histograms
of tokens in our paper, but the counting here is to decide the main
concept of the sentence and not the structure of the sentence. The 
structure is assumed to be fixed English grammar. And beyond this, the
techniques used in two papers are very different.

==========================================================

*** Automatic segmentation of text into structured records
Borkar et al.
http://portal.acm.org/citation.cfm?doid=375663.375731

This paper describes another natural language processing technique to
extract records from unstructured text such as addresses, bib entries,etc.
It learns a Hidden Markov Model which is a probablistic finite state
automata and a dictionary of elements from a set of training data. The HMM
describes the possible structure of the text e.g. an address is house
number followed by road and followed by city with certain probabilities.
And the dictionary tells which words are likely part of a street name, and
which words are likely to be a city, etc. The problem they are attacking is
quite different from ours. We are not trying to give semantics to
free text like they do.

================================

*** Reconciling schemas of disparate data sources: A machine-learning approach
Doan et al.
http://portal.acm.org/citation.cfm?doid=375663.375731

This paper introduces a system called LSD that learns the semantic mapping 
between between two DB schemas. The input to the learning system is already 
structured data such as XMLs and DTDs. We are learning from semi-structured
or unstructured data and we are not trying to identity semantic relations so
the problems are quite different. 

==============================
"Learning information extraction rules for semi-structured and free text "
Soderland
http://www.cs.washington.edu/homes/soderlan/soderland_ml99.pdf  

This paper again describes a system called WHISK that learns information
extraction rules from natural language text. The text can be unstructured,
semi-structured or free text, such as ads from Graigslist. The rules 
learned are in a form of regular expression. The learning starts with 
a number of hand-tagged training set and some user-defined semantic classes
such as what words means bedroom, and some empty rules. Learning and 
human tagging happen interleavingly in iterations. The empty rules contain
slots which need to be filled in with semantic terms such as Time, Date,
Neighborhood, City, Bedroom. The slots are the properties of interest for
later extraction and the number of slots are fixed a priori. Most of the
comparison between natural language processing and our paper applies here.
The technique we are advocating is completely automatic, push-button kind,
whereas in this paper, human knowledge is require in the training (partly
because this is a very hard problem). The learning of regular 
expression could be a userful thing for us to look
deeper, for discovering new tokens in tokenization.

=================================
@Article{DenisLemayTerlutte2004,
   author="F. Denis and A. Lemay and A. Terlutte",
   title="Learning regular languages using RFSAs",
   journal=TCS,
   year="2004",
   volume="313",
   number="2",
   pages="267-294"
}

This paper proposes an algorithm DeLeTe2 to learn a subset of 
regular languages called residual languages by identifying
inclusion relations among them as opposed to equivalence, from
only positive samples. The result of that is a residual finite 
state automaton (RFSA) instead of a DFA. Because learning 
RFSA is not possible in polynomial time, the paper attempts to 
learn a language that is larger than minimum instead. 
Comparison of learning automata with our work above also
applies here.

==================================
@InProceedings{fernau02a,
   booktitle="Proceedings International Workshop on Machine Learning and Data
Mining in Pattern Recognition (MLDM 2001)",
   author="H. Fernau",
   title="Learning XML grammars",
   publisher=SV,
   series=LNCS,
   volume=2123,
   pages="73-87",
   year=2001
}

This paper is an earlier attempt than the Bex et al. above to infer
DTDs from XMLs. They propose a generalized framework of learning
a sub-class of regular language (such as XML) by limiting the language
with a distinguishing function. The algorithm proposed is exponential to
the size of this function. They claim to generalize over k-reversible
language by Angluin and the terminal distinguishing languages by 
Radhakrishnan. I will have to read those two papers to tell more of
the story. This paper proved some properties about XML and also 
the biased language that they infer. For comparison between this paper
and our work, similar argument in the comments on Bex et al. applies
here. Also notice that our method is more of a heuristic nature.  

====================================
@InProceedings{RaeymaekersBruynoogheVandenBussche05,
   series=LNAI,
   year="2005",
   booktitle="Proceedings of ECML'2005" ,
   pages="305--316",
   volume="3720",
   author="Stefan Raeymaekers and Maurice Bruynooghe and  Jan {Van den
Bussche}",
   title="Learning (k,l)-Contextual Tree Languages for Information Extraction"
}

This paper is another attempt at learning a wrapper based on a tree
automaton that represents HTML/XML data. This tree language limits the height
and the cardinality of the input tree to l and k respective to achieve a
balanc between expressiveness and generality. The value l and k need to be
tune with both positive and negative examples. The paper claims to require
fewer positive and negative examples to learn the grammar compared to
previous attempts.

======================================
[2006] Aurélien Lemay and Joachim Niehren and Rémi Gilleron, Learning n-ary
Node Selecting Tree Transducers from Completely Annotated Examples
<http://hal.ccsd.cnrs.fr/view_by_stamp.php?label=INRIA&langue=en&action_todo=vi
ew&id=inria-00088077&version=1>, /International Colloquium on Grammatical
Inference/, Lecture Notes in Artificial Intelligence *4201*, 253-267

This paper looks at the problem of learning queries from XML or HTML trees. 
They propose a polynomial time algorithm to learn tree automata that represent
node queries in the XML trees. The algorithm trains on both positive and negative
examples. This problem is quite different than our problem which is ad hoc
data and not in any tree structure to begin with. Our learning systems also only
trains on positive examples. 

========================================
