@inproceedings{1164139,
 author = {Geert Jan Bex and Frank Neven and Thomas Schwentick and Karl Tuyls},
 title = {Inference of concise DTDs from XML data},
 booktitle = {VLDB '06: Proceedings of the 32nd international conference on Very large data bases},
 year = {2006},
 pages = {115--126},
 location = {Seoul, Korea},
 publisher = {VLDB Endowment},
 }

[meta-note: even though this is not a directly related paper, it does seem
pretty cool and merits a deeper read than I gave it.  There may be ideas
in here that we could exploit at some point.]

This paper describes how to infer a DTD from raw XML data.
Some of the elements of the problem are similar to our problem.
In particular, the scheme is inferred without any intervention
from positive examples only.  However, other elements are quite different:

-- the authors are working with well-structured XML data as opposed to
ad hoc data. Ad hoc data has tokenization problems and 
is not a well-structured, tree-shaped collection of tags.

-- We evaluate the effectiveness of our techniques on ad hoc data.
It is impossible to know how the core ideas in this xml inference
algorithm might work on ad hoc data.

-- this paper shows how to infer a DTD, but does not show how to use it
to automatically generate end-to-end tools (accumulator, grapher, query
engine, xml-transformer).

-- a DTD can be approximated as context-free grammar where the right-hand
sides are regular expressions.  Consequently, in this context, the core 
problem solved by the authors is an inference mechanism for two subclasses
of regular expressions:
  -- SOREs -- where every atomic element can occur at most once in the 
                regular expression
  -- CHAREs -- where every regular expression is a sequence of "factors"

-- even omitting constraints, dependencies and switches, the sorts of
grammars that we infer are not restricted to SOREs or CHAREs.  It
would be interesting to investigate what would happen if we try to
make such a restriction or whether it's simply impossible 
to come up with any reasonable description in such cases.

-- the algorithms used in our paper are completely different

=========================================

@inproceedings{DBLP:conf/vldb/BexNV07,
  author    = {Geert Jan Bex and
               Frank Neven and
               Stijn Vansummeren},
  title     = {Inferring XML Schema Definitions from XML Data},
  booktitle = {VLDB},
  year                  = {2007},
  pages     = {998-1009},
  ee                = {http://www.vldb.org/conf/2007/papers/research/p998-bex.pdf},
  crossref  = {DBLP:conf/vldb/2007},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

-- this paper builds upon the paper above.  in the previous paper,
the DTD definition could not depend upon the parent node.  In this work,
the authors infer "k-local" SOREs where the definition of the SORE can
depend upon its parent, grant-parent, great-grand-parent, etc. to a maximum of
k levels up for some fixed constant k.  This allows the authors to infer
the more powerful XML Schema Definitions (as opposed to just DTDs) for 
their data.

-- these k-local SOREs include an element of dependency -- dependency on 
parents and grandparents.  Since ad hoc data doesn't have the same kind of
nesting structure as XML, we tend to find dependencies between siblings
and represent those as "switches".

-- most of the comparison with the previous paper also applies...

=======================================

InstanceToSchema tool
http://www.xmloperator.net/i2s/

this is open source software written in Java and released under a 
BSD-style license.  It infers RELAX NG schema for xml. 
I couldn't find any papers describing the inference techniques
in any detail.  I didn't look too hard.  There were no obvious links 
from the web page.

=======================================

On schema Discovery
Miller, et el.
http://citeseer.ist.psu.edu/miller03schema.html

This paper uses a clustering algorithm based on attributes to have some
basic mining of constraints on database relations. The purpose of this
is to discovery new database schema for data with errors. This is obviously
quite different from what we are doing. No stats analysis is done here.
The clustering is based on some distance metric defined on the DB attributes.

========================================

@inproceedings{DBLP:conf/webdb/GubanovB06,
  author    = {Michael Gubanov and
               Philip A. Bernstein},
  title     = {Structural text search and comparison using automatically
               extracted schema},
  booktitle = {WebDB},
  year                  = {2006},
  ee                =
{http://db.ucsd.edu/webdb2006/camera-ready/paginated/01-161.pdf},
  crossref  = {DBLP:conf/webdb/2006},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

This paper extracts schema from unstructured text such as instruction
manuals through the use of natural language processing techniques. In 
particular, it separate the nouns and verbs from each sentence and
try to classify the subject or concept of the sentences by the nouns and
generate attributes of the schema with the verbs or actions used. The
counting of distinct words here seems analogous to the building of histograms
of tokens in our paper, but the counting here is to decide the main
concept of the sentence and not the structure of the sentence. The 
structure is assumed to be fixed English grammar. And beyond this, the
techniques used in two papers are very different.

One thing to note is that many of the natural language processing techniques
require some prior knowledge of the domain (in this case nouns and verbs),
and the tokenization used in our work also require such knowledge (e.g.
date, time, url formats). In this sense, there is some similarity between
our work and NLP.

==========================================================

*** Automatic segmentation of text into structured records
Borkar et al.
http://portal.acm.org/citation.cfm?doid=375663.375731

This paper describes another natural language processing technique to
extract records from unstructured text such as addresses, bib entries,etc.
It learns a Hidden Markov Model which is a probablistic finite state
automata and a dictionary of elements from a set of training data. The HMM
describes the possible structure of the text e.g. an address is house
number followed by road and followed by city with certain probabilities.
And the dictionary tells which words are likely part of a street name, and
which words are likely to be a city, etc. The problem they are attacking is
quite different from ours. We are not trying to give semantics to
free text like they do.

================================

*** Reconciling schemas of disparate data sources: A machine-learning approach
Doan et al.
http://portal.acm.org/citation.cfm?doid=375663.375731

This paper introduces a system called LSD that learns the semantic mapping 
between between two DB schemas. The input to the learning system is already 
structured data such as XMLs and DTDs. We are learning from semi-structured
or unstructured data and we are not trying to identity semantic relations so
the problems are quite different. 

==============================
"Learning information extraction rules for semi-structured and free text "
Soderland
http://www.cs.washington.edu/homes/soderlan/soderland_ml99.pdf  

This paper again describes a system called WHISK that learns information
extraction rules from natural language text. The text can be unstructured,
semi-structured or free text, such as ads from Graigslist. The rules 
learned are in a form of regular expression. The learning starts with 
a number of hand-tagged training set and some user-defined semantic classes
such as what words means bedroom, and some empty rules. Learning and 
human tagging happen interleavingly in iterations. The empty rules contain
slots which need to be filled in with semantic terms such as Time, Date,
Neighborhood, City, Bedroom. The slots are the properties of interest for
later extraction and the number of slots are fixed a priori. Most of the
comparison between natural language processing and our paper applies here.
The technique we are advocating is completely automatic, push-button kind,
whereas in this paper, human knowledge is require in the training (partly
because this is a very hard problem). The learning of regular 
expression could be a userful thing for us to look
deeper, for discovering new tokens in tokenization.

=================================
@Article{DenisLemayTerlutte2004,
   author="F. Denis and A. Lemay and A. Terlutte",
   title="Learning regular languages using RFSAs",
   journal=TCS,
   year="2004",
   volume="313",
   number="2",
   pages="267-294"
}

This paper proposes an algorithm DeLeTe2 to learn a subset of 
regular languages called residual languages by identifying
inclusion relations among them as opposed to equivalence, from
only positive samples. The result of that is a residual finite 
state automaton (RFSA) instead of a DFA. Because learning 
RFSA is not possible in polynomial time, the paper attempts to 
learn a language that is larger than minimum instead. 
Comparison of learning automata with our work above also
applies here.

==================================
@InProceedings{fernau02a,
   booktitle="Proceedings International Workshop on Machine Learning and Data
Mining in Pattern Recognition (MLDM 2001)",
   author="H. Fernau",
   title="Learning XML grammars",
   publisher=SV,
   series=LNCS,
   volume=2123,
   pages="73-87",
   year=2001
}

This paper is an earlier attempt than the Bex et al. above to infer
DTDs from XMLs. They propose a generalized framework of learning
a sub-class of regular language (such as XML) by limiting the language
with a distinguishing function. The algorithm proposed is exponential to
the size of this function. They claim to generalize over k-reversible
language by Angluin and the terminal distinguishing languages by 
Radhakrishnan. I will have to read those two papers to tell more of
the story. This paper proved some properties about XML and also 
the biased language that they infer. For comparison between this paper
and our work, similar argument in the comments on Bex et al. applies
here. Also notice that our method is more of a heuristic nature.  

====================================
@InProceedings{RaeymaekersBruynoogheVandenBussche05,
   series=LNAI,
   year="2005",
   booktitle="Proceedings of ECML'2005" ,
   pages="305--316",
   volume="3720",
   author="Stefan Raeymaekers and Maurice Bruynooghe and  Jan {Van den
Bussche}",
   title="Learning (k,l)-Contextual Tree Languages for Information Extraction"
}

This paper is another attempt at learning a wrapper based on a tree
automaton that represents HTML/XML data. This tree language limits the height
and the cardinality of the input tree to l and k respective to achieve a
balanc between expressiveness and generality. The value l and k need to be
tune with both positive and negative examples. The paper claims to require
fewer positive and negative examples to learn the grammar compared to
previous attempts.

======================================
[2006] Aurélien Lemay and Joachim Niehren and Rémi Gilleron, Learning n-ary
Node Selecting Tree Transducers from Completely Annotated Examples
<http://hal.ccsd.cnrs.fr/view_by_stamp.php?label=INRIA&langue=en&action_todo=vi
ew&id=inria-00088077&version=1>, /International Colloquium on Grammatical
Inference/, Lecture Notes in Artificial Intelligence *4201*, 253-267

This paper looks at the problem of learning queries from XML or HTML trees. 
They propose a polynomial time algorithm to learn tree automata that represent
node queries in the XML trees. The algorithm trains on both positive and 
negative examples. And the examples have to be annotated by human. 
This problem is quite different than our problem which is ad hoc
data and not in any tree structure to begin with. Our learning systems also only
trains on positive examples. 

========================================
@InProceedings{MusleaMintonKnoblock03,
   booktitle="IJCAI 2003",
   year="2003",
   title= "Active learning with strong and weak views: a case study on wrapper
induction",
   pages="415--420",
   author="Ion Muslea and Steve Minton and Craig Knoblock"
}

This paper presents a technique to select document samples for human to label
in "active learning". The technique is called aggressive co-testing
which is an improvement from their earlier Naive Co-testing method
which uses two strong views (one forward and one backward) on sample
data. A view in this context is a set of features from the document
that is sufficient to learn the exact concept. Co-testing is applicable
to documents which has multiple disjoint view, each of which sufficient
for learning. Instead of randomly choose examples to label, the
Co-testing method pick those example on which the multiple views disagree,
that is, the concept learned from one view is different from another. 
This method reduces the number of samples that require labeling effective.
The Agressive Co-Testing uses a weak view that doesn't give the exact
concept (but either more general or more specific) when learned on, in
addition to two strong views used in Naive Co-testing. 
The difference between this work and our work is, first this is a technique
that helps labeling samples, which is not even relevant to us; second
the technique is used for learning features in a document and not the structure
or grammar of the entire document like what we are doing; and
finally probability theory is not used here with these techniques. 

==========================================
@inproceedings{DBLP
<http://www.informatik.uni-trier.de/%7Eley/db/about/bibtex.html>:conf/icml/Ires 
onCCFKL05,
 author    = {Neil Ireson and
              Fabio Ciravegna and
              Mary Elaine Califf and
              Dayne Freitag and
              Nicholas Kushmerick and
              Alberto Lavelli},
 title           = {Evaluating machine learning for information extraction},
 booktitle = {ICML},
 year           = {2005},
 pages           = {345-352},
 ee           = {http://doi.acm.org/10.1145/1102351.1102395},
 crossref  = {DBLP:conf/icml/2005},
 bibsource = {DBLP, http://dblp.uni-trier.de}
}

This paper documents an information extraction contest which uses uniform
comparison among all the participants. The competition asks the participants
to learn important features from a number of annotated CFP documents, and then
cross validate on a larger number of test documents. Altogether 18 
different systems were used in the competition. The paper compared the accuracy
and learning curves of these systems. Conclusions include that different
systems implementing the same ML algorithm can perform very differently and
many systems exhibit over-fitting. Parameterization plays a critical role sometimes
in the performance of the algorithm. This paper is not directly related to the kind of
techniques we introduce in our work. However, the empirical methodology is interesting
and could help us in presenting our data to the ML community.

========================================================

Xtract: Learning Document Type Descritors from XML Document Collections
Garofalskis et al.

This is one of the influential papers on XML DTD inferencing. The basic
idea is to generate one regular expression for each node (element name) in
the XML, where the symbols in the expression is the sub-element names under
that node. So at a node level, the DTD inference problem is exactly the 
same as our problem after our data file is completely tokenized. 
The tokens are essentially the sub-element names. The paper takes a number of
positive examples, and starts with a regex which is the union of all
the positive examplees, and then used some generalization techniques to
create sequences (Kleene star) and tuples. The resulting grammar is
more general than the original grammar. Then it uses factoring to simplify the
grammar. During generalization, a number of possible candidate may result, 
MDL scores are computed for each candidate and a best is chosen in the end.
The generalization and factoring are essentially a set of rewriting rules that
leads to a search problem. This is similar to our approach. However, because
the initial grammar can be extremely complex, it is questionable how
effective these rewriting can be. 

==========================================
Wrapper Induction for Information Extraction
Nicholas Kushmerick, IJCAI 1997

This paper (as well as the PhD thesis of the same title) proposed a framework for
extract information from web pages. A wrapper is a function that returns some 
tuples of interest from a web page. It is represented by a procedure such as
"skip everything till first occurrence of h, until an occurence of t, go to next element
between tag l_i and r_i and extract it as the next attribute in the tuple." 
The kind of wrapper being induced here is what they called HLRT wrapper which is
suitable for tabulated content. 

Given a set of sample pages, the framework first use an oracle function to label
the samples. The oracle function makes use of a collection of pre-defined
domain-specific reusable heuristics called the "recognizers" to identify things 
of interest e.g. what constitutes a phone number or what is a country name, etc. 
The recognizers might not be perfect and a "corroboration" process takes care of
the errors in recognization attributes. Labels such generated might be ambiguous.
This is handled by a probablistic model called PAC. The labelling technique is
the key contribution of this paper. 

The number of samples needed to be confident about the induction result is governed
by PAC model which defines an error metric over hypothesis. With each sample page
labeled, a BuiltHLRT algorithm runs through each page, and extract the common
header and footer as well as the prefixes and suffixes of each labeled attribute, and
construct the wrapper. 

The techniques introduced here only requires positive samples and the labelling 
process can be related to the tokenization process of our work. In fact, it will be
interesting to see if the corroboration algorithm can be applied somehow to
our tokenization. However there are significant differences between this work and
ours: 

1. The problem being solved here is information extraction and not learning of
the entire structure of the documents, so our goals are different;
2. The technique only works for html/xml pages of tabular format 
3. the induction used here is really simple which basically constitutes of 
a conjunction of all the features each sample page pocesses and calls that 
a generalization. No stats analysis of any kind is used in the induction.


=================================
