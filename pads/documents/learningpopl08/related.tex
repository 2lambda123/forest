Researchers have been studying {\em grammar induction}, the process of
learning the structure of a data source, since the 1960s; De La Higuera
surveys some recent trends~\cite{higuera01current}.  However,
our system is unique in two important ways.  First, our inference
algorithm does not stand alone; it is part of the more general \pads{}
programming environment.  The fusion of the
\pads{} system, including its automatic data representation generation,
its error detection facilities, its generic programming environment, 
and its powerful tool suite, together with grammar induction
is one of our key contributions.  Second, many researchers have
focused either on grammar induction for natural language processing or
for information extraction from \xml{} or \html{} documents.  In
contrast, we focus on ad hoc data sources such as system logs and
scientific data sets. Ad hoc data is substantially less
structured than \xml{}, and yet, unlike natural language, it is
possible assign our data sources accurate, compact descriptions.  After 
searching the literature and consulting
with multiple experts in grammar induction at the CAGI 2007 
workshop~\cite{}, we could find no existing 
systems that treat a similar domain and come remotely close to
replicating the functionality of our automatically generated tool suite.

% Third, from a
% technical standpoint, we developed a new top-down structure discovery
% algorithm and showed how to combine that productively with a
% classic bottom-up rewriting systems based on the minimum description
% length principle.  In what follows, we compare our system more
% specifically to the most closely related work of other researchers.

Perhaps the most closely related work is from Arasu and 
Garcia-Molina~\cite{arasu+:sigmod03}, who developed a information
extraction system for sets of similar web pages.  The
commonalities between web pages are assumed to be a ``template'' structure
and the differences are assumed to be values drawn from a database
sitting behind the web site.  Arasu uses a top-down grammar induction
algorithm somewhat similar to our rough structure inference phase.  
However, in certain ways, Arasu has a much easier task than we do as html
documents have far more regular structure than ad hoc data sources do.
Arasu exploits this structure (particularly the guarantee of a well-formed
hierarchical nesting structure of \xml{} tags) in essential ways in his 
algorithm,
while we were forced to developed new techniques for our domain.  For example,
we use histograms to summarize the contents of data chunks whereas
Arasu does not.  In addition, a substantial portion of our system
is a description rewriting engine, which Arasu seems not to need.  

TSIMMIS~\cite{chawathe+:tsimmis} is another information extraction
system with similar overall goals to the \pads{} project.  It aims to
allow users to manage and query collections of heterogeneous, ad hoc
data sources.  TSIMMIS sits on top of the Rufus
system~\cite{shoens+:rufus}, which supports automatic classification
of data sources based on features such as the presence of certain
keywords, magic numbers appearing at the beginning of files and file
type.  The sources are classified using categories such as ``email''
and ``C program.''  This sort of categorization is materially
different from the more detailed structure inference mechanisms we
have integrated into the \pads{} system.  For further reading on
information extraction from web pages, Hong's
thesis~\cite{hong:thesis} includes an informative survey.  Though,
Arasu's work and TSIMMIS appear more closely related to our work than
the others Hong mentions.

Another closely related system is Potter's Wheel~\cite{raman+:potterwheel}.
The goal of Potter's Wheel is to help find and purge errors from
relational data sources.  It does so through the use of a spread-sheet
style interface, but in the background, a grammar inference algorithm
infers the structure of the input data, which may be ``ad hoc,'' 
somewhat like ours.  Potter's Wheel's inference mechanism is a brute-force
approach that enumerates all possible sequences of base types that appear
in the training data, although their algorithm is smart enough to
prune some obviously unpromising sequences.  As in our work,
users can specify custom base types, and search for a description
is based on the minimum description length principle.  However,
their application is aimed
strictly at relational data, so they only infer \cd{struct} types
as opposed to enumerations, arrays, switches or unions like we do.  

Other researchers have defined grammar induction algorithms that use
bottom-up rewriting to search through description space for an optimal
description.  Many of these techniques, such as RPNI~\cite{rpni} 
require the availability of both
positive and negative examples.  In our context, negative examples
never exist, making such techniques inapplicable.
% since Gold's early result proved the
%impossibility of {\em perfect} grammar induction for any useful family of
%languages when no negative examples are
%available~\cite{gold:inference}.  
However, others, such as Stolcke and
Omohundro~\cite{stolcke94inducing} and Hong~\cite{hong01using}, do not
assume the existence of negative examples.  These and a number of other systems
search through solution space using
state-merging rewriting rules.  One disadvantage of such
techniques is that the initial state is large (representing
the entire training data set explicitly) and the search space is 
enormous.  Nevertheless, bottom-up state-merging is often used because
it can be difficult to find an effective state-splitting algorithm.
However, for our domain, the histogram-based state-splitting structure
discovery procedure appears to be an effective way to generate a candidate
grammar.  When combined with our rewriting system, the overall
algorithm has proven effective.

% State-merging rewriting rules seem to be
% more popular than 
% that use bottom-up rewriting to find good grammars may suffer from the
% problem of running into local maxima.  The rewriting component of our
% algorithm can also run into a local maximum, but because we start with
% a relatively good candidate generated from our recursive, top-down
% algorithm, this does not appear to be much of a problem for us.  We
% also believe that combining top-down structure discovery with
% bottom-up rewriting has the potential to deal with larger data sources
% than a pure bottom-up approach.  Our empirical experiments demonstrate
% that the top-down structure discovery phase is extremely efficient
% when compared with the cost of rewriting.  However, proposals for
% bottom-up-only inference techniques use the (possibly enormous) data
% source itself as the first description.  We are unaware of other
% systems that combine two techniques similar to ours.
