Researchers have been studying {\em grammar induction}, the process of
learning the structure of a data source, since the 1960s; De La Higuera
surveys some recent trends~\cite{higuera01current}.  However,
our system is unique in two important ways.  First, our inference
algorithm does not stand alone; it is part of the more general \pads{}
programming environment.  The fusion of the
\pads{} system, including its automatic data representation generation,
its error detection facilities, its generic programming environment, 
and its powerful tool suite, together with grammar induction
is one of our key contributions.  Second, many researchers have
focused either on grammar induction for natural language processing or
for information extraction from \xml{} or \html{} documents.  In
contrast, we focus on ad hoc data sources such as system logs and
scientific data sets. Ad hoc data is substantially less
structured than \xml{}, and yet, unlike natural language, it is
possible assign our data sources accurate, compact descriptions.  After 
searching the literature and consulting
with multiple experts in grammar induction at the CAGI 2007 
workshop, where we presented a short, 2-page paper~\cite{burke+:cagi07} 
introducing the ideas in this article, we could find no existing 
system that replicates the functionality of our automatically 
generated tool suite.

% Third, from a
% technical standpoint, we developed a new top-down structure discovery
% algorithm and showed how to combine that productively with a
% classic bottom-up rewriting systems based on the minimum description
% length principle.  In what follows, we compare our system more
% specifically to the most closely related work of other researchers.

Perhaps the most closely related work is from Arasu and 
Garcia-Molina~\cite{arasu+:sigmod03}, who developed a information
extraction system for sets of similar web pages.  
%The
%commonalities between web pages are assumed to be a ``template'' structure
%and the differences are assumed to be values drawn from a database
%sitting behind the web site.  
Arasu uses a top-down grammar induction
algorithm somewhat similar to our rough structure inference phase
(though it does not use histograms),
but has no description rewriting engine.  
%However, in certain ways, Arasu has a much easier task than we do as html
%documents have far more regular structure than ad hoc data sources do.
This algorithm exploits the hierchical nesting
structure of \xml{} documents in essential ways
and so cannot be applied directly to ad hoc data.  
%For example,
%we use histograms to summarize the contents of data chunks whereas
%Arasu does not.  In addition, a substantial portion of our system
%is a description rewriting engine, which Arasu seems not to need.  

The TSIMMIS project~\cite{chawathe+:tsimmis} aims to
allow users to manage and query collections of heterogeneous, ad hoc
data sources.  TSIMMIS sits on top of the Rufus
system~\cite{shoens+:rufus}, which supports automatic classification
of data sources based on features such as the presence of certain
keywords, magic numbers appearing at the beginning of files and file
type.  
%The sources are classified using categories such as ``email''
%and ``C program.''  
This sort of classification is materially
different from the syntactic analysis we have developed.

%For further reading on
%information extraction from web pages, Hong's
%thesis~\cite{hong:thesis} includes an informative survey.  Though,
%Arasu's work and TSIMMIS appear more closely related to our work than
%the others Hong mentions.

Potter's Wheel~\cite{raman+:potterwheel} is a system that attempts to
help users find and purge errors from
relational data sources.  It does so through the use of a spread-sheet
style interface, but in the background, a grammar inference algorithm
infers the structure of the input data, which may be ``ad hoc,'' 
somewhat like ours.  This inference algorithm operates by
enumerating all possible sequences of base types that appear
in the training data.  
%As in our work,
%users can specify custom base types, and search for a description
%is based on the minimum description length principle.  
Since Potter's Wheel is aimed at processing
relational data, they only infer \cd{struct} types
as opposed to enumerations, arrays, switches or unions.  

Other researchers have defined grammar induction algorithms that use
bottom-up rewriting to search through description space for an optimal
description.  Many of these techniques, such as RPNI~\cite{rpni} 
require the availability of both
positive and negative examples.  In our context, negative examples
never exist, making such techniques inapplicable.
% since Gold's early result proved the
%impossibility of {\em perfect} grammar induction for any useful family of
%languages when no negative examples are
%available~\cite{gold:inference}.  
However, others, such as Stolcke and
Omohundro~\cite{stolcke94inducing} and Hong~\cite{hong01using}, do not
assume the existence of negative examples.  These and a number of other systems
search through solution space using
state-merging rewriting rules.  One disadvantage of such
techniques is that the initial state is large (representing
the entire training data set explicitly) and the search space is 
enormous.  Nevertheless, bottom-up state-merging is often used because
it has been difficult to find an effective state-splitting algorithm.
Our histogram-based structure discovery procedure is a new state-splitting
algorithm that appears to work well on ad hoc data when coupled with
bottom-up rewriting.

% State-merging rewriting rules seem to be
% more popular than 
% that use bottom-up rewriting to find good grammars may suffer from the
% problem of running into local maxima.  The rewriting component of our
% algorithm can also run into a local maximum, but because we start with
% a relatively good candidate generated from our recursive, top-down
% algorithm, this does not appear to be much of a problem for us.  We
% also believe that combining top-down structure discovery with
% bottom-up rewriting has the potential to deal with larger data sources
% than a pure bottom-up approach.  Our empirical experiments demonstrate
% that the top-down structure discovery phase is extremely efficient
% when compared with the cost of rewriting.  However, proposals for
% bottom-up-only inference techniques use the (possibly enormous) data
% source itself as the first description.  We are unaware of other
% systems that combine two techniques similar to ours.
