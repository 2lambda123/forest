\section{Performance}
\label{section:performance}

In order to understand the interaction between the layers of our
system as well as the peformance of the system as a whole, we ran
performance measurements at each layer of the corral. All tests were
run on a 1.67 GHz, G4 PowerPC processor, with 500 MB memory, running
Mac OS X 10.3. We performed the tests for two formats, three file
sizes -- 5,10 and 50 megabytes -- and multiple loading strategies
(where appropriate). We started at the bottom of the stack, with the
raw, type-specific parsing functions and measured the time to parse an
entire file. Next, we added a recursive, depth-first walk of the data
through the \padx node representation API (loading the data on demand,
sequentially). In this way, we timed the performance of the
type-specific portion of the \xml interface.  Finally, we used the
\padx concrete data model to traverse the data source.

For the bulk-loading strategy, we found that performance deteriated
for files as small as 20MB, as the system's physical memory was no
longer sufficient to hold all of the data structures. However, our
findings based on the on-demand strategies were more encouraging. At
each stage, we found a linear curve for time vs. file size. For the
first format, we measured $0.13$ {\mu}s per byte, $0.30$ {\mu}s per
byte, and ??? and for the second format, $0.90$ {\mu}s, $1.12$ {\mu}s,
and ???, for the three stages, respectively. For the system as a
whole, we found a constant ratio between stages across file sizes. The
measurements for the two formats differed only in the location of the
curve and the magnitude of the inter-stage ratio.

\begin{verbatim}
dibbler_new:

  |  pads  | node rep | padx concrete ...
--------------------------------------
 5|  .15   | .33      |  
10|  .13   | .31      |
50|  .11   | .29      |

ai:
  | pads  | node rep | padx concrete ...
-------------------------------------
 5|  .91  | 1.24     |  
10|  .90  | 1.13     |
50|  .87  | 1.10     |
\end{verbatim}

% File sizes:
% -rw-r--r--  1 yitzhakm  yitzhakm   4999890 29 Sep 13:41 ai.5MB
% -rw-r--r--  1 yitzhakm  yitzhakm   9999780 28 Sep 20:57 ai.10MB
% -rw-r--r--  1 yitzhakm  yitzhakm  49999915 28 Sep 20:58 ai.50MB

% -rw-r--r--  1 yitzhakm  yitzhakm   4090277 29 Sep 13:34 dibbler_new.5MB
% -rw-r--r--  1 yitzhakm  yitzhakm   8180541 29 Sep 13:34 dibbler_new.10MB
% -rw-r--r--  1 yitzhakm  yitzhakm  49083181 15 Sep 10:07 dibbler_new.50MB

% Total times:
% dibbler: pads:
% timing$ cat CaS_dibbler_new_5MB_linear.time  CaS_dibbler_new_10MB_linear.time  CaS_dibbler_new_50MB_linear.time 
% 0.617 
% 0.618 
% 0.617 
% 1.031 
% 1.047 
% 1.056 
% 5.330 
% 5.542 
% 5.302 

% dibbler_noderep:
% timing$ cat dibbler_new_5MB_linear.time  dibbler_new_10MB_linear.time dibbler_new_50MB_linear.time 
% 1.341 
% 1.364 
% 1.342 
% 2.505 
% 2.498 
% 2.503 
% 14.106
% 14.144
% 14.361

% ai: pads:
% timing$ cat CaS_ai_5MB_linear.time  CaS_ai_10MB_linear.time  CaS_ai_50MB_linear.time 
% 4.527 
% 4.535 
% 4.542 
% 9.134 
% 8.834 
% 8.883 
% 43.451
% 43.350
% 43.355

% ai: noderep:
% timing$ cat ai_5MB_linear.time  ai_10MB_linear.time ai_50MB_linear.time 
% 6.033 
% 5.686 
% 6.904 
% 11.233
% 11.363
% 11.200
% 55.172
% 55.256
% 55.175


\subsection{Querying}

Something about query evaluation:
Although we have not explored custom evaluation plans 
Galax's algebra or optimizer are particularly interesting in the 
\padx{}, we expect to do so 

Give examples of queries that analyst cares about. 

Example of query that can be evaluated in single scan over data
source, but is currently not 

Database person would balk at this point!  Why aren't you just loading
this data into a real database, building indices and getting good
query performance?  B/c data is ephmeral, queries are ephmeral, but
analyst/programmer should profit from disciplined access/querying of
their data.  Don't abandon them to Perl. 

