\section{Performance}
\label{section:performance}

Query performance in \padx{}, as in all query processors, depends on
the efficiency of the underlying \condm{}.  Its performance must be
well understood before it is possible to understand the performance of
particular query plans. We focus on the performance of the \condm{}
and measure the cost of accessing data via the \pads{} type-specific
parsing functions, the \padx{} type-specific node representations, and
the generic \padx{} \condm{}.  At the end of this section, we give
preliminary measurements on query performance.  

We measured data model and query performance for two \pads{} sources:
\dibbler{} and the Web server logs in Figure~\ref{figure:data-sources}
on data sources of 5-50MB in size.  Our measurements were taken on an
1.67GHz Intel Pentium M with 500MB real memory running Linux Redhat
9.0.  Each test was run five times, the high and low times were
dropped, and the mean of the remaining three times is the reported
time.

\cut{
1048557 Sep 29 17:14 ai.1MB
7774374 Sep 28 14:33 ai.xml
* 7.41
1019206 Sep 28 14:43 dibbler.1MB
8047736 Sep 29 20:33 dibbler.xml
* 7.89}

\cut{
\begin{table}
\begin{center}
\begin{tabular}{rl}
% \multicolumn{2}{l}{Platform Configuration} \\ \hline
I: & Intel Pentium M, 1.67GHz, 500MB memory, Linux RH 9.0 \\ 
II:  & G4 PowerPC, 1.67GHz, 500MB memory, Mac OS/X 10.3 \\ 
\end{tabular}
\end{center}
\caption{System configurations}
\label{tab:config}
\end{table}}

\subsection{Concrete Data Model} 
\cut{
In order to understand the interaction between the layers of our
system as well as the peformance of the system as a whole, we ran
performance measurements at each layer of the corral. All tests were
run on a 1.67 GHz, G4 PowerPC processor, with 500 MB memory, running
Mac OS X 10.3. We performed the tests for two formats, three file
sizes -- 5, 10, and 50 megabytes -- and multiple loading strategies
(where appropriate). We started at the bottom of the stack, with the
raw, type-specific parsing functions and measured the time to parse an
entire file. Next, we added a recursive, depth-first walk of the data
through the \padx node representation API (loading the data on demand,
sequentially). In this way, we timed the performance of the
type-specific portion of the \xml{} interface.  Finally, we used the
\padx{} concrete data model to traverse the data source.}

We first measured the time to bulk load data sources of 5, 10, 20, and
50MB by calling the \pads{} parsing functions directly,
\ie{} the lowest level in the \padx{} data model.
Table~\ref{tab:bulk} gives the load time per byte.  For smaller
sources, load time is constant, but eventually increases.
For \dibbler{}, the increasing load time is observed at 50MB and for
the Web server data at 20MB.  We note that for a \pads{} source, the
memory overhead of a \pads{} parsed value can be four to sixteen times
the size of the raw data, depending on the value's type.  In the cases
where non-linear load time occurs, the processes' physical memory usage
is close to or exceeds the physical memory of the platform, CPU
utilization plummets, and the process begins to thrash.  These
measurements indicate that the bulk strategy is only feasible for
smaller data sources.
\begin{table}
\begin{center}
\begin{tabular}{c|r|r}
           & \multicolumn{1}{c|}{Data}  & \multicolumn{1}{c}{\pads{}} \\
Source     & \multicolumn{1}{c|}{size}  & \multicolumn{1}{c}{read} \\ \hline

           &  1MB  & 0.25\\
           &  5MB  & 0.23\\
\dibbler{} & 10MB  & 0.23\\
           & 20MB  & 0.22\\
           & 50MB  & 10.6368  \\ \hline 
           &  1MB  & 0.70\\
           &  5MB  & 0.67\\
Web server & 10MB  & 0.67\\
           & 20MB  & 1.18\\ % 420MB 
           & 50MB  &     \\ 

\end{tabular}
\end{center}
\caption{Bulk strategy: load time per byte in $\mu$s}
\label{tab:bulk}
\end{table}

\cut{   Yitzie's results, which I can't explain
           &  5MB  & 1.76 \\
\dibbler{} & 10MB  & 6.66 \\
           & 20MB  & 8.86 \\ \hline

           &  5MB  & 1.61  \\
Web server & 10MB  & 1.70  \\
           & 20MB  & 3.31  \\}
\cut{We do not yet have a complete explanation for the severity of
this non-linear growth, but }
\cut{So a 20MB source
could expand to 80 to 320 MB in main memory.  Until bulk loading can
be improved, we note that the bulk strategy is only feasible for
sources 10MB or smaller.}

\cut{Here are the numbers for bulk loading (5,10,20): dibbler_new: 1.76,6.66,8.86. ai: 1.61, 1.70, 3.31}

\cut{For the bulk-loading strategy, we found that performance deteriated
for files as small as 20MB, as the system's physical memory was no
longer sufficient to hold all of the data structures. However, our
findings based on the on-demand strategies were more encouraging. At
each stage, we found a linear curve for time vs. file size. For the
first format, we measured $0.13$ {$\mu$}s per byte, $0.30$ {$\mu$}s per
byte, and ??? and for the second format, $0.90$ {$\mu$}s, $1.12$ {$\mu$}s,
and ???, for the three stages, respectively. For the system as a
whole, we found a constant ratio between stages across file sizes. The
measurements for the two formats differed only in the location of the
curve and the magnitude of the inter-stage ratio.}

Next, we measured load time using the on-demand sequential stategy on
sources of 5, 10, and 50 MB.  We were particularly interested in the
overhead introduced at each level in the \condm{}.
Table~\ref{tab:linear} gives the load time per byte in microseconds
($\mu$s) for three levels: reading the source by calling the \pads{}
parsing functions directly; a depth-first walk of the virtual \Xml{}
document by calling the \padx{} node-representation functions; a
depth-first walk of the virtual \Xml{} document by calling the \padx{}
generic data model.  Recall that the node-rep functions are in C and
the generic data model is in O'Caml.
\begin{table}
\begin{center}
\begin{tabular}{c|r|r|r|r}
           &         &           &  \multicolumn{1}{c|}{\padx{}}   &  \multicolumn{1}{c}{\padx{}} \\
           & \multicolumn{1}{c|}{Data}  & \multicolumn{1}{c|}{\pads{}} & \multicolumn{1}{c|}{node}&  \multicolumn{1}{c}{generic} \\
Source     & \multicolumn{1}{c|}{size}  &  \multicolumn{1}{c|}{read}   & \multicolumn{1}{c|}{rep} &  \multicolumn{1}{c}{DM}  \\ \hline
           &  5MB  & 0.07    &  0.27  & 0.61 \\ % 4 times, 2 times
\dibbler{} & 10MB  & 0.06    &  0.26  & 0.56 \\
           & 50MB  & 0.06    &  0.25  & 0.56 \\ \hline
           &  5MB  & 0.54    &  0.78  & 1.63 \\ % 44 percent, 2 times
Web server & 10MB  & 0.53    &  0.74  & 1.61 \\
           & 50MB  & 0.53    &  0.74  & 1.58 \\ 
\end{tabular}
\end{center}
\caption{Sequential strategy: load time per byte in $\mu$s}
\label{tab:linear}
\end{table}

We observe that the load time per byte at each level is near constant for
increasing source size, but that each level incurs a substantial cost
compared to the lower levels.  For the \dibbler{} source, the \padx{}
node-representation is four times slower than the native \pads{}
parsing functions, but for the Web-server source, the \padx{} node
representation is only 44\% slower.  Understanding the source of this
difference requires further experiments with other sources.

For both sources, the generic \condm{} (in O'Caml) is twice as slow as
the node representation (in \C{}).  The interface from the generic data
model to the node representation crosses the O'Caml-C boundary and
uses data marshalling functions generated by the O'Caml IDL tool.  We
have noticed similar per-byte read costs in the \Galax{} secondary
storage system~\cite{galax:ximep2004}, whose data-model architecture
is similar to that of \padx{}. 

We also measured the time to load using the on-demand, random-access
strategy.  In general, it was 10--15\% slower than the on-demand,
sequential strategy. 

These measurements indicate that the on-demand, sequential strategy
scales with increasing data size, and that there is a constant
overhead incurred at each level in the data model.  Ideally, we would
like the cost of accessing data via the generic \condm{} to be close
to the \pads{} read cost, but this will require more engineering
effort.

\cut{ Yitzie's results
           &  5MB  &  0.15   & 0.33    & 3.87 \\
\dibbler{} & 10MB  &  0.13   & 0.31    & 3.82 \\
           & 50MB  &  0.11   & 0.29    & 3.90\\ \hline
           &  5MB  &  0.91   & 1.24    & 5.33 \\
Web server & 10MB  &  0.90   & 1.13    & 5.11 \\
           & 50MB  &  0.87   & 1.10    & 5.07 \\
}

\subsection{Querying}

Ultimately, \padx{}'s query performance depends on \Galax{}, because the
\Galax{} compiler produces and executes the query plans.  Currently,
\Galax{}'s query compiler includes a variety of logical optimizations
for detecting joins and re-grouping constructs in XQuery expressions.
Another important optimization is detecting when a query
can be evaluated in one scan over the input document.  Path
expressions that contain only descendant axes and no branches are one
example of the kind of queries that can be evaluated in one scan.  For
example, the following query, which returns the locations of all
records containing some error in a \dibbler{} source, can be evaluated
in one scan:
\begin{verbatim}
  $pads/Psource/orders/elt/pd/loc
\end{verbatim}

Detecting and evaluating one-scan a.k.a. ``streamable'' queries is
necessary in \Xml{} environments in which the \Xml{} data is an
infinite or bursty stream.  Several query processors already exist in
which streamable queries are evaluated directly over a stream of
tokens produced by SAX-style
parsers~\cite{DBLP:journals/vldb/FlorescuHKLRWCS04,rose:villard:2005}.

Streamable queries are important for \padx{}, because the resulting
plans can be evaluated on large \pads{} sources that are loaded
on-demand and sequentially.  Table~\ref{tab:linear} contains the time
in seconds to evaluate the query above when applied to \pads{} data
sources into which we injected errors randomly in the file
(12 errors per 1MB).  The query plan produced
by \Galax{} is not perfectly pipelined, thus the execution time is super
linear.  
\begin{table}
\begin{center}
\begin{tabular}{r|r}
\multicolumn{1}{c|}{Data}  & \multicolumn{1}{c}{Query}   \\
\multicolumn{1}{c|}{size}  & \multicolumn{1}{c}{time} \\ \hline
  1MB                      &  1.0s                        \\
  5MB                      &  4.8s                        \\
 10MB                      & 10.7s                        \\
 20MB                      & 24.0s                        \\
 50MB                      & 90.0s                        \\
\end{tabular}
\end{center}
\caption{\padx{} query evaluation time in seconds}
\label{tab:linear}
\end{table}

To understand the costs and benefits of other evaluation strategies,
we materialized the 1MB \pads{} source in Table~\ref{tab:linear},
which yielded a 7.4MB \Xml{} document.  We then used \Galax{} to
execute the above query, using the same query execution plan, and
applied it to the 7.4MB \Xml{} document loaded into the main-memory
data model.  The execution time was 13.1s of which 12.9 was spent in
document parsing.  To amortize the cost of document parsing time, we
often store documents in \Galax{}'s secondary storage system.  As a
third alternative, we stored the 7.4MB \Xml{} document in \Galax{}'s
secondary storage system, which required 166MB of disk space.  We then
ran the above query on the stored document.  The execution time was
2.9s, almost three times slower than \padx{} applied to the \pads{}
data directly.  For a fourth alternative, Saxon~\cite{saxon}, a
popular XSLT and XQuery engine, applied to the 7.4MB document required
6.3s.  In summary, our initial impressions are that evaluating
streamable XQuery expressions directly on a \pads{} source appears to
be feasible and efficient.

\cut{
NOTE: After running for some time, I oculd not reproduce these
results.  In fact, all tests ran twice as slow, indicating that
the system was wedged in some way. 

Galax main memory 
13.111807S (12.9 in document parsing) 

Jungle 
real	0m2.906s
user	0m2.500s
sys	0m0.180s

Something about query evaluation:
Although we have not explored custom evaluation plans 
Galax's algebra or optimizer are particularly interesting in the 
\padx{}, we expect to do so 

Give examples of queries that analyst cares about.

Database person would balk at this point!  Why aren't you just loading
this data into a real database, building indices and getting good
query performance?  B/c data is ephmeral, queries are ephmeral, but
analyst/programmer should profit from disciplined access/querying of
their data.  Don't abandon them to Perl. }

\cut{
% File sizes:
% -rw-r--r--  1 yitzhakm  yitzhakm   4999890 29 Sep 13:41 ai.5MB
% -rw-r--r--  1 yitzhakm  yitzhakm   9999780 28 Sep 20:57 ai.10MB
% -rw-r--r--  1 yitzhakm  yitzhakm  49999915 28 Sep 20:58 ai.50MB

% -rw-r--r--  1 yitzhakm  yitzhakm   4090277 29 Sep 13:34 dibbler_new.5MB
% -rw-r--r--  1 yitzhakm  yitzhakm   8180541 29 Sep 13:34 dibbler_new.10MB
% -rw-r--r--  1 yitzhakm  yitzhakm  49083181 15 Sep 10:07 dibbler_new.50MB

% Total times:
% dibbler: pads:
% timing$ cat CaS_dibbler_new_5MB_linear.time  CaS_dibbler_new_10MB_linear.time  CaS_dibbler_new_50MB_linear.time 
% 0.617 
% 0.618 
% 0.617 
% 1.031 
% 1.047 
% 1.056 
% 5.330 
% 5.542 
% 5.302 

% dibbler: noderep:
% timing$ cat dibbler_new_5MB_linear.time  dibbler_new_10MB_linear.time dibbler_new_50MB_linear.time 
% 1.341 
% 1.364 
% 1.342 
% 2.505 
% 2.498 
% 2.503 
% 14.106
% 14.144
% 14.361

% dibbler: ocaml walk doc:
% timing$ cat glxwalk_dibbler_new_5MB_linear.time  glxwalk_dibbler_new_10MB_linear.time  glxwalk_dibbler_new_50MB_linear.time 
% 16.014 15.060 0.140
% 15.780 14.820 0.180
% 15.755 14.590 0.240
% 31.463 29.400 0.270
% 31.058 28.940 0.340
% 31.220 28.970 0.380
% 191.903 169.750 1.630
% 191.396 176.160 1.380
% 190.717 174.180 1.510

% ai: pads:
% timing$ cat CaS_ai_5MB_linear.time  CaS_ai_10MB_linear.time  CaS_ai_50MB_linear.time 
% 4.527 
% 4.535 
% 4.542 
% 9.134 
% 8.834 
% 8.883 
% 43.451
% 43.350
% 43.355

% ai: noderep:
% timing$ cat ai_5MB_linear.time  ai_10MB_linear.time ai_50MB_linear.time 
% 6.033 
% 5.686 
% 6.904 
% 11.233
% 11.363
% 11.200
% 55.172
% 55.256
% 55.175

% ai: ocaml walk doc:
% timing$ cat glxwalk_ai_5MB_linear.time  glxwalk_ai_10MB_linear.time  glxwalk_ai_50MB_linear.time 
% 27.580 24.430 0.310
% 26.241 24.060 0.270
% 26.187 23.910 0.410
% 50.917 47.440 0.580
% 50.973 47.890 0.520
% 51.511 48.140 0.430
% 252.543 237.150 1.860
% 254.043 238.870 1.590
% 253.771 234.570 2.210

}
