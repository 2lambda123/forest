\section{Introduction}
\label{section:intro}
Although enormous amounts of data exist in ``well-behaved'' formats such
as XML and relational databases, massive amounts exist in
non-standard or \textit{ad hoc} data formats as well. \figref{figure:data-sources}
gives some sense of the range and pervasiveness of such data.
\begin{figure}
\begin{center}
\begin{tabular}{|l|l|}
\hline
Name \& Use   &  Representation               \\ \hline\hline
Web server logs (CLF):  &  Fixed-column ASCII records \\ 
Measure web workloads &                             \\ \hline
AT\&T provisioning data: & Variable-width ASCII records  \\ 
Monitor service activation &                              \\ \hline
Call detail: Fraud detection  &  Fixed-width binary records \\  \hline 
AT\&T billing data: & Various Cobol data formats  \\ 
Monitor billing process   &                             \\ \hline
%IP backbone data:  & ASCII   \\
%Monitor network performance  &        \\ \hline
Netflow:                        & Data-dependent number of     \\ 
Monitor network performance  & fixed-width binary records  \\ \hline
Newick:   Immune                 & Fixed-width ASCII records \\ 
system response simulation & in tree-shaped hierachy\\ \hline                                
Gene Ontology:             & Variable-width ASCII records \\
Gene-gene correlations     & in DAG-shaped hierarchy \\ \hline
%HL7:             & Variable-width ASCII records \\
%Medical lab results     &  \\ \hline
CPT codes: Medical diagnoses & Floating point numbers \\ \hline
SnowMed: Medical clinic notes & keyword tags  \\ \hline
\end{tabular}

\caption{Selected ad hoc data sources.}
\label{figure:data-sources}
\end{center}
\end{figure}
Ad hoc data comes in many forms: ASCII, binary, EBCDIC, and mixed
formats.  It can be fixed-width, fixed-column, variable-width, or even
tree-structured. It is often quite large, including some data sources
that generate over a gigabit per second~\cite{gigascope}. It frequently
comes with incomplete and/or out-of-date documentation, and there are
almost always errors in the data.  Sometimes these errors are the most
intersting aspect of the data, because they can indicate where
something is going wrong in a monitored system.

The lack of standard tools for processing ad hoc data forces
analysts to roll their own tools, leading to scenarios such as the
following.  An analyst receives a new ad hoc data source containing
potentially interesting information and a list of pressing questions
about that data.  Could she please provide the answers to the
questions as quickly as possible, preferably last week?  The
accompanying documentation is outdated and missing important
information, so she first has to experiment with the data to discover
its structure.   Eventually, she understands the data well enough to hand-code a
parser, usually in \C{} or \perl{}.  Pressed for time, she interleaves
code to compute the answers to the supplied questions with the parser.
As soon as the answers are computed, she gets a new data source and a
new set of questions to answer.

Through her heroic efforts, the data analyst answered
the necessary questions, but the approach is deficient in many
respects.  
The analyst's hard-won understanding of the data ended up embedded in
a hand-written parser, where it is difficult for others to benefit
from her understanding.
The parser is likely to be brittle with respect to changes in the
input sources.  Consider, for example, how tricky it might be to
figure out which \$3's should be \$4's in a \perl{} parser when a new
column appears in the data.
Errors also pose a significant challenge for hand-coded
parsers.  If the data analyst is thorough in checking for them, then
the error checking code dominates the parser, making it even more
difficult to understand the semantics of the data.  If she is not,
then erroneous data can escape undetected, potentially (silently!)
corrupting down-stream processing.
Finally, during the initial data exploration and in answering the
specified questions, the analyst had to code \textit{how to compute}
the questions rather than being able to express the queries in a
declarative fashion. 
Of course, many of these pitfalls can be avoided with careful design
and sufficient time, but such luxuries might not be available to the analyst.
However, with the appropriate tool support, many aspects of this
process can be greatly simplified.


\cut{
Story: Start with data analyst perspective.  Has ad hoc data source
and a set of questions that he'd like to ask about it.  But the
structure of the data source can change over time and he does not want
his questions to be brittle.  E.g., if using Perl, you don't want to
have to change the Perl program each time a small change occurs in the
structure of the data.  Data sources can be large.  Format conversion
from ad hoc to standard format.  

We had/have solutions to each of these problems in isolation: \pads{} and
Galax.  Story of this paper is the interaction of these systems to
solve all the data analyst's problems simultaneously. 
1. Need to describe it (PADS)
2. Query it and convert to XML (XQuery) 
3. Deal with scale.
Problem focussed, not tool focussed. 

}

We had tools designed to address aspects of the analyst's problem in
isolation: \pads{}~\cite{pldi05} and Galax~\cite{galax}.  \pads{} allows
users to describe ad hoc data sources declaratively and then generates
error-aware parsers and tools for manipulating the sources, including
statistical profiling tools.  Such support would allow the analyst 
to produce a robust, error-aware parser quickly.
Galax supports declarative querying of
semi-structured data sources via XQuery.  Such querying would allow
the analyst to explore the data in the first place and produce the
answers to her questions in the second.  It would also support
converting the data into XML to faciliate further downstream
processing with the vast array of XML-based tools if she so desired.
The challenge we faced was to integrate these tools in a way that
could scale to the large ad hoc data sources we have seen in practice.  

To achieve this integration, we had to evolve \pads{} and Galax in
parallel, modifying the implementation of Galax to support an abstract
data model and augmenting \pads{} with the ability to generate
instances of this data model.  We call the integration of \pads{} and
Galax PADX. \scream{describe architecture of PADX here?}


In the rest of this section, we discuss an example scenario in detail
to illustrate the variety of data management tasks faced by AT\&T data
analysts. We then briefly review \pads{} and Galax in isolation.  We then
describe the architecture of PADX, focusing on issues of scale, and
study the peformance of the resulting system.  We conclude with an
overview of related work and a discussion of future directions.



\cut{

Architecture of PADX.  Supports both non-materialized and virtual  
querying of \pads{} data.  Example: dot query.  User gets to choose
whether to query materialized or virtual XML data.  Appropriate model
depends on query work load and other processes in work flow.  We
support both (!)

Potential benefits: leverage speed of
XQuery processor over native XML document.  Potential costs: same as
having using a materialized view of a database.  ``Staleness'' of XML,
multiple copies, extra time to convert to XML.  Work-load/cost-based
optimization problem.  Tradeoffs between materializing XML document
and querying it multiple times.  Our architecture appropriate for
certain problems.  Data gets regenerated every week or so.  Already
Gigabytes of data.  Could use \pads{} and Galax separately to same
effect.
}

 
\cut{

Systems were developed in parallel.  Solving the above problem
required some additional engineering: implementation of Galax's
abstract data model on top of \pads{} parsing-read functions.  
Hence, we are going to talk about both systems followed by their
synthesis. 

Don't want to explain each system as isolated entities, but show their
interaction---symbiotic relationship that enhances the functionality
of each system.  Explain an interaction between the systems.

Data management standpoint: Vast amounts of data sources that are not
in XML.  Even if you want to view/materialize them in XML, you still
have to get a handle on the data.  Thus, the necessity for \pads{}. 

Programming language spin: a declarative data description, run
compiler, and get parser and related tools. 

XML standpoint: 
XQuery intended to 
\pads{} is an ideal target for Galax as we get a statically typed view of the
non-XML data, and therefore we can statically type check any queries
over the \pads{} data.

There are a couple of stories here.  One is a semantic story: XQuery
is a reasonable query language for \pads{}.  Both represent
semi-structured data.  Error-aware computing by revealing PD in 
XML virtual view.  Question: XML Schema can describe \pads{} types, 
but what about vice versa? Embedding of \pads{} types in XML Schema. 



The other story is about laziness : in Galax's
algebraic query plans, in its tree data model, and in PADX's
implementation of Galax's tree data model.  Laziness supports
scalability of data (and queries?).  Last (small) story is about
PADX's compiled data model---(almost) constant time access to named
fields.

How to support semantic story?  Show mapping from \pads{} types to XML
Schema.  Show realization of pd info.  Show semantics/expressiveness
of queries. 

How to support the importance of laziness? (Or is this too obvious?)
Show scalability of smart loading over bulk loading.  Show improvement
of compiled name access over interpreted name access. 
}

\subsection{Example Scenario}
\label{subsec:example}

%\figref{figure:dibbler-records}
In the telecommunications industry, the term \textit{provisioning} refers to
the process of converting an order for phone service into the
actual service.  This process is complex, involving many interactions
with other companies.  To discover potential problems proactively, the \dibbler{}
project tracks AT\&T's provisioning process by compiling weekly
summaries of the state of certain types of phone service orders.
These summaries, which are stored in flat ASCII text files, can
contain more than 2.2GB of data per week.

These ASCII summaries store the summary date
and one record per order.  Each order record contains a header
followed by a nested sequence of events.  The header has 13 pipe
separated fields: the order number, AT\&T's internal order number, the
order version, four different telephone numbers associated with the
order, the zip code, a billing identifier, the order
type, a measure of the complexity of the order, an unused field, and
the source of the order data.  Many of these fields are optional, in
which case nothing appears between the pipe characters.  The billing
identifier may not be available at the time of processing, in which
case the system generates a unique identifier, and prefixes this value
with the string ``no\_ii'' to indicate the number was generated. The
event sequence represents the various states a service order goes
through; it is represented as a new-line terminated, pipe separated
list of state, timestamp pairs.  There are over 400 distinct states
that an order may go through during provisioning.  It may be apparent from
this description that English is a poor language for describing data
formats!

\begin{figure*}
\begin{small}
\begin{center}
\begin{verbatim}
0|15/Oct/2004:18:46:51
9152|9152|1|9735551212|0||9085551212|07988|no_ii152272|EDTF_6|0|APRL1|DUO|10|16/Oct/2004:10:02:10
9153|9153|1|0|0|0|0||152268|LOC_6|0|FRDW1|DUO|LOC_CRTE|1001476800|LOC_OS_10|17/Oct/2004:08:14:21
\end{verbatim}
\caption{Tiny example of \dibbler{} provisioning data.}
\label{figure:dibbler-records}
\end{center}
\end{small}
\end{figure*}

The data analyst's first task is to write a parser for the
\dibbler{} data  format.  Like many ad hoc data sources, \dibbler{} data
can contain unexpected or corrupted values, so the
parser must handle errors robustly to avoid corrupting the results of
analyses.  Today, parsers for ad hoc formats are often hand-crafted in 
\perl{} or \C{}.  Unfortunately, writing parsers this way is tedious and
error prone, complicated by the lack of documentation, convoluted
encodings designed to save space, and the need to produce efficient
code.  Moreover, the analyst's hard-won understanding of the data ends
up embedded in parsing code, making long-term maintenance difficult
for the original writers and sharing the knowledge with others nearly
impossible.

With \pads{}, the analyst writes a declarative data description of the
physical layout of their data.  The language also permits analysts to
describe expected semantic properties of their data so that deviations
can be flagged as errors. The intent is to allow analysts to capture
in a \pads{} description all that they know about a given data source.

\figref{figure:dibbler} gives the \pads{} description for the
\dibbler{} data format.  In \pads{} descriptions, types are declared
before they are used, so the type that describes the entire data
source, \cd{summary}, appears at the bottom of the description.  In
the next section, we use this example to describe several features of
the \pads{} language.  Here, we simply note that the data analyst
writes this description, and the \pads{} compiler produces
customizable \C{} libraries and tools for parsing, manipulating, and
summarizing the data.  The fact that useful software artifacts are
generated from \pads{} descriptions provides strong incentive for
keeping the descriptions current, allowing them to serve as living
documentation.

\begin{figure}
\begin{small}
\begin{code}
\kw{Precord} \kw{Pstruct} summary\_header\_t \{
  "0|";
  Punixtime tstamp;
\};
\mbox{}
\kw{Pstruct} no\_ramp\_t \{
  "no\_ii";
  Puint64 id;
\};
\mbox{}
\kw{Punion} dib\_ramp\_t \{
  Pint64     ramp;
  no\_ramp\_t  genRamp;
\};
\mbox{}
\kw{Pstruct} order\_header\_t \{
       Puint32             order\_num;
 '|';  Puint32             att\_order\_num;
 '|';  Puint32             ord\_version;
 '|';  \kw{Popt} pn\_t           service\_tn;
 '|';  \kw{Popt} pn\_t           billing\_tn;
 '|';  \kw{Popt} pn\_t           nlp\_service\_tn;
 '|';  \kw{Popt} pn\_t           nlp\_billing\_tn;
 '|';  \kw{Popt} Pzip           zip\_code;
 '|';  dib\_ramp\_t          ramp;
 '|';  Pstring(:'|':)      order\_type;
 '|';  Puint32             order\_details;
 '|';  Pstring(:'|':)      unused;
 '|';  Pstring(:'|':)      stream;
\};
\mbox{}
\kw{Pstruct} event\_t \{
       Pstring(:'|':)    state;   
  '|'; Punixtime         tstamp;
\};
\mbox{}
\kw{Parray} event\_seq\_t \{
  event\_t[] : \kw{Psep}('|') && \kw{Pterm}(\kw{Peor});
\};
\mbox{}
\kw{Precord} \kw{Pstruct} order\_t \{
       order\_header\_t  order\_header;
  '|'; event\_seq\_t     events;
\};
\mbox{}
\kw{Parray} orders\_t \{
  order\_t[];
\};
\mbox{}
\kw{Psource} \kw{Pstruct} summary\{
  summary\_header\_t  summary\_header;
  orders\_t          orders;
\};
\end{code}
\end{small}
\caption{\pads{} description for \dibbler{} provisioning data.}
\label{figure:dibbler}
\end{figure}

Analysts working with ad hoc data also often query their data.  
Questions posed by the \dibbler{} analyst include ``Select all
orders starting within a certain time window,'' ``Count the number of
orders going through a particular state,'' and ``What is the average
time required to go from a particular event state to another
particular event state''.  Such queries are useful for rapid
information discovery and for vetting errors and anomalies in data
before it proceeds to a down-stream process or is loaded into a 
database system. 

\begin{figure}
\begin{small}
\begin{code}
\kw{(: Return orders started in October 2004 :)}
$pads/Psource/orders/elt[events/elt[1]
  [tstamp {>=} {xs:dateTime}("2004-10-01:00:00:00")
{and} tstamp {<} {xs:dateTime}("2004-11-01:00:00:00")]]
\end{code}
\end{small}
\caption{Query applied to \dibbler{} provisioning data.}
\label{figure:dibbler-query}
\end{figure}

With \padx{}, the synthesis of \pads{} and \Galax{}, the analyst
writes declarative XQuery expressions to query his ad hoc data source.
Because XQuery is designed to manipulate semi-structured data, its
expressiveness matches \pads{} data sources well.  XQuery is a
Turing-complete language and therefore powerful
enough to express all the questions above.  For example,
Figure~\ref{figure:dibbler-query} contains an XQuery expression that
produces all orders that started in October, 2004.  In
Section~\ref{section:padx}, we use this example to describe several
features of XQuery and to illustrate why XQuery is an appropriate
query language for ad hoc data.  In particular, XQuery queries may be
statically typed, which helps detects common errors at compile time.
For example, static typing would raise an error if the path expression
in Figure~\ref{figure:dibbler-query} referred to \cd{ordesr} instead
of \cd{orders} or if the analyst erroneoulsy compared the timestamp
\cd{tstamp} to a string.
