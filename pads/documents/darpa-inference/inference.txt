Although PADS descriptions can make managing ad hoc data much easier,
the problem of producing the description in the first place remains.
Writing such descriptions can be tedious and time consuming, often
requiring iteration. First, analysts write a description of
as much of the data as they understand. From this description, they
generate and run analysis tools to find any unknown parts of the data.
They then study those parts and refine the description accordingly.
They repeat this process until all the data either conforms to the
description or constitutes an error in the data rather than an error
in the description.   While the web server log may seem simple
enough to write a description or even a parser by hand, the system log
is much more complex because it has much more variation. (I'm assuming
an earlier section presented sample data from these two formats and
provided at least a minimal description of the content of each.)  Having a
tool to help produce such descriptions would be enormously helpful.

Several observations suggest that we might be able to infer
descriptions automatically.  First, ad hoc data for which we want to
infer descriptions is voluminous and tends to be structured as a
sequence of records.  Consequently, we can view each record as an
independent instance of the desired description.  We can then leverage
the similarities across records to derive the structure of the
description.  The large volume minimizes the influence of errors in
the data, which we assume to be relatively rare.

Second, ad hoc data tends to contain distinctive atomic data such as
dates, IP addresses, file system path names, urls, email addresses,
MAC addresses, etc.  We can leverage the distinctiveness of these data
formats to identify them with high confidence. If a given portion of
many records parses correctly as an IP address, we can assume with high
confidence that that portion of the data is actually an IP address.

Third, text-based ad hoc data formats typically contain *punctuation*,
tokens in the data that serve to delimit the *payload*.  For example,
in web server logs, white space divides the data into columns.
Furthermore, brackets delimit the date field while quotation marks
frame the request.  In the system log example, white space and colons
delimit fields while various key words serve to demark information of
interest to particular applications.  Given a large enough data
source, the frequency of punctuation dominates the frequency of
payload.  Hence we can automatically discover the punctuation.

Viewing identified base types and punctuation as tokens, we can find
groups of tokens that occur with the same frequency in all records.
We call such a group a *token cluster*. At
its simplest, this approach allows us to identify tokens that appear
in every record.  In the web server log example, such a cluster would
include the brackets with their contained date, the white space, the
quotations, and the url embedded in the request.   From this set
of top-level tokens, we can create a PADS Pstruct specification.  The
punctuation tokens become literals in the Pstruct, while each
distinctive base type becomes a simple field with the associated type.
If data appears between a pair of adjacent tokens, then those tokens
define a *context*; we need to further refine the description for this
portion of the data by considering this data in isolation.

\cut{
In more complicated
formats, there may be no single cluster that appears in all records;
in this case, we partition the records into groups according to the
highest frequency token clusters.  For example, if forty percent of the
records have the cluster {"Server", "in bootstrap", "uid"}, twenty percent the cluster
{"lookupd", "version"}, and forty percent an empty cluster {}, we
introduce a Punion with three branches, one for each cluster.  
}

Given a context, we calculate token clusters for
the data occurring in that context.  This technique 
disentangles tokens that play multiple roles in the data
source. If a given cluster appears zero or more times in a given
context, then we introduce an Parray into the inferred description.
If there is no single cluster that appears in all records in the
relevant context, we take each cluster that appears with a significant
frequency and introduce a Punion into the description.  If there are
no high-frequency clusters, we assume that the context must correspond
to a base type, at which point we look for the most descriptive base
type for the observed data.  In either the Parray or the Punion case,
the choice of the constructor introduces new contexts that can be
recursively analyzed until we are left with only base types.

Because we anticipate errors in the data, we do not require that all
records conform to the specification.  In each case where we decide
that a particular context has a certain structure, we allow a small
percentage of the records to fail to match.

Arasu and Garcia-Molena used similar techniques to recover data from
web pages generated from a template and data from a database [cite
Arasu-GarciaMolena, include paper in packet]. Although their domain is
considerable simpler because of the verbosity of XML tags and the lack
of errors in their data, the success of their approach suggests that
our proposed techniques will succeed.

As a final step, we intend to use techniques from the field of
functional dependencies (include citations here) to determine
correlations between record elements.   In particular, for records that
contain a header with a tag and a payload whose format is determined
by the tag, we believe we will be able to discover that
correspondance.   For arrays, we strive to discover a corresponding
field encoding the length. 


Graphical User Interface

Even under the best of circumstances, format inference can only go so far
in constructing a specification.  In particular, the inference system
cannot construct meaningful names for the fields in a description.
For example, although the system can determine that a portion of the data
is a date, it cannot infer that the date represents the start of
a process, or a birthdate, etc.  In addition, users may wish to
inspect records that fail to match the inferred description to
determine if those records are in fact errors or if the description
needs to be further refined. We further envision circumstances under
which the inference system struggles with a particular class of
records and needs guidance to proceed.

To make it easy for users to add field names, to inspect unusual records,
and to provide any necessary guidance, we anticipate extending
LaunchPADS [cite sigmod demo paper; include paper in packet], a
graphical user interface designed to help analysts construct PADS
descriptions visually. (Include screen shot).  

At the moment, LaunchPADS allows users to load a selected data record
(top pane) into an interactive window (middle pane).  In that window,
users can interactively define the structure of the data in a
bottom-up fashion.  The user starts by identifying the atomic pieces
of data in the record: base types and punctuation.  Next, the user
introduces a first layer of structure, marking structs, arrays, and
unions.  As each layer is completed, the corresponding data span is
collapsed into a unit tagged by the associated type, until there
remains only a single, top-level type.  At that point, LaunchPADS
displays the constructed PADS description in the bottom pane and a 
tree-structured view of the description in the left-hand pane.  The
user can edit the tree view to give meaningful field names if desired. 

Of course, any given record is unlikely to capture the full range of
variation in a data source.  To accommodate such variability,
LaunchPADS allows the user to select additional sample records and
refine the description based on their format.  The user can also edit
the description directly.  In addition, LaunchPADS allows the user to
invoke the accumulator tool from the underlying PADS system.  This
tool parses each record in turn according to the current description.
It flags each record that fails to conform as well as tracking for
each leaf in the description tree the percentage of good values and
their range.  This tool allows us to automatically find records that
do not match the description, calling them to the attention of the
user for further refinement.

We anticipate connecting our inference system into such a user
interface.  Instead of having the data analyst start the process by
painstakingly identifying the base types and the higher structure, the
inference system will complete these steps, producing the tree view
and the initial PADS description for the user.  In addition, the
system will be able to automatically construct the accumulator tool
and highlight the records that do not match the specification.  At
this point, the user can refine the specification manually based on
the highlighted records.  This user-supplied information can then be
fed back into a second phase of format inference.



