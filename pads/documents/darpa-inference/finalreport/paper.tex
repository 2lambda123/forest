%acmconf commands
\documentclass{article}
\setlength{\voffset}{-1in}
\setlength{\hoffset}{-1in}
\setlength{\textwidth}{7in}
\setlength{\textheight}{8.65in}
\setlength{\topmargin}{1in}
\setlength{\topskip}{9pt}
\setlength{\oddsidemargin}{0.75in}
\setlength{\evensidemargin}{0.75in}
\setlength{\footskip}{.35in}
%\setlength{\columnsep}{.83cm}
%\setlength{\columnseprule}{0pt}
\setlength{\headheight}{9pt}
\setlength{\headsep}{20pt}
\setlength{\marginparwidth}{0in}
\setlength{\marginparsep}{0in}

\usepackage{latin-abbrevs,math-cmds,math-envs,code,latexsym,amssymb}
\usepackage[dvips]{epsfig}


\title{Real-time Network Forensic Analysis Seedingl Proposal Final Report//
DARPA Contract No. FA8750-07-C-0014}

\author{
David Burke \qquad
Kathleen Fisher \qquad
Andy Moran \qquad
David Walker
}

\begin{document}
\maketitle{}
%\tableofcontents
%\vspace{-1cm}

\section{Introduction}
\label{sec:intro}

An empty citation to prevent bibtex from complaining.~\cite{fisher+:pads}

\section{Expert Interviews}
\label{sec:interviews}

\paragraph*{Chris Miller}

\begin{itemize}
\item Position: Princeton Computer Science Department Systems Administrator

\item What tasks might you use of such a workbench for?  
What kind of problems are you trying to solve?  
(Prevention?  Detection?  Investigation?  Other?)
Normally, the problems Miller attempts to solve are ``investigation'' problems.
Either a user alerts him to a problem or he notices that some machine
has gone down or some software is unresponsive or exhibiting unusual
behavior.  The goal is usually to track down the unusual behavior and
isolate the machine or the user.  In his experience attempting
``prevention'' using automatic methods (anomaly detection) 
does not work very well --
there are too many false positives.

\item What is your workflow?  Miller's workflow may be summarized as
doing 50-60 grep commands in succession over various different kinds of
ASCII log files and email to find out what is different
about the questionable account or machine in the last month.
The process is slow and it would be delightful if it could be speeded up,
but it seems to work.  Normally, the problem is obvious to a human
when it is found ({\em e.g.,} suddenly a surge in logins to a machine
from Brazil).

\item Who are your customers/clients?  Miller works for the computer science 
department, making sure the departmental computing resources are up and 
functioning.

\item What kinds of tools do you use now?  
What would you like improved?
The main tool is grep, 
over and over again over ASCII log files.
This works but it is slow.  If there was a tool that could
summarize all of the log files pertinent to a particular machine or
user over a particular time period ({\em e.g.} the last month)
that would be extremely helpful.  For instance, he might type in
a user name, machine name or an ip address and expect the system to bring up
all ``anomalous'' data relative to that name.  Anything that
speeded up the process of collecting and analyzing log files 
would be helpful.

\item Have you ever had a ``Bad Day'' that you attribute to a lack of 
NFWB.  Is 1-8 hours diagnosing a security problem a ``Bad Day''?  Nothing
more specific mentioned.

\item How much time do you spend on a particular incident?
It varies widely by event, but for just the triage phase 
(the part where Miller figures out *what* happened; 
excluding the part where it must be fixed), it takes anywhere 
from 1 to 8 hours. Miller does not remember ever been in the triage phase 
longer than a full work day. On average, he tends to be able to say 
pretty definitively what happened within 2 hours or so. The higher end 
is for the really serious, widespread incidents. 

\item If you were using the workbench, what would need to be in it to make it useful to you?  ``Good search.''  A facility to help detect ``anomalies''
would be helpful if it worked well, but not absolutely essential.
A facility from graphing certain properties ({\em e.g.,} SPAM vs. non-SPAM
or number of logins/day) might be a nice ``bell'' but not essential.

\item Can you characterize the kinds of data you encounter?  Almost
exclusively ASCII log files and email.  Logs of user logins to 
all our authenticated services, as well as packet filters, 
DHCP logs, and any logged software failures. 

\item What kinds of performance requirements do you have?
\begin{itemize}
\item Size?  In our primary collection, Miller stores 
approximately 150-300MB of gzip-compressed logs per month. 
For a representative month this year (August, 2006), 
that comes out to about 16mil lines of log file per month.

\item How much variation within single file?
In *format*, the individual files do not have much variation; 
probably describable in a dozen-or-less definitions per file. 
Obviously, though, the data itself varies with the size of the 
internet attacking us!

\item How much data do you need to process?
 For a given incident, Miller typically needs to look at 3 months of data. 
Sometimes more, but rarely less. 

\item How frequently do you process data?  
We haven't seen a major incident recently, but since Miller has worked
in the Computer science Department, they have averaged 
a major incident every 2 years or so.

In terms of processing logs, the machines are logged continuously and
the logs are rotated nightly.

\item What bit rate?

Hard to answer, but the CS department 
syslog server (where these logs are collected) 
averages 100kbps of traffic sent to it. 

\item How many different kinds of data sources do you need to process?
Here is a wishlist:
\begin{itemize}
\item syslog files
\item web server log files
\item cisco sflow   (it may
                     not be appropriate, but would be neat.) 
\end{itemize}

\item How frequently do you get a new data source to process?
 A new *type* of source is rare, but new sources are pretty common 
(in the shape of new servers that send additional syslog data). 

\item Are correlations between data sources important?
That would be very important for selling the tool to Miller, 
as correlation between these sources is the biggest time-sink in 
diagnosing an incident.

\end{itemize}  

\item What functions over the data do you need?  Good search/querying 
support. 

\item What would be a show-stopper?  No real show-stoppers.  It has to
be more time efficient than repeated grep.

\item Can we get example data?  No.  The data cannot be released for 
privacy reasons (it could be if we had an appropriate anonymization tool.)

\end{itemize}


\bibliographystyle{abbrv}
{
\bibliography{pads}
}

\appendix

\section{Example Data Formats}
\label{sec:example formats}

\paragraph{UNIX {\tt last}.} Chris Miller often starts out by
invoking this command when he needs to investigate a security breach.

\begin{verbatim}
dpw      pts/1        nj-71-0-108-124. Sat Dec  9 17:34 - 17:50  (00:15)    
dpw      pts/1        nj-71-0-108-124. Sat Dec  9 16:26 - 17:31  (01:04)    
dpw      pts/26       nj-71-0-108-124. Tue Dec  5 18:35 - 19:32  (00:57)    
dpw      pts/22       nj-71-0-108-124. Tue Dec  5 18:29 - 19:32  (01:03)    

wtmp begins Fri Dec  1 04:26:51 2006
\end{verbatim}   

\end{document}










































